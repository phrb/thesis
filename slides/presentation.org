#+STARTUP: beamer overview indent inlineimages logdrawer
#+TITLE: @@latex: Toward Transparent and Parsimonious
#+TITLE: Methods \\ for Automatic Performance Tuning@@
#+AUTHOR:    \footnotesize Pedro Bruel \newline \scriptsize \emph{phrb@ime.usp.br}
#+DATE:      \scriptsize July 9 2021
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:nil @:t \n:nil ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   tex:t latex:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:

* LaTeX Setup                                      :B_ignoreheading:noexport:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

See [[Emacs Setup]] below for local buffer variables

** LaTeX Configuration
:latex_header:
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [10pt, compress, aspectratio=169, xcolor={table,usenames,dvipsnames}]
#+LATEX_HEADER: \mode<beamer>{\usetheme[numbering=fraction, progressbar=none, titleformat frame=regular, titleformat title=regular, sectionpage=progressbar]{metropolis}}

#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)

#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}

#+LATEX_HEADER: \definecolor{Base}{HTML}{191F26}

# #+LATEX_HEADER: \definecolor{Accent}{HTML}{b10000}
# #+LATEX_HEADER: \colorlet{Accent}{PineGreen}
# #+LATEX_HEADER: \colorlet{Accent}{OliveGreen!85!Black}

#+LATEX_HEADER: \colorlet{Accent}{BrickRed}
#+LATEX_HEADER: \colorlet{CodeBg}{Gray!20}
#+LATEX_HEADER: \colorlet{CodeHighBg}{Accent!10}
#+LATEX_HEADER: \colorlet{Highlight}{Accent!18}

#+LATEX_HEADER: \usepackage{tcolorbox}
#+LATEX_HEADER: \tcbset{width=\textwidth,boxrule=0pt,colback=Highlight,
#+LATEX_HEADER:         arc=0pt,auto outer arc,left=0.4em,right=0.4em,
#+LATEX_HEADER:         boxsep=0.3em}


#+LATEX_HEADER: \setbeamercolor{alerted text}{fg=Accent}
#+LATEX_HEADER: \setbeamercolor{frametitle}{fg=Accent,bg=normal text.bg}
#+LATEX_HEADER: \setbeamercolor{normal text}{bg=black!2,fg=Base}

#+LATEX_HEADER: \usefonttheme{professionalfonts}
#+LATEX_HEADER: \usepackage{newpxtext}
#+LATEX_HEADER: \usepackage{newpxmath}

#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usemintedstyle{vs}
#+LATEX_HEADER: \setminted{
#+LATEX_HEADER:            beameroverlays=true,
#+LATEX_HEADER:            frame=none,
#+LATEX_HEADER:            bgcolor=CodeBg,
#+LATEX_HEADER:            fontsize=\footnotesize,
#+LATEX_HEADER:            baselinestretch=1.2,
#+LATEX_HEADER:            framesep=0.6em,
#+LATEX_HEADER:            tabsize=2,
#+LATEX_HEADER:            breaklines
#+LATEX_HEADER: }

#+LATEX_HEADER: \AtBeginEnvironment{snugshade*}{\vspace{-\FrameSep}}
#+LATEX_HEADER: \AfterEndEnvironment{snugshade*}{\vspace{-\FrameSep}}

#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \setmonofont{DejaVuSansMono}
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller[2]\relax}
#+LATEX_HEADER: \addtobeamertemplate{block begin}{}{\justifying}

#+LATEX_HEADER: \captionsetup[figure]{labelformat=empty}

#+LATEX_HEADER: \hypersetup{
#+LATEX_HEADER:     colorlinks=true,
#+LATEX_HEADER:     linkcolor={Accent},
#+LATEX_HEADER:     citecolor={Accent},
#+LATEX_HEADER:     urlcolor={Accent}
#+LATEX_HEADER: }

#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \setbeamertemplate{section page}{
#+LATEX_HEADER:   \centering
#+LATEX_HEADER:   \begin{minipage}{0.5\linewidth}
#+LATEX_HEADER:     \raggedright
#+LATEX_HEADER:     \usebeamercolor[fg]{section title}
#+LATEX_HEADER:     \usebeamerfont{section title}
#+LATEX_HEADER:     \insertsectionhead\\[-1ex]
#+LATEX_HEADER:     \usebeamertemplate*{progress bar in section page}
#+LATEX_HEADER:     \par
#+LATEX_HEADER:     \ifx\insertsubsectionhead\@empty\else%
#+LATEX_HEADER:       \usebeamercolor[fg]{subsection title}%
#+LATEX_HEADER:       \usebeamerfont{subsection title}%
#+LATEX_HEADER:       \insertsubsectionhead
#+LATEX_HEADER:     \fi
#+LATEX_HEADER:   \end{minipage}
#+LATEX_HEADER:   \par
#+LATEX_HEADER:   \vspace{\baselineskip}
#+LATEX_HEADER: }

#+LATEX_HEADER: \setbeamertemplate{title separator}{
#+LATEX_HEADER: \begin{tikzpicture}
#+LATEX_HEADER:   \fill[fg] (0,0) rectangle (0.85\textwidth,
#+LATEX_HEADER:                              \metropolis@titleseparator@linewidth);
#+LATEX_HEADER: \end{tikzpicture}%
#+LATEX_HEADER: \par%
#+LATEX_HEADER: }

#+LATEX_HEADER: \setlength{\metropolis@titleseparator@linewidth}{1pt}
#+LATEX_HEADER: \setlength{\metropolis@progressonsectionpage@linewidth}{2.5pt}
# #+LATEX_HEADER: \setlength{\metropolis@progressinheadfoot@linewidth}{2pt}

#+LATEX_HEADER: \makeatother
:end:


* Ignore Introduction Section Page                          :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
\bgroup\metroset{sectionpage=none}
#+end_export

* Introduction
:PROPERTIES:
:DURATION: 5 minutes
:END:
** High Performance Computing is Needed at Multiple Scales, \dots
#+LaTeX: \begin{columns}\begin{column}[t]{.35\linewidth}\centering
Climate  simulation   for  policies   @@latex:  \mbox{to   fight  \alert{climate
change}}@@

#+begin_export latex
\begin{center}
  \includegraphics[width=\columnwidth]{../img/nasa_climate_change}

  \vspace{0.3em}

  \includegraphics[width=.96\columnwidth]{../img/weather_model}
\end{center}
#+end_export

#+LaTeX: \end{column}\begin{column}[t]{.35\linewidth}\centering
Fluid dynamics for stronger *infrastructure* and fuel *efficiency*

#+begin_export latex
\begin{center}
  \includegraphics[width=.97\columnwidth]{../img/cfd_infrastructure.jpg}

  \vspace{0.24em}

  \includegraphics[width=\columnwidth]{../img/cfd_car.png}
\end{center}
#+end_export

#+LaTeX: \end{column}\begin{column}[t]{.35\linewidth}\centering
Molecular dynamics for *virtual testing* of *drugs* and *vaccines*

#+begin_export latex
\begin{center}
  \includegraphics[width=\columnwidth]{../img/drug_virtual_trials_small}

  \vspace{0.15em}

  \includegraphics[width=.98\columnwidth]{../img/molecules_simulation}
\end{center}
#+end_export

#+LaTeX: \end{column}\end{columns}\bigskip
** \dots and the Performance of Supercomputers has so far Improved Exponentially
#+ATTR_LATEX: :width 1\textwidth
[[file:../img/top500_rmax_rpeak_annotated.pdf]]
** Software must Improve to Leverage Complexity, and Autotuning can Help
#+ATTR_LATEX: :width \textwidth
[[file:../img/49_years_processor_data_annotated.pdf]]

*** Notes                                                       :noexport:
- Hardware has ceased to provide  "effortless" performance gains but performance
  continues to increase
- Code optimization is crucial for performance, and will continue to be

** An Autotuning Example: Loop /Blocking/ and /Unrolling/ for Matrix Multiplication :BMCOL:
:PROPERTIES:
:BEAMER_opt: t,fragile
:END:

*** Pull Up Columns                                       :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

\vspace{-1.4em}

*** Optimizing Matrix Multiplication :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.44
:END:

#+latex: \vspace{0.5em}

How  to  *restructure  memory  accesses*  in loops  to  increase  throughput  by
leveraging *cache locality*?

#+begin_export latex
\uncover<4>{
#+end_export


**** Resulting *Search Space*                                    :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/seymour2008comparison.pdf]]

#+begin_export latex
}
#+end_export

*** Sample in C                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.56
:END:

\vspace{-1.4em}

#+begin_export latex
\begin{onlyenv}<1>
\begin{figure}
\begin{minted}[fontsize=\scriptsize]{C}
int N = 256;

float A[N][N], B[N][N], C[N][N];
int i, j, k;
// Initialize A, B, C
for(i = 0; i < N; i++){ // Load A[i][]
  for(j = 0; j < N; j++){
    // Load C[i][j], B[][j] to fast memory
    for(k = 0; k < N; k++){




      C[i][j] += A[i][k] * B[k][j];
    }


    // Write C[i][j] to main memory
  }
}
\end{minted}
\end{figure}
\end{onlyenv}
#+end_export

#+begin_export latex
\begin{onlyenv}<2>
\begin{figure}
\begin{minted}[fontsize=\scriptsize]{C}
int N = 256;
int B_size = 4;
float A[N][N], B[N][N], C[N][N];
int i, j, k, x, y;
// Initialize A, B, C
for(i = 0; i < N; i += B_size){
  for(j = 0; j < N; j += B_size){
    // Load block (i, j) of C to fast memory
    for(k = 0; k < N; k++){
      // Load block (i, k) of A to fast memory
      // Load block (k, y) of B to fast memory
      for(x = i; x < min(i + B_size, N); x++){
        for(y = j; y < min(j + B_size, N); y++){
          C[x][y] += A[x][k] * B[k][y];
        }
      }
    }
    // Write block (i, j) of C to main memory
  }
} // One parameter: B_size
\end{minted}
\end{figure}
\end{onlyenv}
#+end_export

#+begin_export latex
\begin{onlyenv}<3->
\begin{figure}
\begin{minted}[fontsize=\scriptsize]{C}
int N = 256;
int B_size = 4;
float A[N][N], B[N][N], C[N][N];
int i, j, k, x; // int U_size = 4;
// Initialize A, B, C
for(i = 0; i < N; i += B_size){
  for(j = 0; j < N; j += B_size){
    // Load block (i, j) of C to fast memory
    for(k = 0; k < N; k++){
      // Load block (i, k) of A to fast memory
      // Load block (k, y) of B to fast memory
      for(x = i; x < min(i + B_size, N); x++){
        C[i][j + 0] += A[i][k] * B[k][j + 0];
        C[i][j + 1] += A[i][k] * B[k][j + 1];
        C[i][j + 2] += A[i][k] * B[k][j + 2];
        C[i][j + 3] += A[i][k] * B[k][j + 3];
      }
    } // Write block (i, j) of C to main memory
  }
} // Two parameters: B_size and U_size
\end{minted}
\end{figure}
\end{onlyenv}
#+end_export
** Autotuning Problems in Other Domains: Dimension Becomes an Issue
*** Left Plot                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.54
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/search_spaces_B.pdf]]

\pause
*** Right Plot                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.54
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/search_spaces_A.pdf]]

*** Notes                                                        :noexport:
- Earlier application to optimize BLAS routines
- Autotuning for specific domains and Neural Networks


** Autotuning as an /Optimization/ or /Learning/ Problem
*** Performance as a *Function*                             :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:

#+latex: \vspace{0.5em}

Performance: $f: \mathcal{X} \to \mathbb{R}$

#+latex: \vspace{-0.7em}

- Parameters: $\mathbf{x} = [x_1\;\dots\;x_n]^{\transp} \in \mathcal{X}$
- Performance metric: $y = f(\mathbf{x})$

To *minimize* $f$, we can adapt *proven methods* from other \mbox{domains}:
- @@latex:\textcolor{NavyBlue}{\textbf{Function minimization}}@@,
  @@latex:\textcolor{OliveGreen}{\textbf{Learning}}@@: not necessarily
  *parsimonious* and *transparent*

- @@latex:\textcolor{BrickRed}{\textbf{Design of Experiments}}@@: can help, but
  not widely used for autotuning

*** Search Space Dimension                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/search_methods_annotated.pdf]]

** Toward Transparent and Parsimonious Autotuning
*** Contributions of this Thesis                           :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:BEAMER_env: block
:END:

- *Developing* transparent and  parsimonious autotuning methods  based on
  the *Design of Experiments*
- *Evaluating* different autotuning methods in different HPC domains

**** Transparent

- Use statistics to justify code optimization choices
- Learn about the search space

**** Parsimonious

- Carefully choose which experiments to run
- Minimize $f$ using as few measurements as possible

*** Applications                                                    :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+begin_export latex
\begin{onlyenv}<1>
\begin{table}[]
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ll@{}}
      \textbf{Domain} & \textbf{Method}         \\ \midrule
      CUDA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}}, \phantom{\textbf{L},} \textcolor{BrickRed}{\textbf{D}} \\
      FPGA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}} \\
      OpenCL Laplacian Kernel & \textcolor{NavyBlue}{\textbf{F}},
      \textcolor{OliveGreen}{\textbf{L}}, \textcolor{BrickRed}{\textbf{D}} \\
      SPAPT Kernels & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      CNN Quantization & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \multicolumn{2}{c}{\footnotesize\textcolor{NavyBlue}{\textbf{F}}: Function Minimization,
        \textcolor{OliveGreen}{\textbf{L}}: Learning,} \\[-1em]
      \multicolumn{2}{c}{\footnotesize\textcolor{BrickRed}{\textbf{D}}: Design of Experiments} \\[-0.7em]
      \multicolumn{2}{c}{\footnotesize{\phantom{Dummy Line}}}
    \end{tabular}%
  }
\end{table}
\end{onlyenv}
#+end_export

#+begin_export latex
\begin{onlyenv}<2>
\begin{table}[]
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ll@{}}
      \textbf{Domain} & \textbf{Method}         \\ \midrule
      CUDA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}}, \phantom{\textbf{L},} \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}FPGA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}} \\
      \rowcolor{Accent!15}OpenCL Laplacian Kernel & \textcolor{NavyBlue}{\textbf{F}},
      \textcolor{OliveGreen}{\textbf{L}}, \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}SPAPT Kernels & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}CNN Quantization & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \multicolumn{2}{c}{\footnotesize\textcolor{NavyBlue}{\textbf{F}}: Function Minimization,
        \textcolor{OliveGreen}{\textbf{L}}: Learning,} \\[-1em]
      \multicolumn{2}{c}{\footnotesize\textcolor{BrickRed}{\textbf{D}}: Design of Experiments} \\[-0.7em]
      \multicolumn{2}{c}{\footnotesize\colorbox{Accent!15}{\phantom{A}}: In this presentation}
    \end{tabular}%
  }
\end{table}
\end{onlyenv}
#+end_export



* End Ignore Introduction Section Page                      :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
\egroup
#+end_export
* Applying Methods @@latex:\mbox{for \textcolor{NavyBlue}{Function Minimization}}@@
:PROPERTIES:
:DURATION: 10 minutes
:END:
** Minimizing Functions using Derivatives and Heuristics
*** We know or can compute *information* about $f$          :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:BEAMER_env: block
:END:
#+begin_export latex
\begin{itemize}
\item Directly measure \alert{new} $\mathbf{x}_1,\dots,\mathbf{x}_k,\dots,\mathbf{x}_n$
\item Search for the \alert{global optimum}
\uncover<2->{\item Try to escape \alert{local optima}}
\end{itemize}
\uncover<3->{
#+end_export

**** Strong Hypothesis: we can Compute *Derivatives*
#+begin_export latex
\begin{itemize}
\item  $\mathbf{x}_{k}  =  \mathbf{x}_{k  -  1}  -  \mathbf{H}f(\mathbf{x}_{k  -
  1})\nabla{}f(\mathbf{x}_{k - 1})$
\uncover<4->{\item Move to best point in a \alert{neighborhood}}
\end{itemize}
#+end_export
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
\uncover<5->{
#+end_export

**** Hard to State Hypotheses: Search Heuristics
- *Random Walk*, Simulated Annealing, Genetic Algorithms, and many others
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
\uncover<6->{
#+end_export

**** @@latex: \colorbox{Highlight}{How to \alert{choose} a method?}@@
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
#+end_export

*** Images                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:

#+begin_export latex
\only<1>{
  {\scriptsize
    \begin{align*}
      f(\mathbf{x}) &= (x_1 + 2x_2  - 7)^{2} + (2x_1 + x_2 - 5)^{2}\text{,}\phantom{ + z} \\
      & x_1,x_2 \in [-10, 10]\phantom{\text{, and }z \thicksim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})}
    \end{align*}
  }%
}%
\only<2->{
  {\scriptsize
    \begin{align*}
      \scriptsize
      f(\mathbf{x}) &= (x_1 + 2x_2  - 7)^{2} + (2x_1 + x_2 - 5)^{2} + z\text{,} \\
      & x_1,x_2 \in [-10, 10]\text{, and }z \thicksim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})
    \end{align*}
  }%
}%
\vspace{-2.5em}
\begin{figure}
  \begin{overlayarea}{\columnwidth}{.7\textheight}
    \includegraphics<1>[width=\columnwidth]{../img/simple_search_space_A}
    \includegraphics<2>[width=\columnwidth]{../img/simple_search_space_B}
    \includegraphics<3>[width=\columnwidth]{../img/booth_gradient_C}
    \includegraphics<4>[width=\columnwidth]{../img/booth_gradient_D}
    \includegraphics<5->[width=\columnwidth]{../img/booth_gradient_E}
  \end{overlayarea}
\end{figure}
#+end_export
** Choosing Methods from an Ensemble
\vspace{-1em}
*** Example of an Ensemble of \textcolor{NavyBlue}{Methods} :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:

\vspace{-1em}

#+ATTR_LATEX: :width \textwidth
[[file:../img/mab_opentuner.pdf]]
*** Methods in this Ensemble                                :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:
- \textbf{A},  \textbf{C}:  Simulated  Annealing with  different  *temperature*,
  \textbf{B}: Gradient Descent
**** Minimization of $f$ using *OpenTuner*
- Coordinated by a *Multi-Armed Bandit* (MAB) algorithm
- Methods perform  *measurements* proportionally to  their *score*
- Score: the number  of times a method  found the *best* $\mathbf{x}$  in a time
  window
- The best $\mathbf{x}$ over all methods is reported
** Application: High-Level Synthesis for FPGAs
:PROPERTIES:
:END:

#+begin_export latex
\vspace{-0.3em}
\begin{figure}
  \begin{overlayarea}{\columnwidth}{.4\textheight}
    \includegraphics<1>[width=\columnwidth]{../img/fpga-stack_0.pdf}
    \includegraphics<2>[width=\columnwidth]{../img/fpga-stack_1.pdf}
    \includegraphics<3>[width=\columnwidth]{../img/fpga-stack_2.pdf}
    \includegraphics<4->[width=\columnwidth]{../img/fpga-stack_3.pdf}
  \end{overlayarea}
\end{figure}
\uncover<5->{
#+end_export

*** Search Space                                            :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:
#+ATTR_LATEX: :width \textwidth
[[file:../img/fpga_docker_tuner.pdf]]
#+begin_export latex
}
\uncover<6->{
#+end_export
*** Performance Metrics                                     :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:
- Weighted average of *8 hardware metrics*
- Metrics for the  usage of registers, memory, DSP  units, frequency and
  clock speed
- An   *expert*  devised   weights   for  optimizing   for  area,   latency,
  performance, or for multiple criteria
*** End Block                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
#+end_export

** Results
\vspace{-1.7em}
*** Experimental Settings                                   :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.5
:END:
- *11 problems*
- Up to *300 measurements* per problem
- Compared to *optimized* LegUp configurations for the target FPGA
**** Improvements
- *10% improvement* on weighted average
- *2* and *5 times* improvements for some metrics and scenarios
**** Implementation in OpenTuner                                 :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
- Ensemble with Simulated Annealing, Genetic Algorithms, and Nelder-Mead
*** Figures                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
\vspace{-1.2em}
**** Performance: \textcolor{NavyBlue}{darker blues} are better :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
\vspace{-1em}
#+ATTR_LATEX: :width .95\textwidth
[[file:../img/quali_brazil/heatmap_default_stratixV_perf-eps-converted-to.pdf]]
\vspace{-1.5em}
**** Weighted Average for all Scenarios
\vspace{-1em}
#+ATTR_LATEX: :width .95\textwidth
[[file:../img/quali_brazil/heatmap_wns_comparison-eps-converted-to.pdf]]

** Discussion
\vspace{-1em}
*** Autotuning with Heuristics                              :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.55
:BEAMER_env: block
:END:
- Lack of *structured exploration* prevented statistical analyses and
  interpretation

**** Needle in a Haystack
- Global optimum in *10^{123}* configurations?
- Are there *better configurations* to find?
- For how long should we continue *exploring*?

**** Proprietary Software Stack for FPGAs                        :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
- LegUp is now proprietary software

*** Right Plot                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/search_spaces_A.pdf]]

*** Stop Columns                                          :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

#+begin_export latex
\uncover<2>{
#+end_export

*** Sequential *Design of Experiments*
#+begin_export latex
\vspace{-1em}
\begin{center}
  \begin{tcolorbox}[hbox]
    Structure explorations using \alert{modeling hypotheses} to guide
    \alert{sampling} and \alert{optimization}
  \end{tcolorbox}
\end{center}
#+end_export

*** End Block                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
#+end_export

* Applying Sequential Design of Experiments
:PROPERTIES:
:DURATION: 15 minutes
:END:
** Search Space Hypotheses with \textcolor{OliveGreen}{Linear Models}
*** \textcolor{OliveGreen}{Learning}: Building Surrogates   :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.55
:END:
- $f: \mathcal{X} \to \mathbb{R}$
- Model: $f(\mathbf{x}) = \mathbf{x}^{\transp}\boldsymbol{\theta} + \boldsymbol{\varepsilon}\text{, with }\varepsilon \thicksim \mathcal{N}(0, \sigma^2)$
- Data: $(\mathbf{x}_k, y_k = f(\mathbf{x}_k))$
- $\mathbf{x}_{1,\dots{},n}$ in a *given* design $\mathbf{X}$

#+begin_export latex
\uncover<2->{
#+end_export

**** Minimize a *Surrogate* instead of $f$
- Surrogate: $\hat{f}_{\theta}(\mathbf{x}^{\prime}) = \mathbf{x^{\prime}}^{\transp}\hat{\boldsymbol{\theta}}$
- Estimator: $\hat{\boldsymbol{\theta}} = (\mathbf{X}^{\transp}\mathbf{X})^{-1}\mathbf{X}^{\transp}\mathbf{y}$
#+begin_export latex
\uncover<3->{
#+end_export

**** Variance of $\hat{\boldsymbol{\theta}}$ is independent of $\mathbf{y}$
#+begin_export latex
\vspace{-0.3em}
\begin{center}
  \begin{tcolorbox}[hbox]
    $\text{Var}(\hat{\boldsymbol{\theta}}) = (\mathbf{X}^{\transp}\mathbf{X})^{-1}\sigma^{2}$
  \end{tcolorbox}
\end{center}
}
#+end_export
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
#+end_export

*** 10 Measurements of Booth's Function                     :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:BEAMER_env: block
:END:
#+begin_export latex
\vspace{-1.4em}
\begin{center}
  \only<1>{
    {\scriptsize
      \begin{align*}
        \scriptsize
        f(\mathbf{x}) &= (x_1 + 2x_2  - 7)^{2} + (2x_1 + x_2 - 5)^{2} + \varepsilon
      \end{align*}
    }%
  }%
  \only<2->{
    {\scriptsize
      \begin{align*}
        \hat{f}_{\theta}(\mathbf{x}) = \hat{\theta}_0 + \hat{\theta}_1x_1 +
        \hat{\theta}_2x_2 + \hat{\theta}_3x_{1}^{2} + \hat{\theta}_4x_{2}^{2} +
        \hat{\theta}_5x_1x_2
      \end{align*}
    }%
  }%
\end{center}
\vspace{-1em}
\begin{figure}
  \begin{overlayarea}{\columnwidth}{.7\textheight}
    \includegraphics<1>[width=\columnwidth]{../img/booth_sample_linmod_0}
    \includegraphics<2->[width=\columnwidth]{../img/booth_sample_linmod_2}
  \end{overlayarea}
\end{figure}
#+end_export
** Design of Experiments
\vspace{-1.5em}
*** Choosing the Design $\mathbf{X}$                        :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.55
:END:
- Minimize $\text{Var}(\hat{\boldsymbol{\theta}})$
- Decrease number of experiments in $\mathbf{X}$
- Enable testing hypotheses

**** Components
- $\mathbf{X}_{n\times{}p}$: design matrix
- $\mathbf{x}_{1\times{}n} \in \mathbf{X}$: factor columns
- $x_1,\dots,x_p \in \mathbf{x}$: chosen factor levels

**** Examples
- Factorial designs,  screening, Latin  Hypercube and  low-discrepancy sampling,
  *optimal design*
*** Distance of Experiments Impacts $\text{Var}(\hat{\boldsymbol{\theta}})$ :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:BEAMER_env: block
:END:
#+begin_export latex
\begin{figure}
  \begin{overlayarea}{\columnwidth}{.7\textheight}
    \includegraphics<1>[width=\columnwidth]{../img/experimental_design/confidence_lin_effects_annotated_0}
    \includegraphics<2->[width=\columnwidth]{../img/experimental_design/confidence_lin_effects_annotated_1}
  \end{overlayarea}
\end{figure}
#+end_export
** Optimal Design: Parsimony
\vspace{-3em}
*** Designs                                              :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.55
:END:
- Building surrogates within a *constrained budget*
- Exploiting known search space structure
- *Testing* modeling hypotheses
**** Maximizing $\text{det}(\mathbf{X}^{\transp}\mathbf{X})$ by Swapping Rows :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
- Requires an initial @@latex: \textcolor{OliveGreen}{\textbf{model}}@@
- Choose best rows for $\mathbf{X}$ from a *large set*
- $\text{D}(\mathbf{X}) \propto \text{det}(\mathbf{X}^{\transp}\mathbf{X})$
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
\uncover<5->{
#+end_export

**** @@latex: \colorbox{Highlight}{Best design is \alert{independent of measurements}}@@
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
#+end_export

*** Sampling with Different \textcolor{OliveGreen}{Models}  :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:BEAMER_env: block
:END:
#+begin_export latex
\vspace{-1.5em}
\begin{center}
  \only<1>{
    {\scriptsize
      \begin{align*}
        \scriptsize
        f(\mathbf{x}) &= (x_1 + 2x_2  - 7)^{2} + (2x_1 + x_2 - 5)^{2} + \varepsilon
      \end{align*}
    }%
  }%
  \only<2>{
    {\scriptsize
      \begin{align*}
        \hat{f}_{\theta}(\mathbf{x}) = \hat{\theta}_0 + \hat{\theta}_1x_1 +
        \hat{\theta}_2x_2
      \end{align*}
    }%
  }%
  \only<3>{
    {\scriptsize
      \begin{align*}
        \hat{f}_{\theta}(\mathbf{x}) = \hat{\theta}_0 + \hat{\theta}_1x_1 +
        \hat{\theta}_2x_2 + \hat{\theta}_3x_{1}^{2} + \hat{\theta}_4x_{2}^{2}
      \end{align*}
    }%
  }%
  \only<4->{
    {\scriptsize
      \begin{align*}
        \hat{f}_{\theta}(\mathbf{x}) = \hat{\theta}_0 + \hat{\theta}_1x_1 +
        \hat{\theta}_2x_2 + \hat{\theta}_3x_{1}^{2} + \hat{\theta}_4x_{2}^{2} +
        \hat{\theta}_5x_1x_2
      \end{align*}
    }%
  }%
  \vspace{-2.4em}
  \begin{figure}
    \begin{overlayarea}{\columnwidth}{0.7\textheight}
      \only<1->{\hspace{1.8em}}\includegraphics<1>[width=0.85\columnwidth]{../img/experimental_design/booth_descent_0}%
      \includegraphics<2>[width=0.85\columnwidth]{../img/experimental_design/booth_descent_1}%
      \includegraphics<3>[width=0.85\columnwidth]{../img/experimental_design/booth_descent_2}%
      \includegraphics<4->[width=0.85\columnwidth]{../img/experimental_design/booth_descent_3}%
    \end{overlayarea}
  \end{figure}%
  {\footnotesize
    \textcolor{BrickRed}{$\boldsymbol{\times}$}: global optimum,
    \textcolor{BrickRed}{$\blacksquare$}: best point found, \\
    \(\color{NavyBlue}{\bullet}\): measurements
  }%
\end{center}
#+end_export
** Interpreting Significance with Analysis of Variance
*** Analysis of Variance (*ANOVA*)                          :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.55
:BEAMER_env: block
:END:
- Identify which factors and levels are *significant*

**** Steps
- Group observations by factor and factor levels
- Estimate distributions for each group mean $\mu$
- Run *F-tests* for significance of differences between means

**** @@latex:\colorbox{Highlight}{Enables \alert{refining} initial hypotheses}@@

*** One-Way ANOVA for Levels A, B, C                        :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:BEAMER_env: block
:END:

#+ATTR_LATEX: :width \columnwidth
[[file:../img/anova_example_annotated.pdf]]
** A Transparent and Parsimonious Approach to Autotuning
  #+ATTR_LATEX: :width 0.8\textwidth
  [[file:../img/ccgrid19/doe_anova_strategy_slides.pdf]]
** Application: OpenCL GPU Laplacian Kernel
\vspace{-1em}
*** Edge Detection with the Laplacian                       :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.45
:END:

#+ATTR_LATEX: :width .95\columnwidth
[[file:../img/laplacian/flower.jpg]]

\vspace{-1em}

**** The OpenCL Kernel
- Highly optimized
- Efficiently parametrized
- Generated by BOAST
- Completely evaluated previously

*** Search Space with *10^4* Valid Configurations         :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.55
:END:

#+begin_export latex
\begin{table}[htbp]
  \centering
  \scriptsize
  \begin{tabular}{lll}
    \textbf{Factor} & \textbf{Levels} & \textbf{Short Description}\\
    \midrule
    \textit{vector\_length} & \(2^0,\dots,2^4\) & Size of vectors\\
    \textit{load\_overlap} & \textit{true}, \textit{false} & Load overlaps in vectorization\\
    \textit{temporary\_size} & \(2,4\) & Byte size of temporary data\\
    \textit{elements\_number} & \(1,\dots,24\) & Size of equal data splits\\
    \textit{y\_component\_number} & \(1,\dots,6\) & Loop tile size\\
    \textit{threads\_number} & \(2^5,\dots,2^{10}\) & Size of thread groups\\
    \textit{lws\_y} & \(2^0,\dots,2^{10}\) & Block size in \(y\) dimension\\
  \end{tabular}
\end{table}
#+end_export

**** Performance Metric and Starting \textcolor{OliveGreen}{Model}
#+begin_export latex
\vspace{-2em}
\begin{center}
  {\scriptsize
    \begin{align*}
      \textit{time\_per\_pixel} \sim &  \textit{y\_component\_number} + \frac{1}{\textit{y\_component\_number}} \; + \nonumber \\
      & \textit{temporary\_size} + \textit{vector\_length} + \textit{load\_overlap} \; + \nonumber \\
      & \textit{lws\_y} + \frac{1}{\textit{lws\_y}} + \textit{elements\_number} + \frac{1}{\textit{elements\_number}} \; + \nonumber \\
      & \textit{threads\_number} + \frac{1}{\textit{threads\_number}}
    \end{align*}%
  }%
\end{center}%
#+end_export
** Results: 1000 Repetitions with a Budget of 120 Measurements
#+begin_export latex
\begin{figure}
  \begin{overlayarea}{\columnwidth}{0.63\textheight}
    \includegraphics<1>[width=\columnwidth]{../img/dopt_anova_experiments/comparison_histogram_annotated_0}%
    \includegraphics<2->[width=\columnwidth]{../img/dopt_anova_experiments/comparison_histogram_annotated_1}%
  \end{overlayarea}
\end{figure}%

\begin{center}
  \parbox{\columnwidth}{\scriptsize
    \centering
    RS: Random Sampling, LHS: Latin  Hypercube Sampling, GS: Greedy Search, \\
    GSR: Greedy Search w.  Restart, GA: Genetic Algorithm, LM: Linear Model, QR:
    Quantile  Regression\uncover<2->{,}\\
    \uncover<2->{DLMT:  D-Optimal Designs,  Linear  Model w. Transform}
  }%
\end{center}%
#+end_export
** Results: Parsimony Regarding the Budget
#+ATTR_LATEX: :width 0.7\columnwidth
[[../img/dopt_anova_experiments/comparison_histogram_annotated_1.pdf]]

#+begin_export latex
\vspace{-1em}
\begin{table}
  \centering
  \begingroup\scriptsize
  \begin{tabular}{lrrrrr}
    & \multicolumn{3}{c}{Slowdown} & \multicolumn{2}{c}{Budget} \\
    Method & Mean & Min. & Max. & Mean & Max. \\
    \midrule
    Random Sampling (RS) & 1.10 & 1.00 & 1.39 & 120.00 & 120 \\
    Latin Hypercube Sampling (LHS) & 1.17 & 1.00 & 1.52 & 98.92 & 125 \\
    Greedy Search (GS) & 6.46 & 1.00 & 124.76 & 22.17 & 106 \\
    Greedy Search w. Restart (GSR) & 1.23 & 1.00 & 3.16 & 120.00 & 120 \\
    Genetic Algorithm (GA) & 1.12 & 1.00 & 1.65 & 120.00 & 120 \\
    Linear Model (LM) & 1.02 & 1.01 & 3.77 & 119.00 & 119 \\
    Quantile Regression (QR) & 1.02 & 1.01 & 2.06 & 119.00 & 119 \\
    \rowcolor{Highlight}D-Opt., Linear Model w. Transform (DLMT) & 1.01 & 1.01 & 1.01 & 54.84 &  56 \\
  \end{tabular}%
  \endgroup
\end{table}%
#+end_export
** Interpreting the Optimization
** Application: SPAPT kernels
*** Search Spaces and Performance Metric
** Results
- Pick one?
- Is there anything to find?
- Leave GPR for later
** Interpreting the Optimization
- Maybe remove?
** Discussion
- Sequential and incremental
  - Definitive restrictions
  - Improvements by batch
  - Low model flexibility (rigid models)

- Motivating results in the Laplacian kernel
- It is possible to interpret results, guide optimization
  - sometimes simpler models give better results

- For SPAPT kernels, it is still unclear:
  - if there is something to find, and when to stop exploring
  - is there a global optimum, is it "hidden"?
    - how to find it, if so? (can learning do it?)

- Random Sampling has good performance
  - Abundance of local optima?

- What is the most effective level of abstraction for optimizing a program?
  - Compiler, kernel, machine, model, dependencies?


* Active Learning with Gaussian Processes
:PROPERTIES:
:DURATION: 10 minutes
:END:
** More Flexibility with Gaussian Process Regression
- Introduce notation:
  - Model of $f$: $f(\mathbf{x}) \sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$
  - Surrogate $\hat{f}_{\theta}(\mathbf{x}) \sim f(\mathbf{x}) \; \vert{} \; \mathbf{X}, \mathbf{y}$

** Space-filling Designs
- Curse of dimensionality for sampling:
  - Most sampled points will be on the "shell"
- LHS: Partition and then sample, need to optimize later
- Low-discrepancy: deterministic space-filling sequences
** Expected Improvement: Balancing Exploitation and Exploration
- How to decide where to measure next?
** Application: GPU Laplacian and SPAPT
- GPR was applied to these problems too
- Quickly Present Results Here Too
- GPR is good too, but the simpler model is more consistent
** Application: Quantization for Convolutional Neural Networks
*** Search Space, Constraints, and Performance Metrics
- Comparing with a Reinforcement Learning approach in the original paper
- ImageNet
** Results
** Interpreting the Optimization
- Sobol indices, inconclusive
** Discussion
- Low-discrepancy sampling in high dimension
- Constraints complicate exploration
- Multi-objective optimization
- A more complex method usually produces less interpretable results,
  but not always achieves better optimizations
* Conclusion
:PROPERTIES:
:DURATION: 5 minutes
:END:
** Toward Transparent and Parsimonious Autotuning
*** Contributions of this Thesis                           :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:BEAMER_env: block
:END:

- *Developing* transparent and  parsimonious autotuning methods  based on
  the *Design of Experiments*
- *Evaluating* different autotuning methods in different HPC domains

**** Transparent

- Use statistics to justify code optimization choices
- Learn about the search space

**** Parsimonious

- Carefully choose which experiments to run
- Minimize $f$ using as few measurements as possible

*** Applications                                                    :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+begin_export latex
\begin{table}[]
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ll@{}}
      \textbf{Domain} & \textbf{Method}         \\ \midrule
      CUDA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}}, \phantom{\textbf{L},} \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}FPGA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}} \\
      \rowcolor{Accent!15}OpenCL Laplacian Kernel & \textcolor{NavyBlue}{\textbf{F}},
      \textcolor{OliveGreen}{\textbf{L}}, \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}SPAPT Kernels & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}CNN Quantization & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \multicolumn{2}{c}{\footnotesize\textcolor{NavyBlue}{\textbf{F}}: Function Minimization,
        \textcolor{OliveGreen}{\textbf{L}}: Learning,} \\[-1em]
      \multicolumn{2}{c}{\footnotesize\textcolor{BrickRed}{\textbf{D}}: Design of Experiments} \\[-0.7em]
      \multicolumn{2}{c}{\footnotesize\colorbox{Accent!15}{\phantom{A}}: In this presentation}
    \end{tabular}%
  }
\end{table}
#+end_export



** Reproducibility of Performance Tuning Experiments
- Redoing all the work for different problems
- Complementary approaches:
  - Completely evaluate small sets of a search space
  - Collaborative optimizing for different architectures, problems
** Key Discussions
*** Curse of Dimensionality for Autotuning Problems         :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.5
:END:
- Implications for Sampling and Learning
- Space-filling helps, but does not solve
- Constraints
**** Which method to use?                                        :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
- Design  of  Experiments  for  transparency and  parsimony  when  building  and
  interpreting statistical models
- Linear models for simpler spaces and problems
- Gaussian Process Surrogates for more complex situations
*** It is often unclear if there is something to find       :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:
- Abundance of local optima
- Is there a global optimum, is it "hidden"?
  - How to find it, if so? (can learning do it?)
- What is the most effective level of abstraction for optimizing a program?
  - Compiler, kernel, machine, model, dependencies?
- When to stop?
** Conclusion

* Ending Title :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \maketitle

* Emacs Setup                                      :noexport:B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
See [[LaTeX Setup]] above for the beamer configuration

** Use XeLaTeX
If you  accept this definition  when loading the  buffer, this variable  will be
modified  locally to  the buffer.  This allows  using XeLaTeX  for exporting  to
beamer pdf.

# Local Variables:
# eval: (setq-local org-latex-pdf-process (list "latexmk -xelatex -shell-escape %f"))
# End:
