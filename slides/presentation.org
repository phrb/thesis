#+STARTUP: beamer overview indent inlineimages logdrawer
#+TITLE: @@latex: Toward Transparent and Parsimonious
#+TITLE: Methods \\ for Automatic Performance Tuning@@
#+AUTHOR:    \footnotesize Pedro Bruel \newline \scriptsize \emph{phrb@ime.usp.br}
#+DATE:      \scriptsize July 9 2021
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:nil @:t \n:nil ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   tex:t latex:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:

* LaTeX Setup                                      :B_ignoreheading:noexport:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

See [[Emacs Setup]] below for local buffer variables

** LaTeX Configuration
:latex_header:
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [10pt, compress, aspectratio=169, xcolor={table,usenames,dvipsnames}]
#+LATEX_HEADER: \mode<beamer>{\usetheme[numbering=fraction, progressbar=none, titleformat frame=regular, titleformat title=regular, sectionpage=progressbar]{metropolis}}

#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)

#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}

#+LATEX_HEADER: \definecolor{Base}{HTML}{191F26}

# #+LATEX_HEADER: \definecolor{Accent}{HTML}{b10000}
# #+LATEX_HEADER: \colorlet{Accent}{PineGreen}
# #+LATEX_HEADER: \colorlet{Accent}{OliveGreen!85!Black}

#+LATEX_HEADER: \colorlet{Accent}{BrickRed}
#+LATEX_HEADER: \colorlet{CodeBg}{Gray!20}
#+LATEX_HEADER: \colorlet{CodeHighBg}{Accent!10}
#+LATEX_HEADER: \colorlet{Highlight}{Accent!18}

#+LATEX_HEADER: \setbeamercolor{alerted text}{fg=Accent}
#+LATEX_HEADER: \setbeamercolor{frametitle}{fg=Accent,bg=normal text.bg}
#+LATEX_HEADER: \setbeamercolor{normal text}{bg=black!2,fg=Base}

#+LATEX_HEADER: \usefonttheme{professionalfonts}
#+LATEX_HEADER: \usepackage{newpxtext}
#+LATEX_HEADER: \usepackage{newpxmath}

#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usemintedstyle{vs}
#+LATEX_HEADER: \setminted{
#+LATEX_HEADER:            beameroverlays=true,
#+LATEX_HEADER:            frame=none,
#+LATEX_HEADER:            bgcolor=CodeBg,
#+LATEX_HEADER:            fontsize=\footnotesize,
#+LATEX_HEADER:            baselinestretch=1.2,
#+LATEX_HEADER:            framesep=0.6em,
#+LATEX_HEADER:            tabsize=2,
#+LATEX_HEADER:            breaklines
#+LATEX_HEADER: }

#+LATEX_HEADER: \AtBeginEnvironment{snugshade*}{\vspace{-\FrameSep}}
#+LATEX_HEADER: \AfterEndEnvironment{snugshade*}{\vspace{-\FrameSep}}

#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \setmonofont{DejaVuSansMono}
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller[2]\relax}
#+LATEX_HEADER: \addtobeamertemplate{block begin}{}{\justifying}

#+LATEX_HEADER: \captionsetup[figure]{labelformat=empty}

#+LATEX_HEADER: \hypersetup{
#+LATEX_HEADER:     colorlinks=true,
#+LATEX_HEADER:     linkcolor={Accent},
#+LATEX_HEADER:     citecolor={Accent},
#+LATEX_HEADER:     urlcolor={Accent}
#+LATEX_HEADER: }

#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \setlength{\metropolis@titleseparator@linewidth}{1pt}
#+LATEX_HEADER: \setlength{\metropolis@progressonsectionpage@linewidth}{2.5pt}
# #+LATEX_HEADER: \setlength{\metropolis@progressinheadfoot@linewidth}{2pt}
#+LATEX_HEADER: \makeatother
:end:


* Ignore Introduction Section Page                          :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
\bgroup\metroset{sectionpage=none}
#+end_export

* Introduction
:PROPERTIES:
:DURATION: 5 minutes
:END:
** High Performance Computing is Needed at Multiple Scales, \dots
#+LaTeX: \begin{columns}\begin{column}[t]{.35\linewidth}\centering
Climate  simulation   for  policies   @@latex:  \mbox{to   fight  \alert{climate
change}}@@

#+begin_export latex
\begin{center}
  \includegraphics[width=\columnwidth]{../img/nasa_climate_change}

  \vspace{0.3em}

  \includegraphics[width=.96\columnwidth]{../img/weather_model}
\end{center}
#+end_export

#+LaTeX: \end{column}\begin{column}[t]{.35\linewidth}\centering
Fluid dynamics for stronger *infrastructure* and fuel *efficiency*

#+begin_export latex
\begin{center}
  \includegraphics[width=.97\columnwidth]{../img/cfd_infrastructure.jpg}

  \vspace{0.24em}

  \includegraphics[width=\columnwidth]{../img/cfd_car.png}
\end{center}
#+end_export

#+LaTeX: \end{column}\begin{column}[t]{.35\linewidth}\centering
Molecular dynamics for *virtual testing* of *drugs* and *vaccines*

#+begin_export latex
\begin{center}
  \includegraphics[width=\columnwidth]{../img/drug_virtual_trials_small}

  \vspace{0.15em}

  \includegraphics[width=.98\columnwidth]{../img/molecules_simulation}
\end{center}
#+end_export

#+LaTeX: \end{column}\end{columns}\bigskip
** \dots and the Performance of Supercomputers has so far Improved Exponentially
#+ATTR_LATEX: :width 1\textwidth
[[file:../img/top500_rmax_rpeak_annotated.pdf]]
** Software must Improve to Leverage Complexity, and Autotuning can Help
#+ATTR_LATEX: :width \textwidth
[[file:../img/49_years_processor_data_annotated.pdf]]

*** Notes                                                       :noexport:
- Hardware has ceased to provide  "effortless" performance gains but performance
  continues to increase
- Code optimization is crucial for performance, and will continue to be

** An Autotuning Example: Loop /Blocking/ and /Unrolling/ for Matrix Multiplication :BMCOL:
:PROPERTIES:
:BEAMER_opt: t,fragile
:END:

*** Pull Up Columns                                       :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

\vspace{-1.4em}

*** Optimizing Matrix Multiplication :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.44
:END:

#+latex: \vspace{0.5em}

How  to  *restructure  memory  accesses*  in loops  to  increase  throughput  by
leveraging *cache locality*?

#+begin_export latex
\uncover<4>{
#+end_export


**** Resulting *Search Space*                                    :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/seymour2008comparison.pdf]]

#+begin_export latex
}
#+end_export

*** Sample in C                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.56
:END:

\vspace{-1.4em}

#+begin_export latex
\begin{onlyenv}<1>
\begin{figure}
\begin{minted}[fontsize=\scriptsize]{C}
int N = 256;

float A[N][N], B[N][N], C[N][N];
int i, j, k;
// Initialize A, B, C
for(i = 0; i < N; i++){ // Load A[i][]
  for(j = 0; j < N; j++){
    // Load C[i][j], B[][j] to fast memory
    for(k = 0; k < N; k++){




      C[i][j] += A[i][k] * B[k][j];
    }


    // Write C[i][j] to main memory
  }
}
\end{minted}
\end{figure}
\end{onlyenv}
#+end_export

#+begin_export latex
\begin{onlyenv}<2>
\begin{figure}
\begin{minted}[fontsize=\scriptsize]{C}
int N = 256;
int B_size = 4;
float A[N][N], B[N][N], C[N][N];
int i, j, k, x, y;
// Initialize A, B, C
for(i = 0; i < N; i += B_size){
  for(j = 0; j < N; j += B_size){
    // Load block (i, j) of C to fast memory
    for(k = 0; k < N; k++){
      // Load block (i, k) of A to fast memory
      // Load block (k, y) of B to fast memory
      for(x = i; x < min(i + B_size, N); x++){
        for(y = j; y < min(j + B_size, N); y++){
          C[x][y] += A[x][k] * B[k][y];
        }
      }
    }
    // Write block (i, j) of C to main memory
  }
} // One parameter: B_size
\end{minted}
\end{figure}
\end{onlyenv}
#+end_export

#+begin_export latex
\begin{onlyenv}<3->
\begin{figure}
\begin{minted}[fontsize=\scriptsize]{C}
int N = 256;
int B_size = 4;
float A[N][N], B[N][N], C[N][N];
int i, j, k; // int U_size = 16;
// Initialize A, B, C
for(i = 0; i < N; i += B_size){
  for(j = 0; j < N; j += B_size){
    // Load block (i, j) of C to fast memory
    for(k = 0; k < N; k++){
      // Load block (i, k) of A to fast memory
      // Load block (k, y) of B to fast memory
      C[i + 0][j + 0] += A[i + 0][k] * B[k][j + 0];
      C[i + 0][j + 1] += A[i + 0][k] * B[k][j + 1];
      // Unroll the other 13 iterations
      C[i + Bsize - 1][j + B_size - 1] += A[i + Bsize - 1][k] * B[k][j + B_size - 1];
    }
    // Write block (i, j) of C to main memory
  }
} // Two parameters: B_size and U_size
\end{minted}
\end{figure}
\end{onlyenv}
#+end_export
** Autotuning Problems in Other Domains: Dimension Becomes an Issue
*** Left Plot                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.54
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/search_spaces_B.pdf]]

\pause
*** Right Plot                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.54
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/search_spaces_A.pdf]]

*** Notes                                                        :noexport:
- Earlier application to optimize BLAS routines
- Autotuning for specific domains and Neural Networks


** Autotuning as an /Optimization/ or /Learning/ Problem
*** Performance as a *Function*                             :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:

#+latex: \vspace{0.5em}

Performance: $f: \mathcal{X} \to \mathbb{R}$

#+latex: \vspace{-0.7em}

- Parameters: $\mathbf{x} = [x_1\;\dots\;x_n]^{\transp} \in \mathcal{X}$
- Performance metric: $y = f(\mathbf{x})$

To *minimize* $f$, we can adapt *proven methods* from other \mbox{domains}:
- @@latex:\textcolor{NavyBlue}{\textbf{Function minimization}}@@,
  @@latex:\textcolor{OliveGreen}{\textbf{Learning}}@@: not necessarily
  *parsimonious* and *transparent*

- @@latex:\textcolor{BrickRed}{\textbf{Design of Experiments}}@@: can help, but
  not widely used for autotuning

*** Search Space Dimension                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

#+ATTR_LATEX: :width \textwidth
[[file:../img/search_methods_annotated.pdf]]

** Toward Transparent and Parsimonious Autotuning

*** Contributions of this Thesis                           :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:BEAMER_env: block
:END:

- *Developing* transparent and  parsimonious autotuning methods  based on
  the *Design of Experiments*
- *Evaluating* different autotuning methods in different HPC domains

**** Transparent

- Use statistics to justify code optimization choices
- Learn about the search space

**** Parsimonious

- Carefully choose which experiments to run
- Minimize $f$ using as few measurements as possible

*** Applications                                                    :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+begin_export latex
\begin{onlyenv}<1>
\begin{table}[]
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ll@{}}
      \textbf{Domain} & \textbf{Method}         \\ \midrule
      CUDA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}}, \phantom{\textbf{L},} \textcolor{BrickRed}{\textbf{D}} \\
      FPGA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}} \\
      OpenCL Laplacian Kernel & \textcolor{NavyBlue}{\textbf{F}},
      \textcolor{OliveGreen}{\textbf{L}}, \textcolor{BrickRed}{\textbf{D}} \\
      SPAPT Kernels & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      CNN Quantization & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \multicolumn{2}{c}{\footnotesize\textcolor{NavyBlue}{\textbf{F}}: Function Minimization,
        \textcolor{OliveGreen}{\textbf{L}}: Learning,} \\[-1em]
      \multicolumn{2}{c}{\footnotesize\textcolor{BrickRed}{\textbf{D}}: Design of Experiments} \\[-0.7em]
      \multicolumn{2}{c}{\footnotesize{\phantom{Dummy Line}}}
    \end{tabular}%
  }
\end{table}
\end{onlyenv}
#+end_export

#+begin_export latex
\begin{onlyenv}<2>
\begin{table}[]
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ll@{}}
      \textbf{Domain} & \textbf{Method}         \\ \midrule
      CUDA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}}, \phantom{\textbf{L},} \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}FPGA compiler parameters &
      \textcolor{NavyBlue}{\textbf{F}} \\
      \rowcolor{Accent!15}OpenCL Laplacian Kernel & \textcolor{NavyBlue}{\textbf{F}},
      \textcolor{OliveGreen}{\textbf{L}}, \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}SPAPT Kernels & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \rowcolor{Accent!15}CNN Quantization & \phantom{\textbf{F}, }\textcolor{OliveGreen}{\textbf{L}},
      \textcolor{BrickRed}{\textbf{D}} \\
      \multicolumn{2}{c}{\footnotesize\textcolor{NavyBlue}{\textbf{F}}: Function Minimization,
        \textcolor{OliveGreen}{\textbf{L}}: Learning,} \\[-1em]
      \multicolumn{2}{c}{\footnotesize\textcolor{BrickRed}{\textbf{D}}: Design of Experiments} \\[-0.7em]
      \multicolumn{2}{c}{\footnotesize\colorbox{Accent!15}{\phantom{A}}: In this presentation}
    \end{tabular}%
  }
\end{table}
\end{onlyenv}
#+end_export



* End Ignore Introduction Section Page                      :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
\egroup
#+end_export
* Autotuning with Generic Methods for Function Minimization
:PROPERTIES:
:DURATION: 10 minutes
:END:
** Minimizing Functions using Derivatives and Other Information
*** We know or can compute *information* about $f$          :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:BEAMER_env: block
:END:
- Directly measure *new* $\mathbf{x}_1,\cdots,\mathbf{x}_k,\cdots,\mathbf{x}_n$
- Search for the *global optimum*, try to escape *local optima*
#+begin_export latex
\uncover<3->{
#+end_export
**** Strong Hypothesis: we can Compute *Derivatives*
- $\mathbf{x}_{k}   =  \mathbf{x}_{k   -   1}   -  \mathbf{H}f(\mathbf{x}_{k   -
  1})\nabla{}f(\mathbf{x}_{k - 1})$
- *Locally* and *globally*
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
\uncover<5->{
#+end_export
**** Hard to State Hypotheses: Search Heuristics
- Random Walk,  Simulated Annealing,  Genetic Algorithms, Nelder-Mead,  and many
  others
**** End Block                                           :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+begin_export latex
}
#+end_export
*** Images                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:

#+begin_export latex
\begin{figure}
  \begin{overlayarea}{\columnwidth}{.7\textheight}
    \includegraphics<1>[width=\columnwidth]{../img/simple_search_space_A}
    \includegraphics<2>[width=\columnwidth]{../img/simple_search_space_B}
    \includegraphics<3>[width=\columnwidth]{../img/booth_gradient_C}
    \includegraphics<4>[width=\columnwidth]{../img/booth_gradient_D}
    \includegraphics<5>[width=\columnwidth]{../img/booth_gradient_E}
  \end{overlayarea}
\end{figure}
#+end_export
** Search Heuristics with Multi-Armed Bandit
- OpenTuner
- Ensemble of search heuristics
  - Coordinated by a MAB algorithm (online learning)
** Application: High-Level Synthesis for FPGAs
*** Search Space and Performance Metrics
** Results
** Discussion
- Function minimization methods *are not parsimonious*
- Curse of dimensionality
- It is often unclear:
  - if there is something to find, and when to stop exploring
- It is impossible to:
  - interpret optimizations
* Applying Sequential Design of Experiments
:PROPERTIES:
:DURATION: 15 minutes
:END:
** Linear Models
- Learning: Building surrogates, used for optimization
- Introduce notation:
  - $\hat{f}_{\theta}: \mathcal{X} \to \mathbb{R}$
  - Model of $f$: $f(\mathbf{x}) = \mathbf{x}^{\transp}\theta + \varepsilon$
  - Surrogate $\hat{f}_{\theta}(\mathbf{x}) = \mathbf{x}^{\transp}\hat{\theta}$.
- Best Linear Unbiased Estimator
- Learning methods assume the design $\mathbf{X}$ is given
** Design of Experiments
- Statistical methods  to choose the  design $\mathbf{X}$ to  minimize surrogate
  model variance
- Notation:
  - Factors, levels, design
- Simple linear model example for 2-factor designs
- Factorial designs , screening
** Optimal Design
- Distributing points  according to initial modeling  hypotheses decreases model
  matrix determinant (associated with variance)
- Good for exploiting known search space structure, or verifying existing hypotheses
** Space-filling Designs
- Curse of dimensionality for sampling:
  - Most sampled points will be on the "shell"
- LHS: Partition and then sample, need to optimize later
- Low-discrepancy: deterministic space-filling sequences
** Interpreting Significance
- ANOVA for Linear Models
  - Isolate "significant" factors
- Sobol indices
  - expensive computation
** A Transparent and Parsimonious Approach to Autotuning
- Explain paper diagram
** Application: GPU Laplacian
*** Search Space and Performance Metric
** Results
- Comparison with multiple methods
- Leave GPR for later
** Interpreting the Optimization
** Application: SPAPT kernels
- Pick one?
*** Search Spaces and Performance Metric
** Results
- Is there anything to find?
- Leave GPR for later
** Interpreting the Optimization
** Discussion
- Sequential and incremental
  - Definitive restrictions
  - Improvements by batch
  - Low model flexibility (rigid models)

- Motivating results in the Laplacian kernel
- It is possible to interpret results, guide optimization
  - sometimes simpler models give better results

- For SPAPT kernels, it is still unclear:
  - if there is something to find, and when to stop exploring
  - is there a global optimum, is it "hidden"?
    - how to find it, if so? (can learning do it?)

- Random Sampling has good performance
  - Abundance of local optima?

- What is the most effective level of abstraction for optimizing a program?
  - Compiler, kernel, machine, model, dependencies?


* Active Learning with Gaussian Processes
:PROPERTIES:
:DURATION: 10 minutes
:END:
** More Flexibility with Gaussian Process Regression
- Introduce notation:
  - Model of $f$: $f(\mathbf{x}) \sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$
  - Surrogate $\hat{f}_{\theta}(\mathbf{x}) \sim f(\mathbf{x}) \; \vert{} \; \mathbf{X}, \mathbf{y}$

** Expected Improvement: Balancing Exploitation and Exploration
- How to decide where to measure next?
** Application: GPU Laplacian and SPAPT
- GPR was applied to these problems too
*** Search Spaces and Performance Metrics
** Results: GPU Laplacian
- GPR is good too, but the simpler model is more consistent
** Results: SPAPT
- GPR still can't find better configurations
** Application: Quantization for Convolutional Neural Networks
*** Search Space, Constraints, and Performance Metrics
- Comparing with a Reinforcement Learning approach in the original paper
- ImageNet
** Results
** Interpreting the Optimization
- Sobol indices, inconclusive
** Discussion
- Low-discrepancy sampling in high dimension
- Constraints complicate exploration
- Multi-objective optimization
- A more complex method usually produces less interpretable results,
  but not always achieves better optimizations
* Conclusion
:PROPERTIES:
:DURATION: 5 minutes
:END:
** Contributions of this Thesis
- Striving to develop and apply transparent and parsimonious autotuning methods
- Applications in this thesis:

#+begin_export latex
\begin{table}[]
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ll@{}}
      \textbf{Domain} & \textbf{Method}         \\ \midrule
      CUDA compiler  parameters & \multirow{2}{*}{Function  minimization methods
        with Online Learning} \\
      FPGA compiler parameters &  \\
      OpenCL  Laplacian  Kernel  &  Function  minimization  methods, Linear Models, Gaussian Process Regression \\
      SPAPT Kernels & Linear Models, Gaussian Process Regression \\
      CNN Mixed-Precision Quantization & Gaussian Process Regression
    \end{tabular}%
  }
\end{table}
#+end_export

** Reproducibility of Performance Tuning Experiments
- Redoing all the work for different problems
- Complementary approaches:
  - Completely evaluate small sets of a search space
  - Collaborative optimizing for different architectures, problems
** Key Discussions
*** Curse of Dimensionality for Autotuning Problems         :B_block:BMCOL:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_col: 0.5
:END:
- Implications for Sampling and Learning
- Space-filling helps, but does not solve
- Constraints
**** Which method to use?                                        :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
- Design  of  Experiments  for  transparency and  parsimony  when  building  and
  interpreting statistical models
- Linear models for simpler spaces and problems
- Gaussian Process Surrogates for more complex situations
*** It is often unclear if there is something to find       :B_block:BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:END:
- Abundance of local optima
- Is there a global optimum, is it "hidden"?
  - How to find it, if so? (can learning do it?)
- What is the most effective level of abstraction for optimizing a program?
  - Compiler, kernel, machine, model, dependencies?
- When to stop?
** Conclusion

* Ending Title :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \maketitle

* Emacs Setup                                      :noexport:B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
See [[LaTeX Setup]] above for the beamer configuration

** Use XeLaTeX
If you  accept this definition  when loading the  buffer, this variable  will be
modified  locally to  the buffer.  This allows  using XeLaTeX  for exporting  to
beamer pdf.

# Local Variables:
# eval: (setq-local org-latex-pdf-process (list "latexmk -xelatex -shell-escape %f"))
# End:
