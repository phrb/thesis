#+STARTUP: overview indent inlineimages logdrawer
#+TITLE: Towards Transparent and Parsimonious
#+TITLE: Methods for Automatic Performance Tuning
#+AUTHOR:      Pedro Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: ExportableReports(E)
#+TAGS: FAPESP(f)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS: book
#+LATEX_CLASS_OPTIONS: [11pt,twoside,a4paper]
#+LATEX_HEADER: \usepackage[a4paper]{geometry}
#+LATEX_HEADER: \geometry{
#+LATEX_HEADER:   %top=32mm,
#+LATEX_HEADER:   %bottom=28mm,
#+LATEX_HEADER:   %left=24mm,
#+LATEX_HEADER:   %right=34mm,
#+LATEX_HEADER:   textwidth=152mm, % 210-24-34
#+LATEX_HEADER:   textheight=237mm, % 297-32-28
#+LATEX_HEADER:   vmarginratio=8:7, % 32:28
#+LATEX_HEADER:   hmarginratio=12:17, % 24:34
#+LATEX_HEADER:   % Com geometry, esta medida não é tão relevante; basta garantir que ela
#+LATEX_HEADER:   % seja menor que "top" e que o texto do cabeçalho caiba nela.
#+LATEX_HEADER:   headheight=25.4mm,
#+LATEX_HEADER:   % distância entre o início do texto principal e a base do cabeçalho;
#+LATEX_HEADER:   % ou seja, o cabeçalho "invade" a margem superior nessa medida. Essa
#+LATEX_HEADER:   % é a medida que determina a posição do cabeçalho
#+LATEX_HEADER:   headsep=11mm,
#+LATEX_HEADER:   footskip=10mm,
#+LATEX_HEADER:   marginpar=20mm,
#+LATEX_HEADER:   marginparsep=5mm,
#+LATEX_HEADER: }
#+LATEX_HEADER: \widowpenalty=10000
#+LATEX_HEADER: \clubpenalty=10000
#+LATEX:HEADER: \usepackage[inline]{enumitem}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amssymb,amsthm}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{newpxtext}
#+LATEX_HEADER: \usepackage{newpxmath}
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \usepackage{titling}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-qtree}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstdefinelanguage{Julia}%
#+LATEX_HEADER:   {morekeywords={abstract,struct,break,case,catch,const,continue,do,else,elseif,%
#+LATEX_HEADER:       end,export,false,for,function,immutable,mutable,using,import,importall,if,in,%
#+LATEX_HEADER:       macro,module,quote,return,switch,true,try,catch,type,typealias,%
#+LATEX_HEADER:       while,<:,+,-,::,/},%
#+LATEX_HEADER:    sensitive=true,%
#+LATEX_HEADER:    alsoother={$},%
#+LATEX_HEADER:    morecomment=[l]\#,%
#+LATEX_HEADER:    morecomment=[n]{\#=}{=\#},%
#+LATEX_HEADER:    morestring=[s]{"}{"},%
#+LATEX_HEADER:    morestring=[m]{'}{'},%
#+LATEX_HEADER: }[keywords,comments,strings]%
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\tiny,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=b,
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   numbers=left,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=1,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \setlength{\parskip}{0.5em}
#+LATEX_HEADER: \usepackage[pagestyles,raggedright]{titlesec}
#+LATEX_HEADER: \titleformat{\chapter}[display]{\normalfont\bfseries}{}{0pt}{\Huge}
# #+LATEX_HEADER: \newpagestyle{mystyle}
# #+LATEX_HEADER: {\sethead[\thepage][][\chaptertitle]{}{}{\thepage}}
# #+LATEX_HEADER: \pagestyle{mystyle}

* Thesis Drafts                                                    :noexport:
** Structure Drafts
1. Introduction
   1. Autotuning
      1. Algorithm Selection Problem?
   2. Overview of Autotuning Methods (taxonomy/decision tree)
   3. Search Heuristics
      - Introduction
      - OpenTuner
      - Autotuning GPU compiler parameters
      - Autotuning High Level Synthesis for FPGAs
   4. Statistical Learning
      - Parametric, nonparametric
   5. Related Work
      - Literature Review
2. Design of Experiments
   1. Introduction
      1. Linear Regression
   2. Screening
      1. Main effects
      2. Example with CUDA flags
   3. Factorial Designs
      1. Example?
   4. Optimal Design
      1. Properties of the BLUE, Information Matrix
      2. Variance-optimizing criteria
      3. Example on Laplacian GPU
   5. Autotuning SPAPT Kernels
      - Mixing factor types
      - Sampling with Constraints
      - Heteroscedasticity
3. Gaussian Process Regression
   1. Introduction
      1. Bayesian Linear Model (Rasmussen's Book)
      2. EGO
   2. Revisiting SPAPT kernels
   3. Quantization for Deep Neural Networks
4. Conclusion
   - Expressing structure with kernels? (Duvenaud's thesis)
   - Performance of the Federov Algorithm for D-Optimal design construction?
*** Structure Draft
- Course on performance optimization for HPC, and why it's hard
- Difficulty to optimize programs comes from complexity in:
  - Computer architecture
    - Pursuit of doubling performance, fitting more transistors,
      (Moore's Law), and the end of frequency and power
      scaling (Dennard's),
      mean that we need parallel architectures, which are more complex
  - Software
    - Parallel architectures are harder to program efficiently
** Underlying Hypotheses of Autotuning Methods
:PROPERTIES:
:EXPORT_FILE_NAME: hipotheses.pdf
:END:
*** Introduction                                                 :noexport:
Given  a program  with $X  \in \mathcal{X}$  configurable parameters,  we want  to
choose the best parameter values according  to a performance metric given by the
function  $f(X)$.   Autotuning methods  attempt  find  the $X_{*}$  that  minimizes
$f(\cdot)$.   Despite  their different  approaches,  autotuning  methods share  some
common hypotheses:

- There is no knowledge about the global optimal configuration
- There could be some problem-specific knowledge to exploit
- Measuring the effects of a choice of parameter values is possible but costly

Each  autotuning method  has  assumptions that  justify  its implementation  and
usage. Some of  these hypotheses are explicit,  such as the ones  that come from
the  linear model.   Others are  implicit,  such as  the ones  that support  the
implementation and the justification of optimization heuristics.
*** Overview of Autotuning Methods
:PROPERTIES:
:EXPORT_TITLE:
:EXPORT_FILE_NAME: tree.pdf
:END:
#+begin_export latex
\begin{sidewaysfigure}[t]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      for tree={%
        anchor = north,
        align = center,
        l sep+=1em
      },
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
        draw,
        [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
          draw,
          color = NavyBlue
          [{Search Heuristics},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{\textbf{Random} \textbf{Sampling}}, draw]
            [{Reachable Optima},
              draw,
              color = BurntOrange
              [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
                draw,
                color = BurntOrange
                [{Strong $corr(f(X),d(X,X_{*}))$?},
                  draw,
                  color = NavyBlue
                  [{More Global},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                    [, phantom]]
                  [{More Local},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{High local optima density?},
                      draw,
                      color = NavyBlue
                      [{Exploit Steepest Descent},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                        [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                        [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                      [{Allows\\exploration},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                        [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                        [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
                [,phantom]]
              [,phantom]]]
          [{Statistical Learning},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{Parametric Learning},
              draw,
              color = BurntOrange
              [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                draw,
                color = BurntOrange
                [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                [, phantom]]
              [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                draw,
                color = BurntOrange
                [, phantom]
                [{Check for model adequacy?},
                  draw,
                  alias = adequacy,
                  color = NavyBlue
                  [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                    draw,
                    alias = interactions,
                    color = NavyBlue,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                      draw
                      [, phantom]
                      [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                        edge = {-stealth, ForestGreen, semithick},
                        edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                        draw,
                        alias = estimate,
                        color = ForestGreen]]
                    [{\textbf{Optimal} \textbf{Design}},
                      draw,
                      alias = optimal,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [{\textbf{Space-filling} \textbf{Designs}},
                    draw,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{Model selection},
                      edge = {-stealth, ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      alias = selection,
                      color = ForestGreen]]]]]
            [{Nonparametric Learning},
              draw,
              color = BurntOrange
              [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
              [{\textbf{Gaussian} \textbf{Process Regression}},
                alias = gaussian,
                draw]
              [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
      \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
      \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
      \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
    \end{forest}
  }
  \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
    denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
    denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
    denote algorithm choices, and \textbf{bold} boxes denote methods.}
\end{sidewaysfigure}
#+end_export

*** Previous Attempts                                            :noexport:
#+begin_export latex
\forestset{linebreaks/.style={for tree={align = center}}}
\begin{sidewaysfigure}
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      linebreaks
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\ $Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$}
        [{Does not construct\\estimate $Y = \hat{f}(\cdot, \theta{}(X))$}
          [{Reachable\\optima}
            [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$}
              [{Strong\\$corr(f(X),d(X,X_{*}))$}
                [{Low local\\optima density}
                  [{\textbf{Greedy}\\\textbf{Search}}, draw]
                  [{Estimate $f^{\prime}(X)$}
                    [{\textbf{Gradient}\\\textbf{Descent}}, draw]]]
                [{Introduce a ``population''\\$\mathbf{X} = (X_1,\dots,X_n)$}
                  [{Combination, mutation,\\within $\mathbf{X}$}
                    [{\textbf{Genetic}\\\textbf{Algorithms}}, draw]]
                  [{\textbf{Ant}\\\textbf{Colony}}, draw]]]
              [{Weaker\\$corr(f(X),d(X,X_{*}))$}
                [{Accept\\worst $f(X)$}
                  [{\textbf{Simulated}\\\textbf{Annealing}}, draw]]
                [{Avoid\\recent $X$}
                  [{\textbf{Tabu}\\\textbf{Search}}, draw]]]]]
          [{\textbf{Random}\\\textbf{Sampling}}, draw]]
        [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$}
          [{Parametric\\Learning}
            [{$\hat{f}(X) \approx f_1(X_1) + \dots + f_k(X_k)$}
              [{\textbf{Independent}\\\textbf{Bandit}}, draw]]
            [{$\hat{f}(X) = \mathcal{B}(logit(\mathcal{M}(X)\theta(X) + \varepsilon))$}
              [{\textbf{Logistic}\\\textbf{Regression}}, draw]]
            [{$\hat{f}(X) = \mathcal{M}(X)\theta(X) + \varepsilon$}
              [{\textbf{Linear}\\\textbf{Regression}}, draw]
              [{Measure\\properties of $X$}
                [{Independance\\of effects}
                  [{\textbf{Screening}}, draw]]
                [{Homoscedasticity of $\varepsilon$}
                  [{\textbf{Optimal}\\\textbf{Design}}, draw]]]]]
          [{Nonparametric\\Learning}
            [{Splitting\\rules on $X$}
              [{\textbf{Decision}\\\textbf{Trees}}, draw]]
            [{$\hat{f} = \mathcal{GP}(X; \mathcal{K})$}
              [{\textbf{Gaussian}\\\textbf{Process Regression}}, draw]]
            [{\textbf{Neural}\\\textbf{Networks}}, draw]
            [{\textbf{Multi-armed}\\\textbf{Bandit (?)}}, draw]]]]
    \end{forest}
  }
  \caption{Some hypothesis of some autotuning methods}
\end{sidewaysfigure}

#+end_export

#+begin_export latex
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\begin{table}[ht]
  \center
  \begin{tabular}{@{}p{0.3\textwidth}p{0.5\textwidth}@{}}
    \toprule
    Method &  Hypotheses \\ \midrule
    Metaheuristics & \tabitem There are similarities between natural fenomena and the target problem \\
    & \tabitem Gradual changes in configurations produce gradual changes in performance \\
    & \tabitem The optimal configuration is ``reachable'', by small changes, from non-optimal configurations  \\
    \addlinespace \\
    Machine Learning & \tabitem As more samples are obtained, decreases in ``out-of-sample error'' imply decreases ``in-sample error'' \\
    & \tabitem \textbf{TODO} What are the classes of models? \\
    \addlinespace \\
    Design of Experiments & \tabitem There is ``exploitable search space structure''\\
    & \tabitem Linear model: Response $\bm{Y}$ is an ``unobservable function'' of parameters $\bm{X}$: \\
    & \hspace{0.15\textwidth} $f(\bm{X}) = \bm{Y} = \bm{X\beta} + \bm{\varepsilon}$ \\
    & \tabitem Optimal Design: Variance of estimator $\hat{\bm{\beta}}$ is proportional to $\bm{X}$: \\
    & \hspace{0.15\textwidth} $\bm{\hat{\beta}} = \left(\bm{X}^{\intercal}\bm{X}\right)^{-1}\bm{X}^{\intercal}\bm{Y}$ \\
    \addlinespace \\
    Gaussian Process Regression & \tabitem Response $\bm{Y}$ is a sample from a multidimensional Gaussian distribution, with mean $m(\bf{X})$ and variance $k(\bm{X}, \bm{X}^{\intercal})$: \\
    & \hspace{0.1\textwidth} $\bm{Y} = f(\bm{X}) \sim \mathcal{N}(m(\bm{X}), k(\bm{X}, \bm{X}^{\intercal}))$ \\
    & \tabitem Predictions $\bm{Y_{*}}$ can be made conditioning distribution to observed data\\ \bottomrule
  \end{tabular}%
\end{table}
#+end_export

#+begin_export latex
\resizebox{!}{\textheight}{%
  \begin{tikzpicture}[rotate = -90]
    \begin{scope}
      \tikzset{every tree node/.style = {align = center}}
      \tikzset{level 1+/.style={level distance = 40pt}}
      \Tree [.\node(n0){Minimize $f: X \mapsto \mathbb{R}$ \\ $f(X) = f^{*}(X) + \varepsilon = m$};
        [.{Does not construct \\ estimate $\hat{f}(X; \theta)$}
          [.{Reachability of \\ optima}
            [.{\textbf{Greedy} \\ \textbf{Search}} ]
            [.{$d(x_i, x_j) \to 0$ $\implies$ \\ $d(f(x_i), f(x_j)) \to 0$}
              [.{Abundance of \\ local optima}
                [.{\textbf{Simulated} \\ \textbf{Annealing}} ]]
              [.{Closeness of a \\ ``population'' of $X$}
                [.{\textbf{Genetic} \\ \textbf{Algorithms}} ]]]]
          [.{\textbf{Random} \\ \textbf{Sampling}} ] ]
        [.\node(r1){Constructs surrogate \\ estimate $\hat{f}(X; \theta)$};
          [.{Explicit, variable \\ models of $\theta$}
            [.{$\hat{f} = M(X)\theta + \varepsilon$}
              [.{Independance \\ of effects}
                [.{\textbf{Screening}} ] ]
              [.{Homoscedasticity}
                [.{\textbf{Optimal} \\ \textbf{Design}} ] ] ] ]
          [.{Implicit, fixed \\ models of $\theta$}
            [.{\textbf{Neural Networks}} ] ]
          [.{Samples \\ functions}
            [.{$\hat{f} = \mathcal{GP}(X; \theta, \mathcal{K})$}
              [.{\textbf{Gaussian Process} \\ \textbf{Regression}} ] ] ] ] ]
    \end{scope}
    % \begin{scope}[thick]
    %   \draw [color = orange] (n0) to [bend left = 2] (r1);
    %   \draw [color = green] (n0) to [bend right = 2] (r1);
    % \end{scope}
  \end{tikzpicture}
}
#+end_export
** Application to the Microsoft Latin America PhD Award
*** Search Heuristics and Statistical Learning methods for Autotuning HPC Programs
:PROPERTIES:
:EXPORT_DATE:
:EXPORT_TITLE: @@latex: Search Heuristics and Statistical Learning \\ Methods for Program Autotuning@@
:EXPORT_FILE_NAME: application.pdf
:EXPORT_AUTHOR: Pedro Bruel
:END:

#+latex: \vspace{-4em}

High Performance Computing  has been a cornerstone of  collective scientific and
industrial progress  for at least  five decades.   Paying the cost  of increased
complexity,  software and  hardware  engineering advances  continue to  overcome
several challenges on the way of the sustained performance improvements observed
during the last  fifty years.  This mounting complexity means  that reaching the
advertised hardware  performance for  a given program  requires not  only expert
knowledge  of a  given hardware  architecture, but  also mastery  of programming
models  and  languages for  parallel  and  distributed  computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance,  we  can  draw   and  adapt  proven  methods  from  search,
mathematical  optimization and  statistics. The  effectiveness of  these adapted
methods  on autotuning  problems varies  greatly,  and hinges  on practical  and
mathematical properties of the problem and the corresponding /search space/.

When  adapting methods  for autotuning,  we must  face challenges  emerging from
practical properties  such as restricted  time and cost budgets,  constraints on
feasible  parameter values,  and the  need to  mix /categorical/,  /continuous/, and
/discrete/ parameters. To achieve useful results, we must also choose methods that
make hypotheses compatible with problem search  spaces, such as the existence of
discoverable,  or at  least  exploitable, relationships  between parameters  and
performance.   Choosing  an autotuning  method  requires  determining a  balance
between the exploration of a problem, when we would seek to discover and explain
relationships between  parameters and performance,  and the exploitation  of the
best optimizations we can find, when we would seek only to minimize performance.

The    effectiveness   of    search    heuristics   on    autotuning   can    be
limited\nbsp{}\cite{seymour2008comparison,balaprakash2011can,balaprakash2012experimental},
between other factors, by underlying hypotheses  about the search space, such as
the  reachability of  the  global optimum  and the  smoothness  of search  space
surfaces, which  are frequently not  respected. The derivation  of relationships
between  parameters  and  performance  from search  heuristic  optimizations  is
greatly hindered,  if not rendered impossible,  by the biased way  these methods
explore  parameters.   Some  parametric  learning methods,  such  as  Design  of
Experiments,  are  not widely  applied  to  autotuning.  These  methods  perform
structured  parameter  exploration,  and  can  be used  to  build  and  validate
performance     models,     generating    transparent     and     cost-effective
optimizations\nbsp{}\cite{mametjanov2015autotuning,bruel2019autotuning}. Other methods
from   the  parametric   family   are   more  widely   used,   such  as   Bandit
Algorithms\nbsp{}\cite{xu2017parallel}.   Nonparametric  learning   methods,  such  as
Decision    Trees\nbsp{}\cite{balaprakash2016automomml}     and    Gaussian    Process
Regression\nbsp{}\cite{parsa2019pabo}, are able  to reduce model bias  greatly, at the
expense of increased prediction variance. Figure\nbsp{}\ref{fig:tree} categorizes some
autotuning  methods  according to  some  of  the  key hypotheses  and  branching
questions underlying each method.

During this  thesis I have  adapted and  studied the effectiveness  of different
search heuristics and statistical learning  methods on optimizing performance on
several autotuning domains.  During the beginning of my PhD at the University of
São Paulo (USP), I have published a paper on optimizing the configuration of the
CUDA compiler\nbsp{}\cite{bruel2017autotuning},  where we have  reached up to  4 times
performance improvement  in comparison with a  high-level compiler optimization.
In collaboration with researchers from  Hewlett Packard Enterprise (HPE) in Palo
Alto, I wrote a  paper on the autotuning of a  compiler for High-Level Synthesis
for FPGAs\nbsp{}\cite{bruel2017autotuninghls}, where we  have reached, on average, 25%
improvements on  performance, size, and  complexity of  designs.

At the  end of 2017,  I joined  the /cotutelle/ PhD  program at the  University of
Grenoble Alpes  (UGA) and  became a member  of the POLARIS  Inria team,  where I
applied  Design  of   Experiments  to  the  autotuning   of  a  source-to-source
transformation  compiler\nbsp{}\cite{bruel2019autotuning},  where  we  showed  we  can
achieve significant speedup by exploiting  search space structure using a strict
budget.   I also  have  collaborated with  HPE on  another  paper, providing  an
analysis  of the  applicability  of autotuning  methods  to a  Hardware-Software
Co-design  problem\nbsp{}\cite{bruel2017generalize}.   During  my  Teaching  Assistant
internships,  I  have  published one  paper\nbsp{}\cite{bruel2017openmp}  on  parallel
programming  teaching, and  collaborated on  another\nbsp{}\cite{goncalves2016openmp},
where we showed that teaching lower level programming models, despite being more
challenging at first, provides a stronger core understanding.

I continue to collaborate with HPE  researchers on the application of autotuning
methods to  optimize Neural Networks,  hardware accelerators for  Deep Learning,
and  algorithms  for dealing  with  network  congestion.   With my  advisors,  I
currently manage  1 undergraduate and 4  masters students, who are  applying the
statistical  learning autotuning  methods  I studied  and  adapted to  different
domains  in the  context of  a joint  USP/HPE research  project.  I  am strongly
motivated to continue pursuing a career  on Computer Science research, aiming to
produce  rigorous and  value-adding  contributions. I  hereby  submit my  thesis
proposal and application to the Microsoft Latin America PhD Award.
#+begin_export latex
\begin{center}
  \begin{figure}[t]
    \resizebox{.9\textwidth}{!}{%
      \begin{forest}
        for tree={%
          anchor = north,
          align = center,
          l sep+=1em
        },
        [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
          draw,
          [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
            draw,
            color = NavyBlue
            [{Search Heuristics},
              draw,
              color = BurntOrange,
              edge label = {node[midway, fill=white, font = \scriptsize]{No}}
              [{\textbf{Random} \textbf{Sampling}}, draw]
              [{Reachable Optima},
                draw,
                color = BurntOrange
                [, phantom]
                [{Underlying Hypotheses \\ \textbf{Heuristics}}, draw]]]
            [{Statistical Learning},
              draw,
              color = BurntOrange,
              edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
              [{Parametric Learning},
                draw,
                color = BurntOrange
                [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                  draw,
                  color = BurntOrange
                  [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                  [, phantom]]
                [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                  draw,
                  color = BurntOrange
                  [, phantom]
                  [{Check for model adequacy?},
                    draw,
                    alias = adequacy,
                    color = NavyBlue
                    [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                      draw,
                      alias = interactions,
                      color = NavyBlue,
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                      [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                        draw
                        [, phantom]
                        [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                          edge = {-stealth, ForestGreen, semithick},
                          edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                          draw,
                          alias = estimate,
                          color = ForestGreen]]
                      [{\textbf{Optimal} \textbf{Design}},
                        draw,
                        alias = optimal,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [{\textbf{Space-filling} \textbf{Designs}},
                      draw,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                      [, phantom]
                      [{Model selection},
                        edge = {-stealth, ForestGreen, semithick},
                        edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                        draw,
                        alias = selection,
                        color = ForestGreen]]]]]
              [{Nonparametric Learning},
                draw,
                color = BurntOrange
                [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
                [{\textbf{Gaussian} \textbf{Process Regression}},
                  alias = gaussian,
                  draw]
                [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
        \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
        \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
        \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
        \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
      \end{forest}
    }
    \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
      denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
      denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
      highlight exploration and exploitation choices, and \textbf{bold} boxes denote methods.}
    \label{fig:tree}
  \end{figure}
\end{center}
#+end_export

#+latex: \newpage

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
*** (Short) Search Heuristics and Statistical Learning methods for Autotuning HPC Programs
:PROPERTIES:
:EXPORT_DATE:
:EXPORT_TITLE: @@latex: Search Heuristics and Statistical Learning \\ Methods for Program Autotuning@@
:EXPORT_FILE_NAME: short-application.pdf
:EXPORT_AUTHOR: Pedro Bruel
:END:

#+latex: \vspace{-3em}

High Performance Computing  has been a cornerstone of  collective scientific and
industrial progress  for at least  five decades.   Paying the cost  of increased
complexity,  software and  hardware  engineering advances  continue to  overcome
several challenges on the way of the sustained performance improvements observed
during the last  fifty years.  This mounting complexity means  that reaching the
advertised hardware  performance for  a given program  requires not  only expert
knowledge  of a  given hardware  architecture, but  also mastery  of programming
models  and  languages for  parallel  and  distributed  computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance,  we  can  draw   and  adapt  proven  methods  from  search,
mathematical  optimization and  statistics. The  effectiveness of  these adapted
methods  on autotuning  problems varies  greatly,  and hinges  on practical  and
mathematical properties of the problem and the corresponding /search space/.

When  adapting methods  for autotuning,  we must  face challenges  emerging from
practical properties  such as restricted  time and cost budgets,  constraints on
feasible  parameter values,  and the  need to  mix /categorical/,  /continuous/, and
/discrete/ parameters. To achieve useful results, we must also choose methods that
make hypotheses compatible with problem search  spaces, such as the existence of
discoverable,  or at  least  exploitable, relationships  between parameters  and
performance.   Choosing  an autotuning  method  requires  determining a  balance
between the exploration of a problem, when we would seek to discover and explain
relationships between  parameters and performance,  and the exploitation  of the
best optimizations we can find, when we would seek only to minimize performance.

During this  thesis I have  adapted and  studied the effectiveness  of different
search heuristics and statistical learning  methods on optimizing performance on
several autotuning domains.  During the beginning of my PhD at the University of
São Paulo (USP), I have published a paper on optimizing the configuration of the
CUDA compiler\nbsp{}\cite{bruel2017autotuning},  where we have  reached up to  4 times
performance improvement  in comparison with a  high-level compiler optimization.
In collaboration with researchers from  Hewlett Packard Enterprise (HPE) in Palo
Alto, I wrote a  paper on the autotuning of a  compiler for High-Level Synthesis
for FPGAs\nbsp{}\cite{bruel2017autotuninghls}, where we  have reached, on average, 25%
improvements on  performance, size, and  complexity of  designs.

At the  end of 2017,  I joined  the /cotutelle/ PhD  program at the  University of
Grenoble Alpes  (UGA) and  became a member  of the POLARIS  Inria team,  where I
applied  Design  of   Experiments  to  the  autotuning   of  a  source-to-source
transformation  compiler\nbsp{}\cite{bruel2019autotuning},  where  we  showed  we  can
achieve significant speedup by exploiting  search space structure using a strict
budget.   I also  have  collaborated with  HPE on  another  paper, providing  an
analysis  of the  applicability  of autotuning  methods  to a  Hardware-Software
Co-design  problem\nbsp{}\cite{bruel2017generalize}.

I continue to collaborate with HPE  researchers on the application of autotuning
methods to  optimize Neural Networks,  hardware accelerators for  Deep Learning,
and  algorithms  for dealing  with  network  congestion.   With my  advisors,  I
currently manage  1 undergraduate and 4  masters students, who are  applying the
statistical  learning autotuning  methods  I studied  and  adapted to  different
domains  in the  context of  a joint  USP/HPE research  project.  I  am strongly
motivated to continue pursuing a career  on Computer Science research, aiming to
produce  rigorous and  value-adding  contributions. I  hereby  submit my  thesis
proposal and application to the Microsoft Latin America PhD Award.

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
* Generating Figures                                               :noexport:
** Chapter 1
*** 49 Years of Processor Data
**** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)
df_freq <- read.csv("data/wiki_data/frequency.csv", header = TRUE)
df_transistor <- read.csv("data/wiki_data/transistor_count.csv", header = TRUE)
#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_freq)
#+end_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  12 variables:
 $ date               : int  1971 1972 1972 1972 1972 1973 1973 1973 1974 1974 ...
 $ name               : chr  "4004" "PPS-25" "μPD700" "8008" ...
 $ designer           : chr  "Intel" "Fairchild" "NEC" "Intel" ...
 $ max_clock_khz      : int  740 400 NA 500 200 NA NA NA 715 NA ...
 $ max_clock_mhz      : num  NA NA NA NA NA 2 1 1 NA 2 ...
 $ max_clock_ghz      : num  NA NA NA NA NA NA NA NA NA NA ...
 $ process_micro_m    : num  10 NA NA 10 NA 7.5 6 NA NA 6 ...
 $ process_nm         : int  NA NA NA NA NA NA NA NA NA NA ...
 $ chips              : int  1 2 1 1 1 1 1 1 3 1 ...
 $ transistor_count   : int  2250 NA NA 3500 NA 2500 2800 NA NA 6000 ...
 $ transistor_millions: num  NA NA NA NA NA NA NA NA NA NA ...
 $ logical_cores      : int  1 1 1 1 1 1 1 1 1 1 ...
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_transistor)
#+end_SRC

#+RESULTS:
: 'data.frame':	151 obs. of  6 variables:
:  $ name            : chr  "Intel 4004 " "Intel 8008 " "Toshiba TLCS-12 " "Intel 4040 " ...
:  $ transistor_count: num  2250 3500 11000 3000 4100 ...
:  $ date            : int  1971 1972 1973 1974 1974 1974 1974 1975 1976 1976 ...
:  $ designer        : chr  "Intel" "Intel" "Toshiba" "Intel" ...
:  $ process_nm      : int  10000 10000 6000 10000 6000 6000 8000 8000 5000 4000 ...
:  $ area_mm         : num  12 14 32 12 16 20 11 21 27 18 ...

**** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/49_years_processor_data.pdf" :width 10 :height 5 :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")

point_alpha = 0.9
line_alpha = 0.4
point_size = 2

shapes = c(0, 1, 2, 5)

ggplot() +
    # geom_line(data = df_transistor,
    #           size = point_size,
    #           stat = "smooth",
    #           method = "lm",
    #           alpha = line_alpha,
    #           formula = y ~ x + I(x ^ 2),
    #           aes(x = date,
    #               y = process_nm,
    #               color = "Process (nanometers)"),
    #           show.legend = FALSE) +
    # geom_line(data = df_freq,
    #           size = point_size,
    #           stat = "smooth",
    #           method = "lm",
    #           alpha = line_alpha,
    #           formula = y ~ x + I(x ^ 2) + I(x ^ 3),
    #           aes(x = date,
    #               y = logical_cores,
    #               shape = "Logical Cores (Count)",
    #               color = "Logical Cores (Count)"),
    #           show.legend = FALSE) +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_micro_m * 1e3,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = logical_cores,
                   shape = "Logical Cores (Count)",
                   color = "Logical Cores (Count)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_khz * 1e-3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_mhz,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_ghz * 1e3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_millions * 1e3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    xlab("Year") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 18) +
    theme(axis.title.y = element_blank(),
          legend.position = c(0.14, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 14),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/49_years_processor_data.pdf]]
*** TOP500
**** Loading Data and Packages
Load the /csv/:

#+begin_SRC R :results output :session *R* :exports code :eval no-export
library(dplyr)
library(tidyr)
library(ggplot2)

df <- read.csv("./data/top500/TOP500_history.csv")
#+end_SRC

#+RESULTS:
**** Looking at Data
***** Column Names
We  have many  columns  filled with  `NA`s,  due to  how  metrics were  measured
differently over the years. There's data from 1993 to 2019!

#+begin_SRC R :results output :session *R* :exports both :eval no-export
names(df)
#+end_SRC

#+RESULTS:
#+begin_example
 [1] "Year"                            "Month"
 [3] "Day"                             "Rank"
 [5] "Site"                            "Manufacturer"
 [7] "Computer"                        "Country"
 [9] "Processors"                      "RMax"
[11] "RPeak"                           "Nmax"
[13] "Nhalf"                           "Processor.Family"
[15] "Processor"                       "Processor.Speed..MHz."
[17] "System.Family"                   "Operating.System"
[19] "Architecture"                    "Segment"
[21] "Application.Area"                "Interconnect.Family"
[23] "Interconnect"                    "Region"
[25] "Continent"                       "Power"
[27] "System.Model"                    "Total.Cores"
[29] "Measured.Size"                   "Processor.Cores"
[31] "Accelerator"                     "Name"
[33] "Accelerator.Cores"               "Efficiency...."
[35] "Mflops.Watt"                     "Processor.Technology"
[37] "OS.Family"                       "Cores.per.Socket"
[39] "Processor.Generation"            "Previous.Rank"
[41] "First.Appearance"                "First.Rank"
[43] "Accelerator.Co.Processor.Cores"  "Accelerator.Co.Processor"
[45] "Power.Source"                    "Rmax..TFlop.s."
[47] "Rpeak..TFlop.s."                 "HPCG..TFlop.s."
[49] "Power..kW."                      "Power.Effeciency..GFlops.Watts."
[51] "Site.ID"                         "System.ID"
#+end_example

***** Achieved and Theoretical Performance
#+begin_SRC R :results graphics output :session *R* :file "./img/top500_rmax_rpeak.pdf" :width 10 :height 5 :exports both :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")

point_size = 2.8
shapes = c(0, 1, 2, 5)

plot_df <- df %>%
    filter(Rank <= 1) %>%
    mutate(RMaxT = coalesce(RMax / 1e3, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak / 1e3, Rpeak..TFlop.s.),
           Power = coalesce(Power, Power..kW.)) %>%
    select(Rank,
           Year,
           Power,
           RMaxT,
           RPeakT) %>%
    distinct(Rank, Year, .keep_all = TRUE) %>%
    mutate(Ratio = RMaxT / RPeakT) %>%
    filter(is.finite(Ratio) & Ratio <= 1.0)

ggplot() +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RMaxT,
                   shape = "RMax",
                   color = "RMax")) +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RPeakT,
                   shape = "RPeak",
                   color = "RPeak")) +
    # geom_point(data = plot_df,
    #            size = point_size,
    #            aes(x = Year,
    #                y = Power,
    #                shape = "Power (kW)",
    #                color = "Power (kW)")) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    ylab("Tflops/s") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 20) +
    theme(legend.position = c(0.06, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 16),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/top500_rmax_rpeak.pdf]]

***** Accelerator Core Count
#+begin_SRC R :results graphics output :session *R* :file "./img/top500_accelerator_cores.pdf" :width 10 :height 5 :exports both :eval no-export
library(ggplot2)
library(extrafont)
library(scales)
library(tidyr)

loadfonts(device = "postscript")

point_size = 2.8
shapes = c(0, 1, 2, 5)

plot_df <- df %>%
    filter(Rank <= 1) %>%
    mutate(Accelerators = na_if(Accelerator.Co.Processor.Cores, 0),
           Cores = coalesce(Processors, Total.Cores) -
               replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    select(Rank,
           Year,
           Accelerators,
           Cores) %>%
    distinct(Rank, Year, .keep_all = TRUE)

ggplot() +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = Cores,
                   shape = "Processor",
                   color = "Processor")) +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = Accelerators,
                   shape = "Accelerator",
                   color = "Accelerator")) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    ylab("Cores") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 20) +
    theme(legend.position = c(0.09, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 16),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/top500_accelerator_cores.pdf]]
***** Other Plots
****** Processor Clock
Supercomputer  clock  explosion  and  range  broadening.  Even  top-tier  clocks
stagnate after 2008.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_processors_clock.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Processor.Speed..MHz. / 1000,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
                                        #scale_y_log10() +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Processor Clock (GHz)") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_processors_clock.pdf]]

****** Processors
Core count sustained  exponential increase.  Although top-tier  core count still
increases, range  broadening around  2012 can be  explained by  introduction and
ubiquity of accelerator cores on all tiers.

#+begin_SRC R :results graphics output :session *R* :file "./img/top500_total_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)
library(tidyr)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores) - replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    select(Rank, Year, AllCores, Accelerator.Co.Processor.Cores) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("AllCores",
                                    "Accelerator.Co.Processor.Cores"),
                         labels = c("Processor Cores",
                                    "Accelerator Cores"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Core Count") +
    scale_y_log10() +
    # annotation_logticks(sides = "l") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.67, 0.08),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 4)
#+end_SRC

#+RESULTS:
[[file:./img/top500_total_cores.pdf]]
****** RPeak and RMax
Sustained increase of theoretical peak and  achieved max performance on HPL and,
most recently,  on the  HPCG benchmark.  RPeak does not  guarantee rank  on some
cases.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rpeak.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(RMax = RMax / 1e3,
           RPeak = RPeak / 1e3,
           RMaxT = coalesce(RMax, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak, Rpeak..TFlop.s.)) %>%
    select(Rank,
           Year,
           RMaxT,
           RPeakT,
           HPCG..TFlop.s.) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakT",
                                    "RMaxT",
                                    "HPCG..TFlop.s."),
                         labels = c("RPeak (HPL)",
                                    "RMax (HPL)",
                                    "RMax (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance (TFlops/s)") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.83, 0.09),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 3)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rpeak.pdf]]
****** RMax / Cores
Ratio of performance and core count, for HPL and HPCG. Is this sustained increase due only to accelerator cores, or are there other engineering and software advances?
#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rmax_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores)) %>%
    mutate(RMax = (RMax / 1e3) / AllCores,
           RPeak = (RPeak / 1e3) / AllCores,
           Rmax..TFlop.s. = Rmax..TFlop.s. / AllCores,
           Rpeak..TFlop.s. = Rpeak..TFlop.s. / AllCores,
           RMaxC = coalesce(RMax, Rmax..TFlop.s.),
           RPeakC = coalesce(RPeak, Rpeak..TFlop.s.),
           HPCGC = HPCG..TFlop.s. / AllCores) %>%
    select(Rank,
           Year,
           RMaxC,
           RPeakC,
           HPCGC) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakC",
                                    "RMaxC",
                                    "HPCGC"),
                         labels = c("RPeak / Cores (HPL)",
                                    "RMax / Cores (HPL)",
                                    "RMax / Cores (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance / Core Count") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.85, 0.1),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          strip.text.x = element_text(size = 28),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 5)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rmax_cores.pdf]]

****** NMax
Exponential increase of problem size to reach max performance. Why is there
range broadening after 2011?

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_nmax.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Nmax,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Problem Size to Reach RMax") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_nmax.pdf]]

*** Search Spaces
**** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(extrafont)

df_search_spaces <- read.csv("data/search_spaces/search_spaces.csv")

loadfonts(device = "postscript")
#+end_SRC

#+RESULTS:
#+begin_example

Akaash already registered with postscriptFonts().
AkrutiMal1 already registered with postscriptFonts().
AkrutiMal2 already registered with postscriptFonts().
AkrutiTml1 already registered with postscriptFonts().
AkrutiTml2 already registered with postscriptFonts().
Anonymice Powerline already registered with postscriptFonts().
Arimo for Powerline already registered with postscriptFonts().
Bitstream Vera Sans already registered with postscriptFonts().
Bitstream Vera Sans Mono already registered with postscriptFonts().
Bitstream Vera Serif already registered with postscriptFonts().
Cousine for Powerline already registered with postscriptFonts().
IBM 3270 already registered with postscriptFonts().
IBM 3270 Narrow already registered with postscriptFonts().
IBM 3270 Semi-Narrow already registered with postscriptFonts().
DejaVu Math TeX Gyre already registered with postscriptFonts().
DejaVu Sans already registered with postscriptFonts().
DejaVu Sans Light already registered with postscriptFonts().
DejaVu Sans Condensed already registered with postscriptFonts().
DejaVu Sans Mono already registered with postscriptFonts().
DejaVu Sans Mono for Powerline already registered with postscriptFonts().
DejaVu Serif already registered with postscriptFonts().
DejaVu Serif Condensed already registered with postscriptFonts().
Droid Arabic Kufi already registered with postscriptFonts().
Droid Arabic Naskh already registered with postscriptFonts().
Droid Naskh Shift Alt already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans. Skipping setup for this font.
Droid Sans Arabic already registered with postscriptFonts().
Droid Sans Armenian already registered with postscriptFonts().
Droid Sans Devanagari already registered with postscriptFonts().
Droid Sans Ethiopic already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Fallback. Skipping setup for this font.
Droid Sans Georgian already registered with postscriptFonts().
Droid Sans Hebrew already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Mono. Skipping setup for this font.
Droid Sans Mono Dotted for Powerline already registered with postscriptFonts().
Droid Sans Mono Slashed for Powerline already registered with postscriptFonts().
Droid Sans Tamil already registered with postscriptFonts().
Droid Sans Thai already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Serif. Skipping setup for this font.
Font Awesome 5 Brands Regular already registered with postscriptFonts().
Font Awesome 5 Free Regular already registered with postscriptFonts().
Font Awesome 5 Free Solid already registered with postscriptFonts().
Gargi-1.2b already registered with postscriptFonts().
Goha-Tibeb Zemen already registered with postscriptFonts().
Go Mono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for GurbaniBoliLite. Skipping setup for this font.
Hack already registered with postscriptFonts().
Inconsolata Black already registered with postscriptFonts().
Inconsolata already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Inconsolata for Powerline. Skipping setup for this font.
Inconsolata Condensed already registered with postscriptFonts().
Inconsolata Condensed Black already registered with postscriptFonts().
Inconsolata Condensed Bold already registered with postscriptFonts().
Inconsolata Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Condensed Light already registered with postscriptFonts().
Inconsolata Condensed Medium already registered with postscriptFonts().
Inconsolata Condensed SemiBold already registered with postscriptFonts().
Inconsolata Expanded already registered with postscriptFonts().
Inconsolata Expanded Black already registered with postscriptFonts().
Inconsolata Expanded Bold already registered with postscriptFonts().
Inconsolata Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Expanded Light already registered with postscriptFonts().
Inconsolata Expanded Medium already registered with postscriptFonts().
Inconsolata Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed already registered with postscriptFonts().
Inconsolata Extra Condensed Black already registered with postscriptFonts().
Inconsolata Extra Condensed Bold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Extra Condensed Light already registered with postscriptFonts().
Inconsolata Extra Condensed Medium already registered with postscriptFonts().
Inconsolata Extra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Extra Expanded already registered with postscriptFonts().
Inconsolata Extra Expanded Black already registered with postscriptFonts().
Inconsolata Extra Expanded Bold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Extra Expanded Light already registered with postscriptFonts().
Inconsolata Extra Expanded Medium already registered with postscriptFonts().
Inconsolata Extra Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraLight already registered with postscriptFonts().
Inconsolata Light already registered with postscriptFonts().
Inconsolata Medium already registered with postscriptFonts().
Inconsolata SemiBold already registered with postscriptFonts().
Inconsolata Semi Condensed already registered with postscriptFonts().
Inconsolata Semi Condensed Black already registered with postscriptFonts().
Inconsolata Semi Condensed Bold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Semi Condensed Light already registered with postscriptFonts().
Inconsolata Semi Condensed Medium already registered with postscriptFonts().
Inconsolata Semi Condensed SemiBold already registered with postscriptFonts().
Inconsolata Semi Expanded already registered with postscriptFonts().
Inconsolata Semi Expanded Black already registered with postscriptFonts().
Inconsolata Semi Expanded Bold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Semi Expanded Light already registered with postscriptFonts().
Inconsolata Semi Expanded Medium already registered with postscriptFonts().
Inconsolata Semi Expanded SemiBold already registered with postscriptFonts().
Inconsolata Ultra Condensed already registered with postscriptFonts().
Inconsolata Ultra Condensed Black already registered with postscriptFonts().
Inconsolata Ultra Condensed Bold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Condensed Light already registered with postscriptFonts().
Inconsolata Ultra Condensed Medium already registered with postscriptFonts().
Inconsolata Ultra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Ultra Expanded already registered with postscriptFonts().
Inconsolata Ultra Expanded Black already registered with postscriptFonts().
Inconsolata Ultra Expanded Bold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Expanded Light already registered with postscriptFonts().
Inconsolata Ultra Expanded Medium already registered with postscriptFonts().
Inconsolata Ultra Expanded SemiBold already registered with postscriptFonts().
Liberation Mono already registered with postscriptFonts().
Liberation Sans already registered with postscriptFonts().
Liberation Serif already registered with postscriptFonts().
Ligconsolata already registered with postscriptFonts().
Likhan already registered with postscriptFonts().
Literation Mono Powerline already registered with postscriptFonts().
malayalam already registered with postscriptFonts().
MalOtf already registered with postscriptFonts().
Meslo LG L DZ for Powerline already registered with postscriptFonts().
Meslo LG L for Powerline already registered with postscriptFonts().
Meslo LG M DZ for Powerline already registered with postscriptFonts().
Meslo LG M for Powerline already registered with postscriptFonts().
Meslo LG S DZ for Powerline already registered with postscriptFonts().
Meslo LG S for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for monofur for Powerline. Skipping setup for this font.
More than one version of regular/bold/italic found for Mukti Narrow. Skipping setup for this font.
Noto Kufi Arabic already registered with postscriptFonts().
Noto Kufi Arabic Medium already registered with postscriptFonts().
Noto Kufi Arabic Semi bold already registered with postscriptFonts().
Noto Mono for Powerline already registered with postscriptFonts().
Noto Music already registered with postscriptFonts().
Noto Naskh Arabic already registered with postscriptFonts().
Noto Naskh Arabic UI already registered with postscriptFonts().
Noto Nastaliq Urdu already registered with postscriptFonts().
Noto Sans Black already registered with postscriptFonts().
Noto Sans already registered with postscriptFonts().
Noto Sans Light already registered with postscriptFonts().
Noto Sans Medium already registered with postscriptFonts().
Noto Sans Thin already registered with postscriptFonts().
Noto Sans Adlam already registered with postscriptFonts().
Noto Sans Adlam Unjoined already registered with postscriptFonts().
Noto Sans AnatoHiero already registered with postscriptFonts().
Noto Sans Arabic Blk already registered with postscriptFonts().
Noto Sans Arabic already registered with postscriptFonts().
Noto Sans Arabic Light already registered with postscriptFonts().
Noto Sans Arabic Med already registered with postscriptFonts().
Noto Sans Arabic Thin already registered with postscriptFonts().
Noto Sans Arabic UI Bk already registered with postscriptFonts().
Noto Sans Arabic UI already registered with postscriptFonts().
Noto Sans Arabic UI Lt already registered with postscriptFonts().
Noto Sans Arabic UI Md already registered with postscriptFonts().
Noto Sans Arabic UI Th already registered with postscriptFonts().
Noto Sans Armenian Blk already registered with postscriptFonts().
Noto Sans Armenian already registered with postscriptFonts().
Noto Sans Armenian Light already registered with postscriptFonts().
Noto Sans Armenian Med already registered with postscriptFonts().
Noto Sans Armenian Thin already registered with postscriptFonts().
Noto Sans Avestan already registered with postscriptFonts().
Noto Sans Bamum already registered with postscriptFonts().
Noto Sans Bassa Vah already registered with postscriptFonts().
Noto Sans Batak already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Blk. Skipping setup for this font.
Noto Sans Bengali already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Thin. Skipping setup for this font.
Noto Sans Bengali UI already registered with postscriptFonts().
Noto Sans Bhaiksuki already registered with postscriptFonts().
Noto Sans Brahmi already registered with postscriptFonts().
Noto Sans Buginese already registered with postscriptFonts().
Noto Sans Buhid already registered with postscriptFonts().
Noto Sans CanAborig Bk already registered with postscriptFonts().
Noto Sans CanAborig already registered with postscriptFonts().
Noto Sans CanAborig Lt already registered with postscriptFonts().
Noto Sans CanAborig Md already registered with postscriptFonts().
Noto Sans CanAborig Th already registered with postscriptFonts().
Noto Sans Carian already registered with postscriptFonts().
Noto Sans CaucAlban already registered with postscriptFonts().
Noto Sans Chakma already registered with postscriptFonts().
Noto Sans Cham Blk already registered with postscriptFonts().
Noto Sans Cham already registered with postscriptFonts().
Noto Sans Cham Light already registered with postscriptFonts().
Noto Sans Cham Med already registered with postscriptFonts().
Noto Sans Cham Thin already registered with postscriptFonts().
Noto Sans Cherokee Blk already registered with postscriptFonts().
Noto Sans Cherokee already registered with postscriptFonts().
Noto Sans Cherokee Light already registered with postscriptFonts().
Noto Sans Cherokee Med already registered with postscriptFonts().
Noto Sans Cherokee Thin already registered with postscriptFonts().
Noto Sans Coptic already registered with postscriptFonts().
Noto Sans Cuneiform already registered with postscriptFonts().
Noto Sans Cypriot already registered with postscriptFonts().
Noto Sans Deseret already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Bk. Skipping setup for this font.
Noto Sans Devanagari already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Lt. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Md. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Th. Skipping setup for this font.
Noto Sans Devanagari UI already registered with postscriptFonts().
Noto Sans Display Black already registered with postscriptFonts().
Noto Sans Display already registered with postscriptFonts().
Noto Sans Display Light already registered with postscriptFonts().
Noto Sans Display Medium already registered with postscriptFonts().
Noto Sans Display Thin already registered with postscriptFonts().
Noto Sans Duployan already registered with postscriptFonts().
Noto Sans EgyptHiero already registered with postscriptFonts().
Noto Sans Elbasan already registered with postscriptFonts().
Noto Sans Ethiopic Blk already registered with postscriptFonts().
Noto Sans Ethiopic already registered with postscriptFonts().
Noto Sans Ethiopic Light already registered with postscriptFonts().
Noto Sans Ethiopic Med already registered with postscriptFonts().
Noto Sans Ethiopic Thin already registered with postscriptFonts().
Noto Sans Georgian Blk already registered with postscriptFonts().
Noto Sans Georgian already registered with postscriptFonts().
Noto Sans Georgian Light already registered with postscriptFonts().
Noto Sans Georgian Med already registered with postscriptFonts().
Noto Sans Georgian Thin already registered with postscriptFonts().
Noto Sans Glagolitic already registered with postscriptFonts().
Noto Sans Gothic already registered with postscriptFonts().
Noto Sans Grantha already registered with postscriptFonts().
Noto Sans Gujarati already registered with postscriptFonts().
Noto Sans Gujarati UI already registered with postscriptFonts().
Noto Sans Gurmukhi Black already registered with postscriptFonts().
Noto Sans Gurmukhi already registered with postscriptFonts().
Noto Sans Gurmukhi Light already registered with postscriptFonts().
Noto Sans Gurmukhi Medium already registered with postscriptFonts().
Noto Sans Gurmukhi Thin already registered with postscriptFonts().
Noto Sans Gurmukhi UI Black already registered with postscriptFonts().
Noto Sans Gurmukhi UI already registered with postscriptFonts().
Noto Sans Gurmukhi UI Light already registered with postscriptFonts().
Noto Sans Gurmukhi UI Medium already registered with postscriptFonts().
Noto Sans Gurmukhi UI Thin already registered with postscriptFonts().
Noto Sans HanifiRohg already registered with postscriptFonts().
Noto Sans Hanunoo already registered with postscriptFonts().
Noto Sans Hatran already registered with postscriptFonts().
Noto Sans Hebrew Blk already registered with postscriptFonts().
Noto Sans Hebrew already registered with postscriptFonts().
Noto Sans Hebrew Light already registered with postscriptFonts().
Noto Sans Hebrew Med already registered with postscriptFonts().
Noto Sans Hebrew Thin already registered with postscriptFonts().
Noto Sans ImpAramaic already registered with postscriptFonts().
Noto Sans Indic Siyaq Numbers already registered with postscriptFonts().
Noto Sans InsPahlavi already registered with postscriptFonts().
Noto Sans InsParthi already registered with postscriptFonts().
Noto Sans Javanese already registered with postscriptFonts().
Noto Sans Kaithi already registered with postscriptFonts().
Noto Sans Kannada Black already registered with postscriptFonts().
Noto Sans Kannada already registered with postscriptFonts().
Noto Sans Kannada Light already registered with postscriptFonts().
Noto Sans Kannada Medium already registered with postscriptFonts().
Noto Sans Kannada Thin already registered with postscriptFonts().
Noto Sans Kannada UI Black already registered with postscriptFonts().
Noto Sans Kannada UI already registered with postscriptFonts().
Noto Sans Kannada UI Light already registered with postscriptFonts().
Noto Sans Kannada UI Medium already registered with postscriptFonts().
Noto Sans Kannada UI Thin already registered with postscriptFonts().
Noto Sans Kayah Li already registered with postscriptFonts().
Noto Sans Kharoshthi already registered with postscriptFonts().
Noto Sans Khmer Black already registered with postscriptFonts().
Noto Sans Khmer already registered with postscriptFonts().
Noto Sans Khmer Light already registered with postscriptFonts().
Noto Sans Khmer Medium already registered with postscriptFonts().
Noto Sans Khmer Thin already registered with postscriptFonts().
Noto Sans Khmer UI Black already registered with postscriptFonts().
Noto Sans Khmer UI already registered with postscriptFonts().
Noto Sans Khmer UI Light already registered with postscriptFonts().
Noto Sans Khmer UI Medium already registered with postscriptFonts().
Noto Sans Khmer UI Thin already registered with postscriptFonts().
Noto Sans Khojki already registered with postscriptFonts().
Noto Sans Khudawadi already registered with postscriptFonts().
Noto Sans Lao Blk already registered with postscriptFonts().
Noto Sans Lao already registered with postscriptFonts().
Noto Sans Lao Light already registered with postscriptFonts().
Noto Sans Lao Med already registered with postscriptFonts().
Noto Sans Lao Thin already registered with postscriptFonts().
Noto Sans Lao UI Blk already registered with postscriptFonts().
Noto Sans Lao UI already registered with postscriptFonts().
Noto Sans Lao UI Light already registered with postscriptFonts().
Noto Sans Lao UI Med already registered with postscriptFonts().
Noto Sans Lao UI Thin already registered with postscriptFonts().
Noto Sans Lepcha already registered with postscriptFonts().
Noto Sans Limbu already registered with postscriptFonts().
Noto Sans Linear A already registered with postscriptFonts().
Noto Sans Linear B already registered with postscriptFonts().
Noto Sans Lisu already registered with postscriptFonts().
Noto Sans Lycian already registered with postscriptFonts().
Noto Sans Lydian already registered with postscriptFonts().
Noto Sans Mahajani already registered with postscriptFonts().
Noto Sans Malayalam Black already registered with postscriptFonts().
Noto Sans Malayalam already registered with postscriptFonts().
Noto Sans Malayalam Light already registered with postscriptFonts().
Noto Sans Malayalam Medium already registered with postscriptFonts().
Noto Sans Malayalam Thin already registered with postscriptFonts().
Noto Sans Malayalam UI Black already registered with postscriptFonts().
Noto Sans Malayalam UI already registered with postscriptFonts().
Noto Sans Malayalam UI Light already registered with postscriptFonts().
Noto Sans Malayalam UI Medium already registered with postscriptFonts().
Noto Sans Malayalam UI Thin already registered with postscriptFonts().
Noto Sans Mandaic already registered with postscriptFonts().
Noto Sans Manichaean already registered with postscriptFonts().
Noto Sans Marchen already registered with postscriptFonts().
Noto Sans Math already registered with postscriptFonts().
Noto Sans Mayan Numerals already registered with postscriptFonts().
Noto Sans MeeteiMayek already registered with postscriptFonts().
Noto Sans Mende Kikakui already registered with postscriptFonts().
Noto Sans Meroitic already registered with postscriptFonts().
Noto Sans Miao already registered with postscriptFonts().
Noto Sans Modi already registered with postscriptFonts().
Noto Sans Mongolian already registered with postscriptFonts().
Noto Sans Mono Black already registered with postscriptFonts().
Noto Sans Mono already registered with postscriptFonts().
Noto Sans Mono Light already registered with postscriptFonts().
Noto Sans Mono Medium already registered with postscriptFonts().
Noto Sans Mono Thin already registered with postscriptFonts().
Noto Sans Mro already registered with postscriptFonts().
Noto Sans Multani already registered with postscriptFonts().
Noto Sans Myanmar Blk already registered with postscriptFonts().
Noto Sans Myanmar already registered with postscriptFonts().
Noto Sans Myanmar Light already registered with postscriptFonts().
Noto Sans Myanmar Med already registered with postscriptFonts().
Noto Sans Myanmar Thin already registered with postscriptFonts().
Noto Sans Myanmar UI Black already registered with postscriptFonts().
Noto Sans Myanmar UI already registered with postscriptFonts().
Noto Sans Myanmar UI Light already registered with postscriptFonts().
Noto Sans Myanmar UI Medium already registered with postscriptFonts().
Noto Sans Myanmar UI Thin already registered with postscriptFonts().
Noto Sans Nabataean already registered with postscriptFonts().
Noto Sans Newa already registered with postscriptFonts().
Noto Sans NewTaiLue already registered with postscriptFonts().
Noto Sans N'Ko already registered with postscriptFonts().
Noto Sans Ogham already registered with postscriptFonts().
Noto Sans Ol Chiki already registered with postscriptFonts().
Noto Sans OldHung already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Noto Sans Old Italic. Skipping setup for this font.
Noto Sans OldNorArab already registered with postscriptFonts().
Noto Sans Old Permic already registered with postscriptFonts().
Noto Sans OldPersian already registered with postscriptFonts().
Noto Sans OldSogdian already registered with postscriptFonts().
Noto Sans OldSouArab already registered with postscriptFonts().
Noto Sans Old Turkic already registered with postscriptFonts().
Noto Sans Oriya already registered with postscriptFonts().
Noto Sans Oriya UI already registered with postscriptFonts().
Noto Sans Osage already registered with postscriptFonts().
Noto Sans Osmanya already registered with postscriptFonts().
Noto Sans Pahawh Hmong already registered with postscriptFonts().
Noto Sans Palmyrene already registered with postscriptFonts().
Noto Sans PauCinHau already registered with postscriptFonts().
Noto Sans PhagsPa already registered with postscriptFonts().
Noto Sans Phoenician already registered with postscriptFonts().
Noto Sans PsaPahlavi already registered with postscriptFonts().
Noto Sans Rejang already registered with postscriptFonts().
Noto Sans Runic already registered with postscriptFonts().
Noto Sans Samaritan already registered with postscriptFonts().
Noto Sans Saurashtra already registered with postscriptFonts().
Noto Sans Sharada already registered with postscriptFonts().
Noto Sans Shavian already registered with postscriptFonts().
Noto Sans Siddham already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Blk. Skipping setup for this font.
Noto Sans Sinhala already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Thin. Skipping setup for this font.
Noto Sans Sinhala UI already registered with postscriptFonts().
Noto Sans SoraSomp already registered with postscriptFonts().
Noto Sans Sundanese already registered with postscriptFonts().
Noto Sans Syloti Nagri already registered with postscriptFonts().
Noto Sans Symbols Blk already registered with postscriptFonts().
Noto Sans Symbols already registered with postscriptFonts().
Noto Sans Symbols Light already registered with postscriptFonts().
Noto Sans Symbols Med already registered with postscriptFonts().
Noto Sans Symbols Thin already registered with postscriptFonts().
Noto Sans Symbols2 already registered with postscriptFonts().
Noto Sans Syriac Black already registered with postscriptFonts().
Noto Sans Syriac already registered with postscriptFonts().
Noto Sans Syriac Thin already registered with postscriptFonts().
Noto Sans Tagalog already registered with postscriptFonts().
Noto Sans Tagbanwa already registered with postscriptFonts().
Noto Sans Tai Le already registered with postscriptFonts().
Noto Sans Tai Tham already registered with postscriptFonts().
Noto Sans Tai Viet already registered with postscriptFonts().
Noto Sans Takri already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Blk. Skipping setup for this font.
Noto Sans Tamil already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Thin. Skipping setup for this font.
Noto Sans Tamil Supplement already registered with postscriptFonts().
Noto Sans Tamil UI already registered with postscriptFonts().
Noto Sans Telugu Black already registered with postscriptFonts().
Noto Sans Telugu already registered with postscriptFonts().
Noto Sans Telugu Light already registered with postscriptFonts().
Noto Sans Telugu Medium already registered with postscriptFonts().
Noto Sans Telugu Thin already registered with postscriptFonts().
Noto Sans Telugu UI Black already registered with postscriptFonts().
Noto Sans Telugu UI already registered with postscriptFonts().
Noto Sans Telugu UI Light already registered with postscriptFonts().
Noto Sans Telugu UI Medium already registered with postscriptFonts().
Noto Sans Telugu UI Thin already registered with postscriptFonts().
Noto Sans Thaana Black already registered with postscriptFonts().
Noto Sans Thaana already registered with postscriptFonts().
Noto Sans Thaana Light already registered with postscriptFonts().
Noto Sans Thaana Medium already registered with postscriptFonts().
Noto Sans Thaana Thin already registered with postscriptFonts().
Noto Sans Thai Blk already registered with postscriptFonts().
Noto Sans Thai already registered with postscriptFonts().
Noto Sans Thai Light already registered with postscriptFonts().
Noto Sans Thai Med already registered with postscriptFonts().
Noto Sans Thai Thin already registered with postscriptFonts().
Noto Sans Thai UI Blk already registered with postscriptFonts().
Noto Sans Thai UI already registered with postscriptFonts().
Noto Sans Thai UI Light already registered with postscriptFonts().
Noto Sans Thai UI Med already registered with postscriptFonts().
Noto Sans Thai UI Thin already registered with postscriptFonts().
Noto Sans Tibetan already registered with postscriptFonts().
Noto Sans Tifinagh already registered with postscriptFonts().
Noto Sans Tirhuta already registered with postscriptFonts().
Noto Sans Ugaritic already registered with postscriptFonts().
Noto Sans Vai already registered with postscriptFonts().
Noto Sans WarangCiti already registered with postscriptFonts().
Noto Sans Yi already registered with postscriptFonts().
Noto Serif Black already registered with postscriptFonts().
Noto Serif already registered with postscriptFonts().
Noto Serif Light already registered with postscriptFonts().
Noto Serif Medium already registered with postscriptFonts().
Noto Serif Thin already registered with postscriptFonts().
Noto Serif Ahom already registered with postscriptFonts().
Noto Serif Armenian Bk already registered with postscriptFonts().
Noto Serif Armenian already registered with postscriptFonts().
Noto Serif Armenian Lt already registered with postscriptFonts().
Noto Serif Armenian Md already registered with postscriptFonts().
Noto Serif Armenian Th already registered with postscriptFonts().
Noto Serif Balinese already registered with postscriptFonts().
Noto Serif Bengali Black already registered with postscriptFonts().
Noto Serif Bengali already registered with postscriptFonts().
Noto Serif Bengali Light already registered with postscriptFonts().
Noto Serif Bengali Medium already registered with postscriptFonts().
Noto Serif Bengali Thin already registered with postscriptFonts().
Noto Serif Devanagari Black already registered with postscriptFonts().
Noto Serif Devanagari already registered with postscriptFonts().
Noto Serif Devanagari Light already registered with postscriptFonts().
Noto Serif Devanagari Medium already registered with postscriptFonts().
Noto Serif Devanagari Thin already registered with postscriptFonts().
Noto Serif Display Black already registered with postscriptFonts().
Noto Serif Display already registered with postscriptFonts().
Noto Serif Display Light already registered with postscriptFonts().
Noto Serif Display Medium already registered with postscriptFonts().
Noto Serif Display Thin already registered with postscriptFonts().
Noto Serif Dogra already registered with postscriptFonts().
Noto Serif Ethiopic Bk already registered with postscriptFonts().
Noto Serif Ethiopic already registered with postscriptFonts().
Noto Serif Ethiopic Lt already registered with postscriptFonts().
Noto Serif Ethiopic Md already registered with postscriptFonts().
Noto Serif Ethiopic Th already registered with postscriptFonts().
Noto Serif Georgian Bk already registered with postscriptFonts().
Noto Serif Georgian already registered with postscriptFonts().
Noto Serif Georgian Lt already registered with postscriptFonts().
Noto Serif Georgian Md already registered with postscriptFonts().
Noto Serif Georgian Th already registered with postscriptFonts().
Noto Serif Gujarati Black already registered with postscriptFonts().
Noto Serif Gujarati already registered with postscriptFonts().
Noto Serif Gujarati Light already registered with postscriptFonts().
Noto Serif Gujarati Medium already registered with postscriptFonts().
Noto Serif Gujarati Thin already registered with postscriptFonts().
Noto Serif Gurmukhi Black already registered with postscriptFonts().
Noto Serif Gurmukhi already registered with postscriptFonts().
Noto Serif Gurmukhi Light already registered with postscriptFonts().
Noto Serif Gurmukhi Medium already registered with postscriptFonts().
Noto Serif Gurmukhi Thin already registered with postscriptFonts().
Noto Serif Hebrew Blk already registered with postscriptFonts().
Noto Serif Hebrew already registered with postscriptFonts().
Noto Serif Hebrew Light already registered with postscriptFonts().
Noto Serif Hebrew Med already registered with postscriptFonts().
Noto Serif Hebrew Thin already registered with postscriptFonts().
Noto Serif Kannada Black already registered with postscriptFonts().
Noto Serif Kannada already registered with postscriptFonts().
Noto Serif Kannada Light already registered with postscriptFonts().
Noto Serif Kannada Medium already registered with postscriptFonts().
Noto Serif Kannada Thin already registered with postscriptFonts().
Noto Serif Khmer Black already registered with postscriptFonts().
Noto Serif Khmer already registered with postscriptFonts().
Noto Serif Khmer Light already registered with postscriptFonts().
Noto Serif Khmer Medium already registered with postscriptFonts().
Noto Serif Khmer Thin already registered with postscriptFonts().
Noto Serif Lao Blk already registered with postscriptFonts().
Noto Serif Lao already registered with postscriptFonts().
Noto Serif Lao Light already registered with postscriptFonts().
Noto Serif Lao Med already registered with postscriptFonts().
Noto Serif Lao Thin already registered with postscriptFonts().
Noto Serif Malayalam Black already registered with postscriptFonts().
Noto Serif Malayalam already registered with postscriptFonts().
Noto Serif Malayalam Light already registered with postscriptFonts().
Noto Serif Malayalam Medium already registered with postscriptFonts().
Noto Serif Malayalam Thin already registered with postscriptFonts().
Noto Serif Myanmar Blk already registered with postscriptFonts().
Noto Serif Myanmar already registered with postscriptFonts().
Noto Serif Myanmar Light already registered with postscriptFonts().
Noto Serif Myanmar Med already registered with postscriptFonts().
Noto Serif Myanmar Thin already registered with postscriptFonts().
Noto Serif Sinhala Black already registered with postscriptFonts().
Noto Serif Sinhala already registered with postscriptFonts().
Noto Serif Sinhala Light already registered with postscriptFonts().
Noto Serif Sinhala Medium already registered with postscriptFonts().
Noto Serif Sinhala Thin already registered with postscriptFonts().
Noto Serif Tamil Blk already registered with postscriptFonts().
Noto Serif Tamil already registered with postscriptFonts().
Noto Serif Tamil Light already registered with postscriptFonts().
Noto Serif Tamil Med already registered with postscriptFonts().
Noto Serif Tamil Thin already registered with postscriptFonts().
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Black. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Light. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Medium. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Thin. Skipping setup for this font.
Noto Serif Tangut already registered with postscriptFonts().
Noto Serif Telugu Black already registered with postscriptFonts().
Noto Serif Telugu already registered with postscriptFonts().
Noto Serif Telugu Light already registered with postscriptFonts().
Noto Serif Telugu Medium already registered with postscriptFonts().
Noto Serif Telugu Thin already registered with postscriptFonts().
Noto Serif Thai Blk already registered with postscriptFonts().
Noto Serif Thai already registered with postscriptFonts().
Noto Serif Thai Light already registered with postscriptFonts().
Noto Serif Thai Med already registered with postscriptFonts().
Noto Serif Thai Thin already registered with postscriptFonts().
Noto Serif Tibetan Black already registered with postscriptFonts().
Noto Serif Tibetan already registered with postscriptFonts().
Noto Serif Tibetan Light already registered with postscriptFonts().
Noto Serif Tibetan Medium already registered with postscriptFonts().
Noto Serif Tibetan Thin already registered with postscriptFonts().
NovaMono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Nunito. Skipping setup for this font.
orya already registered with postscriptFonts().
More than one version of regular/bold/italic found for padmaa. Skipping setup for this font.
Pothana2000 already registered with postscriptFonts().
ProFont for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Roboto. Skipping setup for this font.
More than one version of regular/bold/italic found for Roboto Condensed. Skipping setup for this font.
Roboto Mono for Powerline already registered with postscriptFonts().
Roboto Mono Light for Powerline already registered with postscriptFonts().
Roboto Mono Medium for Powerline already registered with postscriptFonts().
Roboto Mono Thin for Powerline already registered with postscriptFonts().
Sagar already registered with postscriptFonts().
Space Mono already registered with postscriptFonts().
Space Mono for Powerline already registered with postscriptFonts().
Symbol Neu for Powerline already registered with postscriptFonts().
TAMu_Kadambri already registered with postscriptFonts().
TAMu_Kalyani already registered with postscriptFonts().
TAMu_Maduram already registered with postscriptFonts().
Tinos for Powerline already registered with postscriptFonts().
TSCu_Comic already registered with postscriptFonts().
TSCu_Paranar already registered with postscriptFonts().
TSCu_Times already registered with postscriptFonts().
Ubuntu already registered with postscriptFonts().
Ubuntu Light already registered with postscriptFonts().
Ubuntu Condensed already registered with postscriptFonts().
Ubuntu Mono already registered with postscriptFonts().
Ubuntu Mono derivative Powerline already registered with postscriptFonts().
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_search_spaces)
#+end_SRC

#+RESULTS:
: 'data.frame':	69 obs. of  8 variables:
:  $ name                   : chr  "atax" "dgemv3" "fdtd4d2d" "gemver" ...
:  $ year                   : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...
:  $ dimension              : int  19 49 30 24 11 15 14 12 20 25 ...
:  $ search_space_size      : num  1.65e+14 2.73e+30 7.06e+24 7.26e+17 1.56e+08 ...
:  $ log10_search_space_size: int  14 30 24 17 8 8 12 8 16 19 ...
:  $ domain                 : chr  "Linear Algebra" "Linear Algebra" "Linear Algebra" "Linear Algebra" ...
:  $ author                 : chr  "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" ...
:  $ gscholar_citation      : chr  "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" ...
**** Generate Caption
#+begin_SRC R :results output :session *R* :eval no-export :exports results
citations <- unique(df_search_spaces$gscholar_citation)
citations <- paste(citations[citations != ""], collapse = ",")
cat(paste("\\nbsp{}\\cite{", citations, "}", sep = ""))
#+end_SRC

#+RESULTS:
:
: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,abdelfattah2016performance,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}

**** Plots
- Geom_label around thesis work
- Mark Seymour et al. example
#+begin_SRC R :results graphics output :session *R* :file "./img/search_spaces.pdf" :width 18 :height 8.7 :eval no-export
library(ggplot2)
library(dplyr)
library(scales)
library(RColorBrewer)
library(ggrepel)
library(patchwork)

point_alpha = 1.0
point_size = 3
label_size = 6

shapes = c(15, 16, 17, 18, 6, 7, 9, 0, 3, 5, 12, 14, 13, 11)

legend_rows = length(unique(df_search_spaces$domain)) / 2
legend_position = c(0.66, 0.12)

base_size = 25
font_family = "Liberation Sans"

x_lims <- c(0, 60)
y_lims <- c(1, 50)

color_palette = colorRampPalette(brewer.pal(9,
                                            "Set1"))(
                                                length(
                                                    unique(
                                                        df_search_spaces$domain)))

x_text = element_text(size = 26)
y_text = element_text(size = 26)

x_label = element_text(size = 28)
y_label = element_text(size = 28)

scientific_10 <- function(x) {
    print(x)
    result <- parse(text = gsub("(.*)",
                                "10^\\1",
                                format(x)))
    print(result)
    return(result)
}

p1 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_rect(aes(xmin = x_lims[1],
                  xmax = x_lims[2],
                  ymin = y_lims[1],
                  ymax = y_lims[2]),
              show.legend = FALSE,
              fill = NA,
              color = "gray35",
              linetype = 2) +
    geom_text(data = data.frame(x = x_lims[2],
                                y = y_lims[1],
                                label = "Detailed"),
              aes(x = x,
                  y = y,
                  label = label,
                  shape = NA),
              color = "gray35",
              vjust = 1.3,
              hjust = 0,
              angle = 90,
              size = label_size,
              show.legend = FALSE) +
    geom_point(alpha = point_alpha,
               size = point_size,
               show.legend = FALSE) +
    geom_text_repel(data = filter(df_search_spaces,
                                  thesis == FALSE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    xlim = c(130, 200),
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  name != "resnet50_weights" &
                                  name != "gemv" &
                                  dimension > 40 &
                                  log10_search_space_size >= 30),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    xlim = c(130, NA),
                    ylim = c(NA, NA),
                    nudge_y = 1,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30) %>%
                     filter(name == "resnet50_weights"),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(60, NA),
                    xlim = c(130, NA),
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30) %>%
                     filter(name == "gemv"),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(50, NA),
                    xlim = c(130, NA),
                    size = label_size,
                    show.legend = FALSE) +
    xlab("Dimension") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(text = element_text(family = font_family),
          axis.text.x = x_text,
          axis.text.y = y_text,
          axis.title.y = element_blank())

p2 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_point(alpha = point_alpha,
               size = point_size) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 40 &
                               dimension < 60 &
                               thesis == FALSE &
                               log10_search_space_size >= 30,
                               log10_search_space_size < 50),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(30, 50),
                    xlim = c(NA, 45),
                    nudge_x = -3,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension > 40 &
                               dimension < 60 &
                               thesis == TRUE &
                               log10_search_space_size >= 30,
                               log10_search_space_size < 50),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(30, 50),
                    xlim = c(NA, 45),
                    nudge_x = -3,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension < 20 &
                               thesis == FALSE &
                               log10_search_space_size > 19),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(20, 50),
                    xlim = c(0, 25),
                    nudge_y = 1.6,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension < 20 &
                               thesis == TRUE &
                               log10_search_space_size > 19),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(20, 50),
                    xlim = c(0, 25),
                    nudge_y = 1.6,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 30 &
                               thesis == FALSE &
                               log10_search_space_size < 25),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(15, 50),
                    xlim = c(30, NA),
                    nudge_y = 2,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension > 30 &
                               thesis == TRUE &
                               log10_search_space_size < 25),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(15, 50),
                    xlim = c(30, NA),
                    nudge_y = 2,
                    size = label_size,
                    show.legend = FALSE) +
    xlim(x_lims[1], x_lims[2]) +
    xlab("Dimension") +
    ylab("Search Space Size") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(limits = y_lims, label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(axis.text.x = x_text,
          axis.text.y = y_text,
          legend.position = legend_position,
          legend.direction = "horizontal",
          legend.spacing.x = unit(0.0, 'cm'),
          legend.spacing.y = unit(0.0, 'cm'),
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 15),
          text = element_text(family = font_family)) +
    guides(color = guide_legend(nrow = legend_rows,
                                override.aes = list(alpha = 1.0,
                                                    size = 3)))

p2 * p1
#+end_SRC

#+RESULTS:
[[file:./img/search_spaces.pdf]]
*** Loop Blocking and Unrolling
**** First try
:PROPERTIES:
:EXPORT_FILE_NAME: blocking_unrolling.pdf
:END:

#+begin_src latex :fit true
% Export this heading with C-c C-e C-s C-b l p
\documentclass{standalone}
\usepackage{tikz}
\begin{document}
\begin{tikzpicture}
\draw[red] (0,0) circle (2cm);
\end{tikzpicture}
\end{document}
#+end_src try with export
**** Arnaud's tip with src blocks
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
\\usepackage[usenames]{color}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: \usepackage[usenames]{color}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage{tikz}")
#+HEADER: :exports results :results raw :file ./img/test.pdf
#+begin_src latex :eval no-export
\begin{tikzpicture}
\draw[blue] (0,0) circle (2cm);
\end{tikzpicture}
#+end_src

#+RESULTS:
[[file:./img/test.pdf]]

**** ggplot
***** Setup
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(extrafont)
loadfonts(device = "postscript")
#+end_SRC

#+RESULTS:
#+begin_example

Akaash already registered with postscriptFonts().
AkrutiMal1 already registered with postscriptFonts().
AkrutiMal2 already registered with postscriptFonts().
AkrutiTml1 already registered with postscriptFonts().
AkrutiTml2 already registered with postscriptFonts().
Anonymice Powerline already registered with postscriptFonts().
Arimo for Powerline already registered with postscriptFonts().
Bitstream Vera Sans already registered with postscriptFonts().
Bitstream Vera Sans Mono already registered with postscriptFonts().
Bitstream Vera Serif already registered with postscriptFonts().
Cousine for Powerline already registered with postscriptFonts().
IBM 3270 already registered with postscriptFonts().
IBM 3270 Narrow already registered with postscriptFonts().
IBM 3270 Semi-Narrow already registered with postscriptFonts().
DejaVu Math TeX Gyre already registered with postscriptFonts().
DejaVu Sans already registered with postscriptFonts().
DejaVu Sans Light already registered with postscriptFonts().
DejaVu Sans Condensed already registered with postscriptFonts().
DejaVu Sans Mono already registered with postscriptFonts().
DejaVu Sans Mono for Powerline already registered with postscriptFonts().
DejaVu Serif already registered with postscriptFonts().
DejaVu Serif Condensed already registered with postscriptFonts().
Droid Arabic Kufi already registered with postscriptFonts().
Droid Arabic Naskh already registered with postscriptFonts().
Droid Naskh Shift Alt already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans. Skipping setup for this font.
Droid Sans Arabic already registered with postscriptFonts().
Droid Sans Armenian already registered with postscriptFonts().
Droid Sans Devanagari already registered with postscriptFonts().
Droid Sans Ethiopic already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Fallback. Skipping setup for this font.
Droid Sans Georgian already registered with postscriptFonts().
Droid Sans Hebrew already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Mono. Skipping setup for this font.
Droid Sans Mono Dotted for Powerline already registered with postscriptFonts().
Droid Sans Mono Slashed for Powerline already registered with postscriptFonts().
Droid Sans Tamil already registered with postscriptFonts().
Droid Sans Thai already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Serif. Skipping setup for this font.
Font Awesome 5 Brands Regular already registered with postscriptFonts().
Font Awesome 5 Free Regular already registered with postscriptFonts().
Font Awesome 5 Free Solid already registered with postscriptFonts().
Gargi-1.2b already registered with postscriptFonts().
Goha-Tibeb Zemen already registered with postscriptFonts().
Go Mono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for GurbaniBoliLite. Skipping setup for this font.
Hack already registered with postscriptFonts().
Inconsolata Black already registered with postscriptFonts().
Inconsolata already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Inconsolata for Powerline. Skipping setup for this font.
Inconsolata Condensed already registered with postscriptFonts().
Inconsolata Condensed Black already registered with postscriptFonts().
Inconsolata Condensed Bold already registered with postscriptFonts().
Inconsolata Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Condensed Light already registered with postscriptFonts().
Inconsolata Condensed Medium already registered with postscriptFonts().
Inconsolata Condensed SemiBold already registered with postscriptFonts().
Inconsolata Expanded already registered with postscriptFonts().
Inconsolata Expanded Black already registered with postscriptFonts().
Inconsolata Expanded Bold already registered with postscriptFonts().
Inconsolata Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Expanded Light already registered with postscriptFonts().
Inconsolata Expanded Medium already registered with postscriptFonts().
Inconsolata Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed already registered with postscriptFonts().
Inconsolata Extra Condensed Black already registered with postscriptFonts().
Inconsolata Extra Condensed Bold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Extra Condensed Light already registered with postscriptFonts().
Inconsolata Extra Condensed Medium already registered with postscriptFonts().
Inconsolata Extra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Extra Expanded already registered with postscriptFonts().
Inconsolata Extra Expanded Black already registered with postscriptFonts().
Inconsolata Extra Expanded Bold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Extra Expanded Light already registered with postscriptFonts().
Inconsolata Extra Expanded Medium already registered with postscriptFonts().
Inconsolata Extra Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraLight already registered with postscriptFonts().
Inconsolata Light already registered with postscriptFonts().
Inconsolata Medium already registered with postscriptFonts().
Inconsolata SemiBold already registered with postscriptFonts().
Inconsolata Semi Condensed already registered with postscriptFonts().
Inconsolata Semi Condensed Black already registered with postscriptFonts().
Inconsolata Semi Condensed Bold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Semi Condensed Light already registered with postscriptFonts().
Inconsolata Semi Condensed Medium already registered with postscriptFonts().
Inconsolata Semi Condensed SemiBold already registered with postscriptFonts().
Inconsolata Semi Expanded already registered with postscriptFonts().
Inconsolata Semi Expanded Black already registered with postscriptFonts().
Inconsolata Semi Expanded Bold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Semi Expanded Light already registered with postscriptFonts().
Inconsolata Semi Expanded Medium already registered with postscriptFonts().
Inconsolata Semi Expanded SemiBold already registered with postscriptFonts().
Inconsolata Ultra Condensed already registered with postscriptFonts().
Inconsolata Ultra Condensed Black already registered with postscriptFonts().
Inconsolata Ultra Condensed Bold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Condensed Light already registered with postscriptFonts().
Inconsolata Ultra Condensed Medium already registered with postscriptFonts().
Inconsolata Ultra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Ultra Expanded already registered with postscriptFonts().
Inconsolata Ultra Expanded Black already registered with postscriptFonts().
Inconsolata Ultra Expanded Bold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Expanded Light already registered with postscriptFonts().
Inconsolata Ultra Expanded Medium already registered with postscriptFonts().
Inconsolata Ultra Expanded SemiBold already registered with postscriptFonts().
Liberation Mono already registered with postscriptFonts().
Liberation Sans already registered with postscriptFonts().
Liberation Serif already registered with postscriptFonts().
Ligconsolata already registered with postscriptFonts().
Likhan already registered with postscriptFonts().
Literation Mono Powerline already registered with postscriptFonts().
malayalam already registered with postscriptFonts().
MalOtf already registered with postscriptFonts().
Meslo LG L DZ for Powerline already registered with postscriptFonts().
Meslo LG L for Powerline already registered with postscriptFonts().
Meslo LG M DZ for Powerline already registered with postscriptFonts().
Meslo LG M for Powerline already registered with postscriptFonts().
Meslo LG S DZ for Powerline already registered with postscriptFonts().
Meslo LG S for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for monofur for Powerline. Skipping setup for this font.
More than one version of regular/bold/italic found for Mukti Narrow. Skipping setup for this font.
Noto Kufi Arabic already registered with postscriptFonts().
Noto Kufi Arabic Medium already registered with postscriptFonts().
Noto Kufi Arabic Semi bold already registered with postscriptFonts().
Noto Mono for Powerline already registered with postscriptFonts().
Noto Music already registered with postscriptFonts().
Noto Naskh Arabic already registered with postscriptFonts().
Noto Naskh Arabic UI already registered with postscriptFonts().
Noto Nastaliq Urdu already registered with postscriptFonts().
Noto Sans Black already registered with postscriptFonts().
Noto Sans already registered with postscriptFonts().
Noto Sans Light already registered with postscriptFonts().
Noto Sans Medium already registered with postscriptFonts().
Noto Sans Thin already registered with postscriptFonts().
Noto Sans Adlam already registered with postscriptFonts().
Noto Sans Adlam Unjoined already registered with postscriptFonts().
Noto Sans AnatoHiero already registered with postscriptFonts().
Noto Sans Arabic Blk already registered with postscriptFonts().
Noto Sans Arabic already registered with postscriptFonts().
Noto Sans Arabic Light already registered with postscriptFonts().
Noto Sans Arabic Med already registered with postscriptFonts().
Noto Sans Arabic Thin already registered with postscriptFonts().
Noto Sans Arabic UI Bk already registered with postscriptFonts().
Noto Sans Arabic UI already registered with postscriptFonts().
Noto Sans Arabic UI Lt already registered with postscriptFonts().
Noto Sans Arabic UI Md already registered with postscriptFonts().
Noto Sans Arabic UI Th already registered with postscriptFonts().
Noto Sans Armenian Blk already registered with postscriptFonts().
Noto Sans Armenian already registered with postscriptFonts().
Noto Sans Armenian Light already registered with postscriptFonts().
Noto Sans Armenian Med already registered with postscriptFonts().
Noto Sans Armenian Thin already registered with postscriptFonts().
Noto Sans Avestan already registered with postscriptFonts().
Noto Sans Bamum already registered with postscriptFonts().
Noto Sans Bassa Vah already registered with postscriptFonts().
Noto Sans Batak already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Blk. Skipping setup for this font.
Noto Sans Bengali already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Thin. Skipping setup for this font.
Noto Sans Bengali UI already registered with postscriptFonts().
Noto Sans Bhaiksuki already registered with postscriptFonts().
Noto Sans Brahmi already registered with postscriptFonts().
Noto Sans Buginese already registered with postscriptFonts().
Noto Sans Buhid already registered with postscriptFonts().
Noto Sans CanAborig Bk already registered with postscriptFonts().
Noto Sans CanAborig already registered with postscriptFonts().
Noto Sans CanAborig Lt already registered with postscriptFonts().
Noto Sans CanAborig Md already registered with postscriptFonts().
Noto Sans CanAborig Th already registered with postscriptFonts().
Noto Sans Carian already registered with postscriptFonts().
Noto Sans CaucAlban already registered with postscriptFonts().
Noto Sans Chakma already registered with postscriptFonts().
Noto Sans Cham Blk already registered with postscriptFonts().
Noto Sans Cham already registered with postscriptFonts().
Noto Sans Cham Light already registered with postscriptFonts().
Noto Sans Cham Med already registered with postscriptFonts().
Noto Sans Cham Thin already registered with postscriptFonts().
Noto Sans Cherokee Blk already registered with postscriptFonts().
Noto Sans Cherokee already registered with postscriptFonts().
Noto Sans Cherokee Light already registered with postscriptFonts().
Noto Sans Cherokee Med already registered with postscriptFonts().
Noto Sans Cherokee Thin already registered with postscriptFonts().
Noto Sans Coptic already registered with postscriptFonts().
Noto Sans Cuneiform already registered with postscriptFonts().
Noto Sans Cypriot already registered with postscriptFonts().
Noto Sans Deseret already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Bk. Skipping setup for this font.
Noto Sans Devanagari already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Lt. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Md. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Th. Skipping setup for this font.
Noto Sans Devanagari UI already registered with postscriptFonts().
Noto Sans Display Black already registered with postscriptFonts().
Noto Sans Display already registered with postscriptFonts().
Noto Sans Display Light already registered with postscriptFonts().
Noto Sans Display Medium already registered with postscriptFonts().
Noto Sans Display Thin already registered with postscriptFonts().
Noto Sans Duployan already registered with postscriptFonts().
Noto Sans EgyptHiero already registered with postscriptFonts().
Noto Sans Elbasan already registered with postscriptFonts().
Noto Sans Ethiopic Blk already registered with postscriptFonts().
Noto Sans Ethiopic already registered with postscriptFonts().
Noto Sans Ethiopic Light already registered with postscriptFonts().
Noto Sans Ethiopic Med already registered with postscriptFonts().
Noto Sans Ethiopic Thin already registered with postscriptFonts().
Noto Sans Georgian Blk already registered with postscriptFonts().
Noto Sans Georgian already registered with postscriptFonts().
Noto Sans Georgian Light already registered with postscriptFonts().
Noto Sans Georgian Med already registered with postscriptFonts().
Noto Sans Georgian Thin already registered with postscriptFonts().
Noto Sans Glagolitic already registered with postscriptFonts().
Noto Sans Gothic already registered with postscriptFonts().
Noto Sans Grantha already registered with postscriptFonts().
Noto Sans Gujarati already registered with postscriptFonts().
Noto Sans Gujarati UI already registered with postscriptFonts().
Noto Sans Gurmukhi Black already registered with postscriptFonts().
Noto Sans Gurmukhi already registered with postscriptFonts().
Noto Sans Gurmukhi Light already registered with postscriptFonts().
Noto Sans Gurmukhi Medium already registered with postscriptFonts().
Noto Sans Gurmukhi Thin already registered with postscriptFonts().
Noto Sans Gurmukhi UI Black already registered with postscriptFonts().
Noto Sans Gurmukhi UI already registered with postscriptFonts().
Noto Sans Gurmukhi UI Light already registered with postscriptFonts().
Noto Sans Gurmukhi UI Medium already registered with postscriptFonts().
Noto Sans Gurmukhi UI Thin already registered with postscriptFonts().
Noto Sans HanifiRohg already registered with postscriptFonts().
Noto Sans Hanunoo already registered with postscriptFonts().
Noto Sans Hatran already registered with postscriptFonts().
Noto Sans Hebrew Blk already registered with postscriptFonts().
Noto Sans Hebrew already registered with postscriptFonts().
Noto Sans Hebrew Light already registered with postscriptFonts().
Noto Sans Hebrew Med already registered with postscriptFonts().
Noto Sans Hebrew Thin already registered with postscriptFonts().
Noto Sans ImpAramaic already registered with postscriptFonts().
Noto Sans Indic Siyaq Numbers already registered with postscriptFonts().
Noto Sans InsPahlavi already registered with postscriptFonts().
Noto Sans InsParthi already registered with postscriptFonts().
Noto Sans Javanese already registered with postscriptFonts().
Noto Sans Kaithi already registered with postscriptFonts().
Noto Sans Kannada Black already registered with postscriptFonts().
Noto Sans Kannada already registered with postscriptFonts().
Noto Sans Kannada Light already registered with postscriptFonts().
Noto Sans Kannada Medium already registered with postscriptFonts().
Noto Sans Kannada Thin already registered with postscriptFonts().
Noto Sans Kannada UI Black already registered with postscriptFonts().
Noto Sans Kannada UI already registered with postscriptFonts().
Noto Sans Kannada UI Light already registered with postscriptFonts().
Noto Sans Kannada UI Medium already registered with postscriptFonts().
Noto Sans Kannada UI Thin already registered with postscriptFonts().
Noto Sans Kayah Li already registered with postscriptFonts().
Noto Sans Kharoshthi already registered with postscriptFonts().
Noto Sans Khmer Black already registered with postscriptFonts().
Noto Sans Khmer already registered with postscriptFonts().
Noto Sans Khmer Light already registered with postscriptFonts().
Noto Sans Khmer Medium already registered with postscriptFonts().
Noto Sans Khmer Thin already registered with postscriptFonts().
Noto Sans Khmer UI Black already registered with postscriptFonts().
Noto Sans Khmer UI already registered with postscriptFonts().
Noto Sans Khmer UI Light already registered with postscriptFonts().
Noto Sans Khmer UI Medium already registered with postscriptFonts().
Noto Sans Khmer UI Thin already registered with postscriptFonts().
Noto Sans Khojki already registered with postscriptFonts().
Noto Sans Khudawadi already registered with postscriptFonts().
Noto Sans Lao Blk already registered with postscriptFonts().
Noto Sans Lao already registered with postscriptFonts().
Noto Sans Lao Light already registered with postscriptFonts().
Noto Sans Lao Med already registered with postscriptFonts().
Noto Sans Lao Thin already registered with postscriptFonts().
Noto Sans Lao UI Blk already registered with postscriptFonts().
Noto Sans Lao UI already registered with postscriptFonts().
Noto Sans Lao UI Light already registered with postscriptFonts().
Noto Sans Lao UI Med already registered with postscriptFonts().
Noto Sans Lao UI Thin already registered with postscriptFonts().
Noto Sans Lepcha already registered with postscriptFonts().
Noto Sans Limbu already registered with postscriptFonts().
Noto Sans Linear A already registered with postscriptFonts().
Noto Sans Linear B already registered with postscriptFonts().
Noto Sans Lisu already registered with postscriptFonts().
Noto Sans Lycian already registered with postscriptFonts().
Noto Sans Lydian already registered with postscriptFonts().
Noto Sans Mahajani already registered with postscriptFonts().
Noto Sans Malayalam Black already registered with postscriptFonts().
Noto Sans Malayalam already registered with postscriptFonts().
Noto Sans Malayalam Light already registered with postscriptFonts().
Noto Sans Malayalam Medium already registered with postscriptFonts().
Noto Sans Malayalam Thin already registered with postscriptFonts().
Noto Sans Malayalam UI Black already registered with postscriptFonts().
Noto Sans Malayalam UI already registered with postscriptFonts().
Noto Sans Malayalam UI Light already registered with postscriptFonts().
Noto Sans Malayalam UI Medium already registered with postscriptFonts().
Noto Sans Malayalam UI Thin already registered with postscriptFonts().
Noto Sans Mandaic already registered with postscriptFonts().
Noto Sans Manichaean already registered with postscriptFonts().
Noto Sans Marchen already registered with postscriptFonts().
Noto Sans Math already registered with postscriptFonts().
Noto Sans Mayan Numerals already registered with postscriptFonts().
Noto Sans MeeteiMayek already registered with postscriptFonts().
Noto Sans Mende Kikakui already registered with postscriptFonts().
Noto Sans Meroitic already registered with postscriptFonts().
Noto Sans Miao already registered with postscriptFonts().
Noto Sans Modi already registered with postscriptFonts().
Noto Sans Mongolian already registered with postscriptFonts().
Noto Sans Mono Black already registered with postscriptFonts().
Noto Sans Mono already registered with postscriptFonts().
Noto Sans Mono Light already registered with postscriptFonts().
Noto Sans Mono Medium already registered with postscriptFonts().
Noto Sans Mono Thin already registered with postscriptFonts().
Noto Sans Mro already registered with postscriptFonts().
Noto Sans Multani already registered with postscriptFonts().
Noto Sans Myanmar Blk already registered with postscriptFonts().
Noto Sans Myanmar already registered with postscriptFonts().
Noto Sans Myanmar Light already registered with postscriptFonts().
Noto Sans Myanmar Med already registered with postscriptFonts().
Noto Sans Myanmar Thin already registered with postscriptFonts().
Noto Sans Myanmar UI Black already registered with postscriptFonts().
Noto Sans Myanmar UI already registered with postscriptFonts().
Noto Sans Myanmar UI Light already registered with postscriptFonts().
Noto Sans Myanmar UI Medium already registered with postscriptFonts().
Noto Sans Myanmar UI Thin already registered with postscriptFonts().
Noto Sans Nabataean already registered with postscriptFonts().
Noto Sans Newa already registered with postscriptFonts().
Noto Sans NewTaiLue already registered with postscriptFonts().
Noto Sans N'Ko already registered with postscriptFonts().
Noto Sans Ogham already registered with postscriptFonts().
Noto Sans Ol Chiki already registered with postscriptFonts().
Noto Sans OldHung already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Noto Sans Old Italic. Skipping setup for this font.
Noto Sans OldNorArab already registered with postscriptFonts().
Noto Sans Old Permic already registered with postscriptFonts().
Noto Sans OldPersian already registered with postscriptFonts().
Noto Sans OldSogdian already registered with postscriptFonts().
Noto Sans OldSouArab already registered with postscriptFonts().
Noto Sans Old Turkic already registered with postscriptFonts().
Noto Sans Oriya already registered with postscriptFonts().
Noto Sans Oriya UI already registered with postscriptFonts().
Noto Sans Osage already registered with postscriptFonts().
Noto Sans Osmanya already registered with postscriptFonts().
Noto Sans Pahawh Hmong already registered with postscriptFonts().
Noto Sans Palmyrene already registered with postscriptFonts().
Noto Sans PauCinHau already registered with postscriptFonts().
Noto Sans PhagsPa already registered with postscriptFonts().
Noto Sans Phoenician already registered with postscriptFonts().
Noto Sans PsaPahlavi already registered with postscriptFonts().
Noto Sans Rejang already registered with postscriptFonts().
Noto Sans Runic already registered with postscriptFonts().
Noto Sans Samaritan already registered with postscriptFonts().
Noto Sans Saurashtra already registered with postscriptFonts().
Noto Sans Sharada already registered with postscriptFonts().
Noto Sans Shavian already registered with postscriptFonts().
Noto Sans Siddham already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Blk. Skipping setup for this font.
Noto Sans Sinhala already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Thin. Skipping setup for this font.
Noto Sans Sinhala UI already registered with postscriptFonts().
Noto Sans SoraSomp already registered with postscriptFonts().
Noto Sans Sundanese already registered with postscriptFonts().
Noto Sans Syloti Nagri already registered with postscriptFonts().
Noto Sans Symbols Blk already registered with postscriptFonts().
Noto Sans Symbols already registered with postscriptFonts().
Noto Sans Symbols Light already registered with postscriptFonts().
Noto Sans Symbols Med already registered with postscriptFonts().
Noto Sans Symbols Thin already registered with postscriptFonts().
Noto Sans Symbols2 already registered with postscriptFonts().
Noto Sans Syriac Black already registered with postscriptFonts().
Noto Sans Syriac already registered with postscriptFonts().
Noto Sans Syriac Thin already registered with postscriptFonts().
Noto Sans Tagalog already registered with postscriptFonts().
Noto Sans Tagbanwa already registered with postscriptFonts().
Noto Sans Tai Le already registered with postscriptFonts().
Noto Sans Tai Tham already registered with postscriptFonts().
Noto Sans Tai Viet already registered with postscriptFonts().
Noto Sans Takri already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Blk. Skipping setup for this font.
Noto Sans Tamil already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Thin. Skipping setup for this font.
Noto Sans Tamil Supplement already registered with postscriptFonts().
Noto Sans Tamil UI already registered with postscriptFonts().
Noto Sans Telugu Black already registered with postscriptFonts().
Noto Sans Telugu already registered with postscriptFonts().
Noto Sans Telugu Light already registered with postscriptFonts().
Noto Sans Telugu Medium already registered with postscriptFonts().
Noto Sans Telugu Thin already registered with postscriptFonts().
Noto Sans Telugu UI Black already registered with postscriptFonts().
Noto Sans Telugu UI already registered with postscriptFonts().
Noto Sans Telugu UI Light already registered with postscriptFonts().
Noto Sans Telugu UI Medium already registered with postscriptFonts().
Noto Sans Telugu UI Thin already registered with postscriptFonts().
Noto Sans Thaana Black already registered with postscriptFonts().
Noto Sans Thaana already registered with postscriptFonts().
Noto Sans Thaana Light already registered with postscriptFonts().
Noto Sans Thaana Medium already registered with postscriptFonts().
Noto Sans Thaana Thin already registered with postscriptFonts().
Noto Sans Thai Blk already registered with postscriptFonts().
Noto Sans Thai already registered with postscriptFonts().
Noto Sans Thai Light already registered with postscriptFonts().
Noto Sans Thai Med already registered with postscriptFonts().
Noto Sans Thai Thin already registered with postscriptFonts().
Noto Sans Thai UI Blk already registered with postscriptFonts().
Noto Sans Thai UI already registered with postscriptFonts().
Noto Sans Thai UI Light already registered with postscriptFonts().
Noto Sans Thai UI Med already registered with postscriptFonts().
Noto Sans Thai UI Thin already registered with postscriptFonts().
Noto Sans Tibetan already registered with postscriptFonts().
Noto Sans Tifinagh already registered with postscriptFonts().
Noto Sans Tirhuta already registered with postscriptFonts().
Noto Sans Ugaritic already registered with postscriptFonts().
Noto Sans Vai already registered with postscriptFonts().
Noto Sans WarangCiti already registered with postscriptFonts().
Noto Sans Yi already registered with postscriptFonts().
Noto Serif Black already registered with postscriptFonts().
Noto Serif already registered with postscriptFonts().
Noto Serif Light already registered with postscriptFonts().
Noto Serif Medium already registered with postscriptFonts().
Noto Serif Thin already registered with postscriptFonts().
Noto Serif Ahom already registered with postscriptFonts().
Noto Serif Armenian Bk already registered with postscriptFonts().
Noto Serif Armenian already registered with postscriptFonts().
Noto Serif Armenian Lt already registered with postscriptFonts().
Noto Serif Armenian Md already registered with postscriptFonts().
Noto Serif Armenian Th already registered with postscriptFonts().
Noto Serif Balinese already registered with postscriptFonts().
Noto Serif Bengali Black already registered with postscriptFonts().
Noto Serif Bengali already registered with postscriptFonts().
Noto Serif Bengali Light already registered with postscriptFonts().
Noto Serif Bengali Medium already registered with postscriptFonts().
Noto Serif Bengali Thin already registered with postscriptFonts().
Noto Serif Devanagari Black already registered with postscriptFonts().
Noto Serif Devanagari already registered with postscriptFonts().
Noto Serif Devanagari Light already registered with postscriptFonts().
Noto Serif Devanagari Medium already registered with postscriptFonts().
Noto Serif Devanagari Thin already registered with postscriptFonts().
Noto Serif Display Black already registered with postscriptFonts().
Noto Serif Display already registered with postscriptFonts().
Noto Serif Display Light already registered with postscriptFonts().
Noto Serif Display Medium already registered with postscriptFonts().
Noto Serif Display Thin already registered with postscriptFonts().
Noto Serif Dogra already registered with postscriptFonts().
Noto Serif Ethiopic Bk already registered with postscriptFonts().
Noto Serif Ethiopic already registered with postscriptFonts().
Noto Serif Ethiopic Lt already registered with postscriptFonts().
Noto Serif Ethiopic Md already registered with postscriptFonts().
Noto Serif Ethiopic Th already registered with postscriptFonts().
Noto Serif Georgian Bk already registered with postscriptFonts().
Noto Serif Georgian already registered with postscriptFonts().
Noto Serif Georgian Lt already registered with postscriptFonts().
Noto Serif Georgian Md already registered with postscriptFonts().
Noto Serif Georgian Th already registered with postscriptFonts().
Noto Serif Gujarati Black already registered with postscriptFonts().
Noto Serif Gujarati already registered with postscriptFonts().
Noto Serif Gujarati Light already registered with postscriptFonts().
Noto Serif Gujarati Medium already registered with postscriptFonts().
Noto Serif Gujarati Thin already registered with postscriptFonts().
Noto Serif Gurmukhi Black already registered with postscriptFonts().
Noto Serif Gurmukhi already registered with postscriptFonts().
Noto Serif Gurmukhi Light already registered with postscriptFonts().
Noto Serif Gurmukhi Medium already registered with postscriptFonts().
Noto Serif Gurmukhi Thin already registered with postscriptFonts().
Noto Serif Hebrew Blk already registered with postscriptFonts().
Noto Serif Hebrew already registered with postscriptFonts().
Noto Serif Hebrew Light already registered with postscriptFonts().
Noto Serif Hebrew Med already registered with postscriptFonts().
Noto Serif Hebrew Thin already registered with postscriptFonts().
Noto Serif Kannada Black already registered with postscriptFonts().
Noto Serif Kannada already registered with postscriptFonts().
Noto Serif Kannada Light already registered with postscriptFonts().
Noto Serif Kannada Medium already registered with postscriptFonts().
Noto Serif Kannada Thin already registered with postscriptFonts().
Noto Serif Khmer Black already registered with postscriptFonts().
Noto Serif Khmer already registered with postscriptFonts().
Noto Serif Khmer Light already registered with postscriptFonts().
Noto Serif Khmer Medium already registered with postscriptFonts().
Noto Serif Khmer Thin already registered with postscriptFonts().
Noto Serif Lao Blk already registered with postscriptFonts().
Noto Serif Lao already registered with postscriptFonts().
Noto Serif Lao Light already registered with postscriptFonts().
Noto Serif Lao Med already registered with postscriptFonts().
Noto Serif Lao Thin already registered with postscriptFonts().
Noto Serif Malayalam Black already registered with postscriptFonts().
Noto Serif Malayalam already registered with postscriptFonts().
Noto Serif Malayalam Light already registered with postscriptFonts().
Noto Serif Malayalam Medium already registered with postscriptFonts().
Noto Serif Malayalam Thin already registered with postscriptFonts().
Noto Serif Myanmar Blk already registered with postscriptFonts().
Noto Serif Myanmar already registered with postscriptFonts().
Noto Serif Myanmar Light already registered with postscriptFonts().
Noto Serif Myanmar Med already registered with postscriptFonts().
Noto Serif Myanmar Thin already registered with postscriptFonts().
Noto Serif Sinhala Black already registered with postscriptFonts().
Noto Serif Sinhala already registered with postscriptFonts().
Noto Serif Sinhala Light already registered with postscriptFonts().
Noto Serif Sinhala Medium already registered with postscriptFonts().
Noto Serif Sinhala Thin already registered with postscriptFonts().
Noto Serif Tamil Blk already registered with postscriptFonts().
Noto Serif Tamil already registered with postscriptFonts().
Noto Serif Tamil Light already registered with postscriptFonts().
Noto Serif Tamil Med already registered with postscriptFonts().
Noto Serif Tamil Thin already registered with postscriptFonts().
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Black. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Light. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Medium. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Thin. Skipping setup for this font.
Noto Serif Tangut already registered with postscriptFonts().
Noto Serif Telugu Black already registered with postscriptFonts().
Noto Serif Telugu already registered with postscriptFonts().
Noto Serif Telugu Light already registered with postscriptFonts().
Noto Serif Telugu Medium already registered with postscriptFonts().
Noto Serif Telugu Thin already registered with postscriptFonts().
Noto Serif Thai Blk already registered with postscriptFonts().
Noto Serif Thai already registered with postscriptFonts().
Noto Serif Thai Light already registered with postscriptFonts().
Noto Serif Thai Med already registered with postscriptFonts().
Noto Serif Thai Thin already registered with postscriptFonts().
Noto Serif Tibetan Black already registered with postscriptFonts().
Noto Serif Tibetan already registered with postscriptFonts().
Noto Serif Tibetan Light already registered with postscriptFonts().
Noto Serif Tibetan Medium already registered with postscriptFonts().
Noto Serif Tibetan Thin already registered with postscriptFonts().
NovaMono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Nunito. Skipping setup for this font.
orya already registered with postscriptFonts().
More than one version of regular/bold/italic found for padmaa. Skipping setup for this font.
Pothana2000 already registered with postscriptFonts().
ProFont for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Roboto. Skipping setup for this font.
More than one version of regular/bold/italic found for Roboto Condensed. Skipping setup for this font.
Roboto Mono for Powerline already registered with postscriptFonts().
Roboto Mono Light for Powerline already registered with postscriptFonts().
Roboto Mono Medium for Powerline already registered with postscriptFonts().
Roboto Mono Thin for Powerline already registered with postscriptFonts().
Sagar already registered with postscriptFonts().
Space Mono already registered with postscriptFonts().
Space Mono for Powerline already registered with postscriptFonts().
Symbol Neu for Powerline already registered with postscriptFonts().
TAMu_Kadambri already registered with postscriptFonts().
TAMu_Kalyani already registered with postscriptFonts().
TAMu_Maduram already registered with postscriptFonts().
Tinos for Powerline already registered with postscriptFonts().
TSCu_Comic already registered with postscriptFonts().
TSCu_Paranar already registered with postscriptFonts().
TSCu_Times already registered with postscriptFonts().
Ubuntu already registered with postscriptFonts().
Ubuntu Light already registered with postscriptFonts().
Ubuntu Condensed already registered with postscriptFonts().
Ubuntu Mono already registered with postscriptFonts().
Ubuntu Mono derivative Powerline already registered with postscriptFonts().
#+end_example

***** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/blocking_unrolling.pdf" :width 17.4 :height 6 :eval no-export
library(ggplot2)
library(dplyr)
library(tidyr)
library(paletteer)
library(patchwork)

font_family = "Liberation Sans"
base_size = 20

x_lab = "Columns"
y_lab = "Rows"

x_text = element_text(size = 20)
y_text = element_text(size = 20)

regular <- function(block_size) {
    matrix_size <- block_size * 4
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size)){
        for(j in seq(1, matrix_size)){
            df_matrix[i, j] <- iteration
            iteration <- iteration + 1
        }
    }

    return(df_matrix)
}

tiled <- function(block_size) {
    matrix_size <- block_size * 4
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size, by = block_size)){
        for(j in seq(1, matrix_size, by = block_size)){
            for(x in seq(i, min(i + block_size, matrix_size + 1) - 1)){
                for(y in seq(j, min(j + block_size, matrix_size + 1) - 1)){
                    df_matrix[x, y] <- iteration
                    iteration <- iteration + 1
                }
            }
        }
    }

    return(df_matrix)
}

unrolled <- function(block_size) {
    matrix_size <- block_size * 4
    unrolling_factor <- block_size
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size, by = block_size)){
        for(j in seq(1, matrix_size, by = block_size)){
            df_matrix[seq(i, min(i + block_size, matrix_size + 1) - 1),
                      seq(j, min(j + block_size, matrix_size + 1) - 1)] <- iteration
            iteration <- iteration + 1
        }
    }

    return(df_matrix)
}

to_tile <- function(df) {
    df %>%
        data.frame() %>%
        mutate(row = row_number()) %>%
        gather(key = "col", value = "order", -row) %>%
        mutate(col = unlist(
                   lapply(col,
                          function(x) {
                              as.integer(strsplit(x, "X")[[1]][2])
                          })))
}

regular_access <- to_tile(regular(4))
tiled_access <- to_tile(tiled(4))
unrolled_access <- to_tile(unrolled(4))

t_regular_access <- to_tile(t(regular(4)))
t_tiled_access <- to_tile(t(tiled(4)))
t_unrolled_access <- to_tile(t(unrolled(4)))

cf_palette <- "pals::jet"

p1 <- ggplot() +
    geom_tile(data = regular_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order),
              show.legend = FALSE) +
    scale_fill_paletteer_c(cf_palette) +
    scale_color_paletteer_c(cf_palette) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(a) Regular Access Pattern") +
    theme_bw(base_size = base_size) +
    theme(axis.title.x = x_text,
          axis.title.y = y_text,
          text = element_text(family = font_family),
          legend.position = "bottom",
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 16),
          legend.title = element_text(size = 23, margin = margin(r = 10)),
          legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

p2 <- ggplot() +
    geom_tile(data = tiled_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order),
              show.legend = FALSE) +
    scale_fill_paletteer_c(cf_palette) +
    scale_color_paletteer_c(cf_palette) +
    #scale_color_paletteer_c(name = "Access Order",
    #                        cf_palette,
    #                        guide = guide_colorbar(title.vjust = 1.1,
    #                                               title.hjust = 1,
    #                                               label.position = "bottom"),
    #                        breaks = c(1, 256),
    #                        labels = c("First", "Last")) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(b) After Tiling") +
    theme_bw(base_size = base_size) +
    theme(axis.title.x = x_text,
          axis.title.y = y_text,
          text = element_text(family = font_family),
          legend.position = "bottom",
          legend.direction = "horizontal",
          # legend.background = element_rect(fill = "transparent", colour = NA),
          # legend.text = element_text(size = 16),
          legend.title = element_text(margin = margin(r = 40)),
          # legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

p3 <- ggplot() +
    geom_tile(data = tiled_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order)) +
    scale_color_paletteer_c(cf_palette) +
    scale_fill_paletteer_c(name = "Order",
                           cf_palette,
                           guide = guide_colorbar(barwidth = 0.7,
                                                  barheight = 20,
                                                  direction = "vertical",
                                                  label.hjust = 1,
                                                  ticks = FALSE,
                                                  reverse = FALSE),
                           limits = c(1, 256),
                           breaks = c(1, 256),
                           labels = c("Last", "First")) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(c) After Tiling and Unrolling") +
    theme_bw(base_size = base_size) +
    theme(text = element_text(family = font_family),
          axis.title.x = x_text,
          axis.title.y = y_text,
          legend.position = "right",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 20),
          legend.title = element_text(size = 23, margin = margin(b = 15)),
          # legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    guides(color = FALSE)

p1 + p2 + p3
#+end_SRC

#+RESULTS:
[[file:./img/blocking_unrolling.pdf]]



** Chapter 3
*** Complete Classification Tree for Function Minimization Methods
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" )
#+HEADER: :exports results :results raw :file ./img/tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    l sep+=1em
  },
  [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
      draw,
      color = NavyBlue
      [{Search Heuristics},
        draw,
        color = BurntOrange,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{\textbf{Random} \textbf{Sampling}}, draw]
        [{Reachable Optima},
          draw,
          color = BurntOrange
          [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
            draw,
            color = BurntOrange
            [{Strong $corr(f(X),d(X,X_{*}))$?},
              draw,
              color = NavyBlue
              [{More Global},
                draw,
                color = BurntOrange,
                edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                [, phantom]]
              [{More Local},
                draw,
                color = BurntOrange,
                edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                [, phantom]
                [{High local optima density?},
                  draw,
                  color = NavyBlue
                  [{Exploit Steepest Descent},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                    [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                  [{Allows\\exploration},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                    [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
            [,phantom]]
          [,phantom]]]
      [{Statistical Learning},
        draw,
        color = BurntOrange,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Parametric Learning},
          draw,
          color = BurntOrange
          [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
            draw,
            color = BurntOrange
            [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
            [, phantom]]
          [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
            draw,
            color = BurntOrange
            [, phantom]
            [{Check for model adequacy?},
              draw,
              alias = adequacy,
              color = NavyBlue
              [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                draw,
                alias = interactions,
                color = NavyBlue,
                edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                  edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                  draw
                  [, phantom]
                  [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                    edge = {-stealth, ForestGreen, semithick},
                    edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                    draw,
                    alias = estimate,
                    color = ForestGreen]]
                [{\textbf{Optimal} \textbf{Design}},
                  draw,
                  alias = optimal,
                  edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [{\textbf{Space-filling} \textbf{Designs}},
                draw,
                edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                [, phantom]
                [{Model selection},
                  edge = {-stealth, ForestGreen, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                  draw,
                  alias = selection,
                  color = ForestGreen]]]]]
        [{Nonparametric Learning},
          draw,
          color = BurntOrange
          [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
            draw
            [, phantom]
            [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
              edge = {-stealth, ForestGreen, semithick},
              draw,
              alias = uncertainty,
              color = ForestGreen
              [{Minimize $uncertainty(\hat{f}(X))$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                draw,
                color = ForestGreen]
              [{Minimize $\hat{f}(X)$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                draw,
                color = ForestGreen]
              [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                draw,
                color = ForestGreen]]]
          [{\textbf{Gaussian} \textbf{Process Regression}},
            alias = gaussian,
            draw]
          [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
  \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
  \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
  \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
  \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/tree.pdf]]

*** Simplest Version
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" )
#+HEADER: :exports results :results raw :file ./img/simplest_tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    if n children=0{tier=terminal}{},
    l sep+=1em
  },
  [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$?},
      draw,
      [{Search\\Heuristics},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{\textbf{Random}\\\textbf{Sampling}}, draw]
        [{Reachable\\Optima},
          draw,
          [{Introduce a\\\textit{population} of $X$\\[0.5em]\textbf{Genetic}\\\textbf{Algorithms}}, draw]
          [{Estimate $f^{\prime}(X)$\\[0.5em]\textbf{Gradient}\\\textbf{Descent}}, draw]
          [{Allow worse $f(X)$\\[0.5em]\textbf{Simulated}\\\textbf{Annealing}}, draw]]]
      [{Statistical\\Learning},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Parametric\\Learning},
          draw,
          [{\textbf{Independent}\\\textbf{Bandits}}, draw]
          [{Choose best $X$ for\\
              a Linear Model\\
              $\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$\\[0.5em]
              \textbf{Experimental}\\\textbf{Design}},
              draw]
          [{Unsupervised\\[0.5em]\textbf{Clustering}}, draw]]
        [{Nonparametric\\Learning},
          draw,
          [{\textbf{Decision}\\\textbf{Trees}},
            draw]
          [{\textbf{Gaussian}\\\textbf{Process}\\\textbf{Regression}},
            draw]
          [{\textbf{Neural}\\\textbf{Networks}}, draw]]]]]
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/simplest_tree.pdf]]

*** Representing Sampling Strategies
**** Generate Fake Data with Algorithms
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
fake_gradient_data_seed <- data.frame(x1 = c(1, 1, 99, 99),
                                      x2 = c(1, 99, 1, 99),
                                      run = c(1, 2, 3, 4),
                                      sign1 = c(1, 1, -1, -1),
                                      sign2 = c(1, -1, 1, -1))

fake_gradient_data <- NULL

for(run_id in c(1, 2, 3, 4)) {
  if (is.null(fake_gradient_data)) {
      fake_gradient_data <- fake_gradient_data_seed[run_id, ]
  } else {
      fake_gradient_data <- rbind(fake_gradient_data, fake_gradient_data_seed[run_id, ])
  }

  for(i in 1:10) {
      row <- nrow(fake_gradient_data)
      fake_descent <- data.frame(x1 = ceiling(fake_gradient_data[row, "x1"] + (fake_gradient_data[row, "sign1"] * runif(1, min = 1, max = 5))),
                                 x2 = ceiling(fake_gradient_data[row, "x2"] + (fake_gradient_data[row, "sign2"] * runif(1, min = 1, max = 5))),
                                 run = fake_gradient_data[row, "run"],
                                 sign1 = fake_gradient_data[row, "sign1"],
                                 sign2 = fake_gradient_data[row, "sign2"])
      fake_gradient_data <- rbind(fake_gradient_data, fake_descent)
  }
}

fake_gradient_data$name <- rep("Gradient Descent", nrow(fake_gradient_data))
data <- bind_rows(data, fake_gradient_data)

fake_sima_data_seed <- data.frame(x1 = c(30, 30, 70, 70),
                                  x2 = c(30, 70, 30, 70),
                                  run = c(1, 2, 3, 4),
                                  sign1 = c(1, 1, -1, -1),
                                  sign2 = c(1, -1, 1, -1))

fake_sima_data <- NULL

for(run_id in c(1, 2, 3, 4)) {
  if (is.null(fake_sima_data)) {
      fake_sima_data <- fake_sima_data_seed[run_id, ]
  } else {
      fake_sima_data <- rbind(fake_sima_data, fake_sima_data_seed[run_id, ])
  }

  for(i in 1:10) {
      row <- nrow(fake_sima_data)
      fake_descent <- data.frame(x1 = ceiling(fake_sima_data[row, "x1"] + (fake_sima_data[row, "sign1"] * runif(1, min = -5, max = 5))),
                                 x2 = ceiling(fake_sima_data[row, "x2"] + (fake_sima_data[row, "sign2"] * runif(1, min = -5, max = 5))),
                                 run = fake_sima_data[row, "run"],
                                 sign1 = fake_sima_data[row, "sign1"],
                                 sign2 = fake_sima_data[row, "sign2"])
      fake_sima_data <- rbind(fake_sima_data, fake_descent)
  }
}

fake_sima_data$name <- rep("Simulated Annealing", nrow(fake_sima_data))
data <- bind_rows(data, fake_sima_data)
#+END_SRC
**** Generate Data
#+HEADER: :results output :session *R* :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)
library(RColorBrewer)

sample_size <- 50
pre_sample_size <- 30 * sample_size
search_space_size <- 100

center_x1 <- (search_space_size / 2) - 30
center_x2 <- (search_space_size / 2) - 30

get_cost <- function(data) {
    return(((data$x1 - center_x1) ^ 2) + ((data$x2 - center_x2) ^ 2) + ((abs((data$x1 - center_x1) * (data$x2 - center_x2)))**.7 * sin((data$x1 - center_x1) * (data$x2 - center_x2))))
}

objective_data <- expand.grid(seq(0, search_space_size, 1),
                              seq(0, search_space_size, 1))
names(objective_data) <- c("x1", "x2")

objective_data$Y <- get_cost(objective_data)

sima_samples <- 15

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_sima_data <- as.data.frame(locator(n = sima_samples, type = "l"))
names(fake_sima_data) <- c("x1", "x2")
dev.off()

fake_sima_data$run <- c(rep(1, nrow(fake_sima_data)))
fake_sima_data$name <- rep("Simulated Annealing", nrow(fake_sima_data))

fake_sima_data$cost <- get_cost(fake_sima_data)
fake_sima_data$min <- fake_sima_data$cost == min(fake_sima_data$cost)

data <- fake_sima_data

descent_samples <- 20

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_descent_data <- as.data.frame(locator(n = descent_samples, type = "l"))
names(fake_descent_data) <- c("x1", "x2")
dev.off()

paths <- 5
fake_runs <- rep(1, descent_samples / paths)
for(i in 2:paths){
  fake_runs <- c(fake_runs, rep(i, descent_samples / paths))
}

fake_descent_data$run <- fake_runs
fake_descent_data$name <- rep("Gradient Descent", nrow(fake_descent_data))

fake_descent_data$cost <- get_cost(fake_descent_data)
fake_descent_data$min <- fake_descent_data$cost == min(fake_descent_data$cost)

data <- bind_rows(data, fake_descent_data)

rs_data <- data.frame(x1 = sample(0:search_space_size, sample_size, replace = T),
                      x2 = sample(0:search_space_size, sample_size, replace = T))
rs_data$name <- rep("Random Sampling", nrow(rs_data))

rs_data$cost <- get_cost(rs_data)
rs_data$min <- rs_data$cost == min(rs_data$cost)

data <- bind_rows(data, rs_data)

lhs_data <- lhs.design(nruns = sample_size, nfactors = 2, digits = 0, type = "maximin",
                       factor.names = list(x1 = c(0, search_space_size), x2 = c(0, search_space_size)))
lhs_data$name <- rep("Latin Hypercube Sampling", nrow(lhs_data))

lhs_data$cost <- get_cost(lhs_data)
lhs_data$min <- lhs_data$cost == min(lhs_data$cost)

data <- bind_rows(data, lhs_data)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2, full_factorial, nTrials = sample_size)
dopt_data <- output$design

dopt_data$name <- rep("DOpt. Linear Model", nrow(dopt_data))
dopt_data$cost <- get_cost(dopt_data)
dopt_data$min <- rep(FALSE, nrow(dopt_data))

regression <- lm(cost ~ x1 + x2, data = dopt_data)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Linear Model"
best$min <- TRUE

dopt_data <- bind_rows(dopt_data, best)
data <- bind_rows(data, dopt_data)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
doptq_data <- output$design

doptq_data$name <- rep("DOpt. Quadratic Model", nrow(doptq_data))
doptq_data$cost <- get_cost(doptq_data)
doptq_data$min <- rep(FALSE, nrow(doptq_data))

regression <- lm(cost ~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), data = doptq_data)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Quadratic Model"
best$min <- TRUE

doptq_data <- bind_rows(doptq_data, best)
data <- bind_rows(data, doptq_data)
#+END_SRC

#+RESULTS:
#+begin_example
Want to understand how all the pieces fit together? See the R for Data
Science book: http://r4ds.had.co.nz/
Loading required package: FrF2
Loading required package: DoE.base
Loading required package: grid
Loading required package: conf.design

Attaching package: ‘DoE.base’

The following objects are masked from ‘package:stats’:

    aov, lm

The following object is masked from ‘package:graphics’:

    plot.design

The following object is masked from ‘package:base’:

    lengths

Loading required package: rsm

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

**** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file ./img/sampling_comparison.pdf :exports none :width 15 :height 11.5 :eval no-export
#+BEGIN_SRC R
library(extrafont)
data$facet <- factor(data$name, levels = c("Random Sampling",
                                           "Latin Hypercube Sampling",
                                           "Gradient Descent",
                                           "Simulated Annealing",
                                           "DOpt. Linear Model",
                                           "DOpt. Quadratic Model"))
ggplot(data, aes(x = x1, y = x2)) +
    scale_x_continuous(limits = c(-1, 101),
                       expand = c(0, 0)) +
    scale_y_continuous(limits = c(-1, 101),
                       expand = c(0, 0)) +
    xlab("x") +
    ylab("y") +
    facet_wrap(facet ~ .,
               ncol = 3) +
    #geom_raster(data = objective_data, aes(fill = Y), show.legend = FALSE) +
    #geom_contour(data = objective_data, aes(z = Y), colour = "white", linetype = 8) + #, breaks = 1 * (2 ^ (2:20))) +
    geom_contour(data = objective_data,
                 aes(z = Y),
                 linetype = 1,
                 colour = "black",
                 alpha = 0.6,
                 show.legend = FALSE,
                 breaks = 1 * (2 ^ (4:20))) +
    geom_path(data = subset(data,
                            name %in% c("Gradient Descent", "Simulated Annealing")),
              aes(group = run),
              color = "black",
              alpha = 0.55,
              size = 1) +
    geom_point(shape = 19,
               size = 3,
               colour = "black",
               alpha = 0.55) +
    geom_jitter(data = subset(data,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.55,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(data,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.55,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(data,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.45,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(data,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.45,
                width = 8,
                height = 8) +
    scale_fill_distiller(palette = "Greys",
                         direction = -1,
                         limits = c(min(objective_data$Y) - 1000,
                                    max(objective_data$Y))) +
    geom_point(data = subset(data,
                             min == TRUE),
               color = "red",
               shape = 3,
               size = 12,
               alpha = 1,
               stroke = 2) +
    theme_bw(base_size = 35) +
    theme(panel.grid = element_blank(),
          text = element_text(family="serif"),
          strip.background = element_rect(fill = "white"),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/sampling_comparison.pdf]]
* Introduction
** The Need for Autotuning
High Performance Computing  has been a cornerstone of  scientific and industrial
progress for at least five decades.  By paying the cost of increased complexity,
software  and  hardware  engineering   advances  continue  to  overcome  several
challenges on the way of  the sustained performance improvements observed during
the  last fifty  years.   A  consequence of  this  mounting  complexity is  that
reaching the theoretical peak hardware  performance for a given program requires
not only expert  knowledge of specific hardware architectures,  but also mastery
of programming models and languages for parallel and distributed computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance, we  can draw  from and  adapt proven  methods from  search,
mathematical optimization, and statistical  learning. The effectiveness of these
adapted methods on performance optimization  problems varies greatly, and hinges
on practical  and mathematical properties  of the problem and  the corresponding
/search space/. The  application of such methods to the  automation of performance
tuning for specific hardware, under a set of /constraints/, is named /autotuning/.

Improving performance  also relies on gathering  application-specific knowledge,
which entails extensive  experimental costs since, with the  exception of linear
algebra  routines,  theoretical  peak  performance is  not  always  a  reachable
comparison  baseline.   When  adapting  methods  for  autotuning  we  must  face
challenges emerging from practical properties,  such as restricted time and cost
budgets,  constraints  on  feasible  parameter  values,  and  the  need  to  mix
/categorical/, /continuous/, and /discrete/ parameters.   To achieve useful results we
must also  choose methods  that make hypotheses  compatible with  problem search
spaces,  such  as  the  existence  of /discoverable/,  or  at  least  /exploitable/,
relationships between parameters and performance.  Choosing an autotuning method
requires balancing  the exploration of a  problem, that is, seeking  to discover
and  explain   relationships  between   parameters  and  performance,   and  the
exploitation of known or discovered relationships, seeking only to find the best
possible performance.

Search  algorithms  based  on  machine  learning heuristics  are  not  the  best
candidates for  autotuning domains  where measurements  are lengthy  and costly,
such as compiling industrial-level FPGA  programs, because these algorithms rely
on the  availability of a  large number of  measurements. They also  assume good
optimizations  are  reachable from  a  starting  position, and  that  tendencies
observed locally in the search space are exploitable.  These assumptions are not
usually true in  common autotuning domains, as  shown in the work  of Seymour /et
al./\nbsp\cite{seymour2008comparison}.

Autotuning search spaces also usually  have non-linear constraints and undefined
regions, which are  also expected to decrease the effectiveness  of search based
on heuristics and  machine learning.  An additional downside  to heuristics- and
machine learning-based search  is that, usually, optimization  choices cannot be
explained, and knowledge  gained during optimization is not  reusable.  The main
contribution of this  thesis is to study  how to overcome the  reliance on these
assumptions about search spaces, and the lack of explainable optimizations, from
the point  of view  of a  /Design of Experiments/  (DoE), or  /Experimental Design/,
methodology to autotuning.

One of  the first detailed  descriptions and  mathematical treatment of  DoE was
presented  by Ronald  Fisher\nbsp\cite{fisher1937design}  in his  1937 book  /The
Design of Experiments/,  where he discussed principles  of experimentation, latin
square  sampling and  factorial  designs.  Later  books such  as  the ones  from
Jain\nbsp\cite{bukh1992art}, Montgomery\nbsp\cite{montgomery2017design}  and Box
/et   al./\nbsp\cite{box2005statistics}   present   comprehensive   and   detailed
foundations.   Techniques  based on  DoE  are  /parsimonious/ because  they  allow
decreasing   the  number   of   measurements  required   to  determine   certain
relationships  between  parameters  and  metrics, and  are  /transparent/  because
parameter  selections and  configurations can  be  justified by  the results  of
statistical tests.

In DoE terminology, a  /design/ is a plan for executing  a series of measurements,
or /experiments/, whose objective is to identify relationships between /factors/ and
/responses/.  While factors and responses can refer to different concrete entities
in  other  domains,  in  computer   experiments  factors  can  be  configuration
parameters for algorithms  and compilers, for example, and responses  can be the
execution time or memory consumption of a program.

Designs  can  serve diverse  purposes,  from  identifying the  most  significant
factors  for  performance, to  fitting  analytical  performance models  for  the
response.  The  field of DoE  encompasses the mathematical formalization  of the
construction of experimental designs.  More practical works in the field present
algorithms to generate designs with different objectives and restrictions.

The contributions of  this thesis are strategies to apply  to program autotuning
the         DoE         methodology,        and         /Gaussian         Process
Regression/\nbsp{}\cite{williams2006gaussian}.   This thesis  presents background
and  a high-level  view  of  the theoretical  foundations  of  each method,  and
detailed  discussions of  the challenges  involved in  specializing the  general
definitions of search  heuristics and statistical learning  methods to different
autotuning problems,  as well as what  can be /learned/ about  specific autotuning
search spaces,  and how  that acquired  knowledge can  be leveraged  for further
optimization.

This  chapter aims  to substantiate  the claim  that autotuning  methods have  a
fundamental  role to  play on  the future  of program  performance optimization,
arguing  that the  value and  the difficulty  of the  efforts to  carefully tune
software became more apparent ever since advances in hardware stopped leading to
effortless performance improvements, at least from the programmer's perspective.
The following sections discuss the historical  context for the changes in trends
on  computer  architecture,  and  characterize  the  search  spaces  found  when
optimizing performance on different domains.

*** Historical Trends in Hardware Design
The physical  constraints imposed  by technological  advances on  circuit design
were evident since  the first vacuum tube computers that  already spanned entire
floors,  such   as  the  ENIAC  in   1945\nbsp{}\cite{ceruzzi2003history}.   The
practical and  economical need to fit  more computing power into  real estate is
one  force for  innovation in  hardware design  that spans  its history,  and is
echoed in  modern supercomputers,  such as  the /Summit/  from /Oak  Ridge National
Laboratory/\nbsp{}\cite{olcf2020summit}, which spans an entire room.

#+NAME: fig:trends
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: 49 years of microprocessor data, highlighting the sustained exponential
#+CAPTION: increases and reductions on transistor counts and fabrication processes, the
#+CAPTION: stagnation of frequency scaling around 2005, and one solution found for it,
#+CAPTION: the simultaneous exponential increase on logical core count.
#+CAPTION: Data from Wikipedia\nbsp{}\cite{wiki2020transistor,wiki2020chronology}
[[file:img/49_years_processor_data.pdf]]

Figure\nbsp{}[[fig:trends]] highlights the unrelenting and so far successful pursuit
of smaller transistor fabrication processes, and the resulting capability to fit
more computing power on  a fixed chip area.  This trend  was already observed in
integrated      circuits      by      Gordon      Moore      /et      al./      in
1965\nbsp{}\cite{moore1965cramming},  who also  postulated its  continuity.  The
performance improvements produced by the design efforts to make Moore's forecast
a self-fulfilling  prophecy were  boosted until around  2005 by  the performance
gained from increases in circuit frequency.

Robert  Dennard  /et  al./ remarked  in  1974\nbsp{}\cite{dennard1974design}  that
smaller  transistors, in  part  because they  generate  shorter circuit  delays,
decrease  the energy  required to  power  a circuit  and enable  an increase  in
operation  frequency without  breaking  power usage  constraints.  This  scaling
effect,  named /Dennard's  scaling/,  is hindered  primarily  by leakage  current,
caused    by     quantum    tunneling    effects    in     small    transistors.
Figure\nbsp{}[[fig:trends]] shows  a marked  stagnation on frequency  increase after
around 2005,  as transistors crossed  the 10^{2}nm fabrication process.   It was
expected that leakage  due to tunneling would limit  frequency scaling strongly,
even     before      the     transistor     fabrication      process     reached
10nm\nbsp{}\cite{frank2001device}.

Current hardware is now past the  effects of Dennard's scaling.  The increase in
logical cores around  2015 can be interpreted as preparation  for and mitigation
of the  end of frequency  scaling, and ushered in  an age of  multicore scaling.
Still, in order to meet power consumption constraints, up to half of a multicore
processor could have to be powered down, at all times.  This phenomenon is named
/Dark   Silicon/\nbsp{}\cite{esmaeilzadeh2011dark},   and   presents   significant
challenges        to        current         hardware        designers        and
programmers\nbsp{}\cite{venkataramani2015approximate,cheng2015core,henkel2015new}.

The   /Top500/\nbsp{}\cite{top5002020list}   list    gathers   information   about
commercially  available supercomputers,  and ranks  them by  performance on  the
/LINPACK/ benchmark\nbsp{}\cite{dongarra2003linpack}.  Figure\nbsp{}[[fig:rmax-rpeak]]
shows  the peak  theoretical performance  $RPeak$, and  the maximum  performance
achieved  on the  LINPACK benchmark  $RMax$, in  $Tflops/s$, for  the top-ranked
supercomputers on TOP500.   Despite the smaller performance  gains from hardware
design  that are  to  be  expected for  post-Dennard's  scaling processors,  the
increase in computer  performance has sustained an  exponential climb, sustained
mostly by software improvements.

Although  /hardware accelerators/  such as  GPUs and  FPGAs, have  also helped  to
support exponential performance  increases, their use is not an  escape from the
fundamental  scaling  constraints  imposed   by  current  semiconductor  design.
Figure\nbsp{}[[fig:acc-cores]] shows the increase  in processor and accelerator core
count  on the  top-ranked  supercomputers  on Top500.   Half  of the  top-ranked
supercomputers in the  last decade had accelerator cores and,  of those, all had
around ten times more accelerator than processor cores.  The apparent stagnation
of  core  count in  top-ranked  supercomputers,  even considering  accelerators,
highlights the crucial impact software optimization has on performance.

#+NAME: fig:rmax-rpeak
#+CAPTION: Sustained exponential increase of theoretical \textit{RPeak} and
#+CAPTION: achieved \textit{RMax} performance for
#+CAPTION: the supercomputer ranked 1^{st} on TOP500\nbsp{}\cite{top5002020list}
#+ATTR_LATEX: :width \textwidth :placement [t]
[[file:./img/top500_rmax_rpeak.pdf]]

#+NAME: fig:acc-cores
#+CAPTION: Processor and accelerator core count
#+CAPTION: in supercomputers ranked 1^{st} on
#+CAPTION: TOP500\nbsp{}\cite{top5002020list}.
#+CAPTION: Core  count trends for supercomputers are not
#+CAPTION: necessarily bound to processor trends observed on
#+CAPTION: Figure\nbsp{}\ref{fig:trends}.
#+ATTR_LATEX: :width \textwidth :placement [h!]
[[file:./img/top500_accelerator_cores.pdf]]

Advances in hardware  design are currently not capable  of providing performance
improvements  via frequency  scaling  without dissipating  more  power than  the
processor was  designed to support,  which violates power constraints  and risks
damaging the circuit.  From the programmer's perspective, effortless performance
improvements from hardware  have not been expected for quite  some time, and the
key  to  sustaining  historical  trends  in  performance  scaling  has  lied  in
accelerators, parallel and distributed programming libraries, and fine tuning of
several stages of  the software stack, from instruction selection  to the layout
of neural networks.

The problem of optimizing software  for performance presents its own challenges.
The search  spaces that emerge from  autotuning problems grow quickly  to a size
for which  it would  take a  prohibitive amount  of time  to determine  the best
configuration by exhaustively evaluating  all possibilities. Although this means
we must  seek to decrease  the amount  of possibilities, by  restricting allowed
parameter values, or dropping parameters completely,  it is often unclear how to
decide  which parameters  should be  restricted or  dropped.  The  next sections
introduce a simple  autotuning problem, present an overview of  the magnitude of
the dimension  of autotuning  search spaces, and  briefly introduce  the methods
commonly used to explore search spaces, some of which are discussed in detail in
Chapter\nbsp{}[[Methods for Function Minimization]].
*** Loop Nest Optimization as an Autotuning Problem
Algorithms for linear  algebra problems are fundamental  to scientific computing
and statistics. Therefore,  decreasing the execution time of  algorithms such as
general  matrix multiplication  (GEMM)\nbsp{}\cite{dongarra1990set}, and  others
from the original BLAS\nbsp{}\cite{lawson1979basic},  is an interesting and well
motivated example, that we will use to introduce the autotuning problem.

One way to improve the performance of such linear algebra programs is to exploit
cache locality by  reordering and organizing loop iterations,  using source code
transformation methods such as loop /tiling/, or /blocking/, and /unrolling/.  We will
now  briefly describe  loop tiling  and unrolling  for a  simple linear  algebra
problem. After,  we will  discuss an  autotuning search  space for  blocking and
unrolling applied to  GEMM, and how these transformations  generate a relatively
large and complex search space, which we can explore using autotuning methods.

Figure\nbsp{}\ref{fig:transpose-c}  shows  three  versions  of  code  in  the  /C/
language that, given three square matrices $A$,  $B$, and $C$, computes $C = C +
A +  B^{\top}$. The first  optimization we can make  is to preemptively  load to
cache, or /prefetch/, as  many as possible of the elements we  know will be needed
at       any       given       iteration,       as       is       shown       in
Figure\nbsp{}\ref{fig:transpose-regular-access}. The shaded  elements on the top
row of Figure\nbsp{}[[fig:blocking-unrolling]] represent  the elements that could be
prefetched in iterations of Figure\nbsp{}\ref{fig:transpose-regular-access}.

Since /C/ matrices are stored in /row-major order/, each access of an element of $B$
forces loading the next row elements, even if we explicitly prefetch a column of
$B$.  Since  we are accessing  $B$ in a  /column-major order/, the  prefetched row
elements would not be used until we reached the corresponding column. Therefore,
the next column elements will have  to be loaded at each iteration, considerably
slowing down the computation.

# #+NAME: fig:blocking-unrolling
# #+ATTR_LATEX: :width \textwidth :placement [t]
# #+CAPTION: Access patterns on GEMM, for the \textit{destination} matrix, using
# #+CAPTION: loop  nest optimizations. Panel \textit{(a)} shows the access order
# #+CAPTION: of a naive implementation, panel \textit{(b)} shows the effect of
# #+CAPTION: loop \textit{tiling}, or \textit{blocking}, and panel \textit{(c)}
# #+CAPTION: shows the compounded effect of loop \textit{unrolling}
# [[file:./img/blocking_unrolling.pdf]]

#+begin_export latex
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      float A[N][N], B[N][N], C[N][N];
      int i, j;
      // Initialize A, B, C
      for(i = 0; i < N; i++){
        // Load line i of A to fast memory
        for(j = 0; j < N; j++){
          // Load C[i][j] to fast memory
          // Load column j of B to fast memory
          C[i][j] += A[i][j] + B[j][i];
          // Write C[i][j] to main memory
        }
      }
    \end{lstlisting}
    \caption{Regular implementation}
    \label{fig:transpose-regular-access}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, x, y;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          // Load block (i, j) of A to fast memory
          // Load block (j, i) of B to fast memory
          for(x = i; x < min(i + B_size, N); x++){
            for(y = j; y < min(j + B_size, N); y++){
              C[x][y] += A[x][y] + B[y][x];
            }
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Blocked, or tiled}
    \label{fig:transpose-blocked-tiled}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          // Load block (i, j) of A to fast memory
          // Load block (j, i) of B to fast memory
          C[i + 0][j + 0] += A[i + 0][j] * B[i][j + 0];
          C[i + 0][j + 1] += A[i + 0][j] * B[i][j + 1];
          // Unroll the remaining 12 iterations
          C[i + Bsize - 1][j + B_size - 2] += A[i + Bsize - 1][j] * B[i][j + B_size - 2];
          C[i + Bsize - 1][j + B_size - 1] += A[i + Bsize - 1][j] * B[i][j + B_size - 1];
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Tiled and unrolled}
    \label{fig:transpose-unrolling}
  \end{subfigure}
  \caption{Loop nest optimizations for $C = C + A + B^{\top}$, in C}
  \label{fig:transpose-c}
\end{figure}
#+end_export

#+NAME: fig:blocking-unrolling
#+ATTR_LATEX: :width .7\textwidth :placement [h!]
#+CAPTION: Access patterns for matrices in $C = C + A + B^{\top}$, with
#+CAPTION: loop  nest optimizations. Panel \textit{(a)} shows the access order
#+CAPTION: of a regular implementation, and panel \textit{(b)} shows the effect of
#+CAPTION: loop \textit{tiling}, or \textit{blocking}
[[file:./img/access_patterns.pdf]]

We  can  solve this  problem  by  reordering  memory  accesses to  request  only
prefetched elements. It  suffices to adequately split loop  indices into blocks,
as shown in Figure\nbsp{}\ref{fig:transpose-blocked-tiled}. Now, memory accesses
are   be   performed    in   /tiles/,   as   shown   on   the    bottom   row   of
Figure\nbsp{}[[fig:blocking-unrolling]].  If  blocks are  correctly sized to  fit in
cache, we  can improve performance  by explicitly prefetching each  tile.  After
blocking, we can  still improve performance by /unrolling/  loop iterations, which
forces register  usage and helps  the compiler to  identify regions that  can be
/vectorized/.   A  conceptual  implementation  of   loop  unrolling  is  shown  in
Figure\nbsp{}\ref{fig:transpose-unrolling}.

Looking at the  loop nest optimization problem from  the autotuning perspective,
the two  /parameters/ that  emerge from  the implementations  are the  /block size/,
which controls the  stride, and the /unrolling factor/, which  controls the number
of unrolled  iterations.  Larger block sizes  are desirable, because we  want to
avoid extra comparisons,  but blocks should be small enough  to ensure access to
as few as possible out-of-cache elements.  Likewise, the unrolling factor should
be large,  to leverage vectorization and  available registers, but not  so large
that it forces memory to the stack.

The values  of block size  and unrolling  factor that optimize  performance will
depend on the  cache hierarchy, register layout,  and vectorization capabilities
of the  target processor, but  also on the memory  access pattern of  the target
algorithm.   In  addition  to  finding   the  best  values  for  each  parameter
independently, an  autotuner must  ideally aim to  account for  the /interactions/
between parameters, that is, for the fact that the best value for each parameter
might also depend on the value chosen for the other.

The    next    loop    optimization    example    comes    from    Seymour    /et
al./\nbsp{}\cite{seymour2008comparison}, and considers 128 blocking and unrolling
values, in the interval $[0,127]$, for  the GEMM algorithm.  The three panels of
Figure\nbsp{}\ref{fig:gemm-c} show  conceptual implementations of  loop blocking
and unrolling for GEMM in /C/.  A block size of zero results in the implementation
from  Figure\nbsp{}\ref{fig:regular-access}, and  an  unrolling  factor of  zero
performs a single iteration per condition check.

#+begin_export latex
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      float A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i++){
        // Load line i of A to fast memory
        for(j = 0; j < N; j++){
          // Load C[i][j] to fast memory
          // Load column j of B to fast memory
          for(k = 0; k < N; k++){
            C[i][j] += A[i][k] * B[k][j];
          }
          // Write C[i][j] to main memory
        }
      }
    \end{lstlisting}
    \caption{Regular implementation}
    \label{fig:regular-access}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k, x, y;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          for(k = 0; k < N; k++){
            // Load block (i, k) of A to fast memory
            // Load block (k, y) of B to fast memory
            for(x = i; x < min(i + B_size, N); x++){
              for(y = j; y < min(j + B_size, N); y++){
                C[x][y] += A[x][k] * B[k][y];
              }
            }
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Blocked, or tiled}
    \label{fig:blocked-tiled}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          for(k = 0; k < N; k++){
            // Load block (i, k) of A to fast memory
            // Load block (k, y) of B to fast memory
            C[i + 0][j + 0] += A[i + 0][k] * B[k][j + 0];
            C[i + 0][j + 1] += A[i + 0][k] * B[k][j + 1];
            // Unroll the remaining 12 iterations
            C[i + Bsize - 1][j + B_size - 2] += A[i + Bsize - 1][k] * B[k][j + B_size - 2];
            C[i + Bsize - 1][j + B_size - 1] += A[i + Bsize - 1][k] * B[k][j + B_size - 1];
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Tiled and unrolled}
    \label{fig:unrolling}
  \end{subfigure}
  \caption{Loop nest optimizations for GEMM, in C}
  \label{fig:gemm-c}
\end{figure}
#+end_export

#+NAME: fig:search-space
#+ATTR_LATEX: :width .6\textwidth :placement [h!]
#+CAPTION: An exhaustively measured search space, defined by loop blocking and unrolling
#+CAPTION: parameters, for a sequential GEMM kernel.
#+CAPTION: Reproduced from Seymour \textit{et al.}\nbsp{}\cite{seymour2008comparison}
[[file:img/seymour2008comparison.pdf]]

It  is straightforward  to change  the block  size of  the implementations  from
Figure\nbsp{}\ref{fig:gemm-c},  but the  unrolling factor  is not  exposed as  a
parameter.  To test different unrolling values  we need to generate new versions
of the  source code with  different numbers of  unrolled iterations.  We  can do
that   with   code   generators    or   with   /source-to-source   transformation/
tools\nbsp{}\cite{videau2017boast,hartono2009annotation,ansel2009petabricks}.
It is  often necessary to  modify the  program we wish  to optimize in  order to
provide a configuration  interface and expose its implicit  parameters.  Once we
are able to control  the block size and the loop  unrolling factor, we determine
the target search space by choosing the values to be explored.

In this  example, the search  space is defined by  the $128^2 =  16384$ possible
combinations  of  blocking  and  unrolling  values.   The  performance  of  each
combination    in     the    search     space,    shown    in     /Mflops/s/    in
Figure\nbsp{}[[fig:search-space]],    was   measured    for   a    sequential   GEMM
implementation,        using        square        matrices        of        size
400\nbsp{}\cite{seymour2008comparison}. We can  represent this autotuning search
space as a /3D landscape/, since we  have two configurable parameters and a single
target  performance metric.   In  this setting,  the objective  is  to find  the
/highest/ point, since the objective is to /maximize/ Mflops/s, although usually the
performance metric is transformed so that the objective is its /minimization/.

On a first look, there seems to  be no apparent global search space structure in
the landscape on  Figure\nbsp{}[[fig:search-space]], but local features  jump to the
eyes, such as  the ``valley'' across all block sizes  for low unrolling factors,
the ``ramp'' across all unrolling factors for low block sizes, and the series of
jagged  ``plateaus'' across  the middle  regions, with  ridges for  identical or
divisible block sizes  and unrolling factors.  A careful look  reveals also that
there is  a curvature  along the  unrolling factor  axis.  Also  of note  is the
abundance in  this landscape of  /local minima/,  that is, points  with relatively
good performance, surrounded  by points with worse  performance. By exhaustively
evaluating  all  possibilities, the  original  study  determined that  the  best
performance  on this  program  was achieved  with  a  block size  of  80 and  an
unrolling factor of 2.

In this  conceptual example,  all ${\approx}1.64\times10^4$  configurations were
exhaustively evaluated,  but it is  impossible to do  so in most  settings where
autotuning methods are  useful.  The next section provides a  perspective of the
autotuning  domains  and methods  employed  in  current research,  presenting  a
selection of  search spaces and  discussing the trends  that can be  observed on
search space size, targeted HPC domains, and chosen optimization methods.

*** Characterizing Search Spaces
Autotuning  methods have  been used  to improve  performance in  an increasingly
large  variety of  domains,  from  the earlier  applications  to linear  algebra
subprograms,  to the  now ubiquitous  construction and  configuration of  neural
networks,  to the  configuration  of  the increasingly  relevant  tools for  the
re-configurable hardware  of FPGAs.  In this  setting, it is not  far-fetched to
establish a  link between  the continued increases  in performance  and hardware
complexity, that  we discussed previously in  this chapter, to the  increases in
dimension and size of the autotuning problems that we can now tackle.

Figure\nbsp{}[[fig:search-spaces]] presents search space  dimension, measured as the
number of  parameters involved,  and size,  measured as  the number  of possible
parameter  combinations, for  a selection  of search  spaces from  14 autotuning
domains.   Precise  information about  search  space  characterization is  often
missing from works on autotuning  methods and applications. The characterization
of  most of  the search  spaces in  Figure\nbsp{}[[fig:search-spaces]] was  obtained
directly from the text of the corresponding published paper, but for some it was
necessary to extract  characterizations from the available  source code.  Still,
it was impossible  to obtain detailed descriptions of search  spaces for many of
the published works on autotuning methods  and applications, and in that way the
sample shown in this section is  biased, because it contains only information on
works that provided it.

#+NAME: fig:search-spaces
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Dimension and search space size for autotuning problems from 14 domains
#+CAPTION: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,abdelfattah2016performance,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}
#+CAPTION: The left panel shows a zoomed view of the right panel
[[file:img/search_spaces.pdf]]

The left hand  panel of Figure\nbsp{}[[fig:search-spaces]] shows  search spaces with
up to 60 parameters. The over-representation of search spaces for linear algebra
domains in this sample stands out on the  left hand panel, but the domain is not
present on the remaining  portion of the sample, shown on  the right hand panel.
The  largest  search spaces  for  which  we were  able  to  find information  on
published  work are  defined for  the domains  of neural  network configuration,
High-Level  Synthesis  for  FPGAs,   compiler  parameters,  and  domain-specific
languages.

None of the largest search spaces in  this sample, that is, the ones outside the
zoomed area  of the  left hand  panel, come  from works  earlier than  2009. The
sustained  performance improvements  we  discussed previously  have enabled  and
pushed autotuning research toward progressively  larger problems, which has also
been  done to  most  research  areas.  Increased  computing  power  has made  it
feasible,  or at  least tolerable,  to apply  search heuristics  and statistical
learning methods to find program configurations that improve performance.

It is  straightforward to  produce an extremely  large autotuning  search space.
Compilers have  hundreds of binary flags  that can be considered  for selection,
generating  a large  set of  combinations. Despite  that, regarding  performance
improvements, it is likely that most  configuration parameters will have a small
impact, that is,  that only a handful of parameters  are responsible for changes
in performance.  Search  spaces are often much more restrictive  than the one we
discussed  in Section\nbsp{}[[Loop  Nest  Optimization as  an Autotuning  Problem]].
Autotuning problem definitions usually come with /constraints/ on parameter values
and limited /experimental  budgets/, and /runtime/ failures  for some configurations
are often  unpredictable. In this  context, finding configurations  that improve
performance   and  determining   the  subset   of  /significant   parameters/  are
considerable challenges.

Search  heuristics, such  as methods  based on  genetic algorithms  and gradient
descent, are  a natural way to  tackle these challenges because  they consist of
procedures for exploiting existing  and unknown relationships between parameters
and  performance  without making  or  requiring  /explicit hypotheses/  about  the
problem.  Despite that,  most commonly used heuristics  make /implicit hypotheses/
about search  spaces which are not  always verified, such as  assuming that good
configurations are /reachable/ from a random starting point.

#+begin_SRC R :results output latex :session *R* :eval no-export :exports results
library(knitr)
library(kableExtra)
library(dplyr)

df <- read.csv("data/search_spaces/search_methods.csv")

df <- df %>%
    mutate(citation_name = paste(name,
                                 "~\\cite{",
                                 gscholar_citation,
                                 "}",
                                 sep = "")) %>%
    select(citation_name,
           problem_domain,
           shorthand_method,
           year) %>%
    arrange(year)

names(df) <- c("System", "Domain", "Method", "Year")

caption <- paste("Autotuning methods used by a sample of systems,",
                 "in different domains, ordered by publishing year.",
                 "Methods were classified as either Search Heuristics (SH),",
                 "Machine Learning (ML), or more precisely",
                 "when the originating work provided detailed information.",
                 "Earlier work favored employing Search Heuristics, which",
                 "are less prominent in recent work, which favors methods",
                 "based on Machine Learning.")

kable(df,
      format = "latex",
      escape = FALSE,
      booktabs = TRUE,
      linesep = "",
      label = "search-methods",
      caption = caption) %>%
    kable_styling(latex_options = c("scale_down", "hold_position"))
#+end_SRC

#+RESULTS:
#+begin_export latex

\begin{table}[!h]

\caption{\label{tab:search-methods}Autotuning methods used by a sample of systems, in different domains, ordered by publishing year. Methods were classified as either Search Heuristics (SH), Machine Learning (ML), or more precisely when the originating work provided detailed information. Earlier work favored employing Search Heuristics, which are less prominent in recent work, which favors methods based on Machine Learning.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lllr}
\toprule
System & Domain & Method & Year\\
\midrule
PhiPAC~\cite{bilmes1997optimizing} & Linear Algebra & SH (Exhaustive) & 1997\\
ATLAS~\cite{dongarra1998automatically} & Linear Algebra & SH (Exhaustive) & 1998\\
FFTW~\cite{frigo1998fftw} & Digital Signal Processing & SH (Exhaustive) & 1998\\
Active Harmony~\cite{tapus2002active} & Domain-Specific Language & SH & 2002\\
OSKI~\cite{vuduc2005oski} & Linear Algebra & SH & 2005\\
Seymour, K. \textit{et al.}~\cite{seymour2008comparison} & Linear Algebra & SH & 2008\\
PRO~\cite{tiwari2009scalable} & Linear Algebra & SH & 2009\\
ParamILS~\cite{hutter2009paramils} & Combinatorial Auctions & SH & 2009\\
PetaBricks~\cite{ansel2009petabricks} & Domain-Specific Language & SH (Genetic Algorithm) & 2009\\
MILEPOST GCC~\cite{fursin2011milepost} & Compiler Parameters & ML & 2011\\
Orio~\cite{balaprakash2012spapt} & Linear Algebra & ML (Decision Trees) & 2012\\
pOSKI~\cite{byun2012autotuning} & Linear Algebra & SH & 2012\\
INSIEME~\cite{jordan2012multi} & Compiler Parameters & SH (Genetic Algorithm) & 2012\\
OpenTuner~\cite{ansel2014opentuner} & Compiler Parameters & SH & 2014\\
Lgen~\cite{spampinato2014basic} & Linear Algebra & SH & 2014\\
OPAL~\cite{audet2014optimization} & Parallel Computing & SH & 2014\\
Mametjanov, A. \textit{et al.}~\cite{mametjanov2015autotuning} & High-Level Synthesis & ML (Decision Trees) & 2015\\
CLTune~\cite{nugteren2015cltune} & Parallel Computing & SH & 2015\\
Guerreirro, J. \textit{et al.}~\cite{guerreiro2015multi} & Parallel Computing & SH & 2015\\
Collective Mind~\cite{fursin2015collective} & Compiler Parameters & ML & 2015\\
Abdelfattah, A. \textit{et al.}~\cite{abdelfattah2016performance} & Linear Algebra & SH (Exhaustive) & 2016\\
TANGRAM~\cite{chang2016efficient} & Domain-Specific Language & SH & 2016\\
MASE-BDI~\cite{coelho2016mase} & Environmental Land Change & SH & 2016\\
Xu, C. \textit{et al.}~\cite{xu2017parallel} & High-Level Synthesis & SH & 2017\\
Apollo~\cite{beckingsale2017apollo} & Parallel Computing & ML (Decision Trees) & 2017\\
DeepHyper~\cite{balaprakash2018deephyper} & Neural Networks & ML (Decision Trees) & 2018\\
Tuzov, I. \textit{et al.}~\cite{tuzov2018tuning} & High-Level Synthesis & Design of Experiments & 2018\\
Periscope~\cite{gerndt2018multi} & Compiler Parameters & SH & 2018\\
SynTunSys~\cite{ziegler2019syntunsys} & High-Level Synthesis & SH & 2019\\
Kwon, J. \textit{et al.}~\cite{kwon2019learning} & High-Level Synthesis & ML & 2019\\
FuncyTuner~\cite{wang2019funcytuner} & Compiler Parameters & SH & 2019\\
Ol’ha, J. \textit{et al.}~\cite{olha2019exploiting} & Parallel Computing & Sensitivity Analysis & 2019\\
Petrovic, F. \textit{et al.}~\cite{petrovivc2020benchmark} & Linear Algebra & SH & 2020\\
Chu, Y. \textit{et al.}~\cite{chu2020improving} & Search/MVWCP & SH & 2020\\
\bottomrule
\end{tabular}}
\end{table}
#+end_export

Autotuning methods that make explicit  hypotheses about the target program, such
as methods  based on Design of  Experiments, require some initial  knowledge, or
willingness to make assumptions, about  underlying relationships, and are harder
to adapt to constrained scenarios, but have the potential to produce /explainable/
optimizations.   In  general, methods  based  on  Machine Learning  have  enough
flexibility to  perform well in complex  search spaces and make  few assumptions
about problems, but usually provide little, if  any, that can be used to explain
optimization choices or derive relationships between parameters and performance.

Table\nbsp{}\ref{tab:search-methods} lists some autotuning systems, their target
domains, and  the employed  method, ordered by  publication date.   Some systems
that  did not  provide  detailed  search space  descriptions  and  could not  be
included in Figure\nbsp{}[[fig:search-spaces]], especially some of the earlier work,
provided enough information to categorize their autotuning methods. In contrast,
many  more  recent  works,  especially  those using  methods  based  on  Machine
Learning, did not provide specific  method information. Earlier work often deals
with  search spaces  small enough  to  exhaustively evaluate,  and using  search
heuristics to optimize linear algebra programs is the most prominent category of
earlier work in this sample.  Later  autotuning work target more varied domains,
with  the  most prominent  domains  in  this  sample being  parallel  computing,
compiler parameters, and  High-Level Synthesis.  Systems using  methods based on
Machine Learning become more common on later work than systems using heuristics.

Chapter\nbsp{}[[Application of Function Minimization Methods to Autotuning]] provides more
detailed definitions  and discussions  of the applicability,  effectiveness, and
explanatory  power  of  autotuning  methods   based  on  search  heuristics  and
statistical learning.  The  remainder of this chapter  details the contributions
of this thesis and the structure of this document.
*** Thesis Contributions
Computer  performance has  sustained exponential  increases over  the last  half
century,  despite   current  physical  limits  on   hardware  design.   Software
optimization  has  performed an  increasingly  substantial  role on  performance
improvement over the last 15 years, requiring the exploration of larger and more
complex search spaces than ever before.

Autotuning  methods  are one  approach  to  tackle performance  optimization  of
complex search  spaces, enabling exploitation of  existing relationships between
program  parameters and  performance.   We can  derive  autotuning methods  from
well-established  statistics,  although   their  usage  is  not   common  in  or
standardized for autotuning domains.

Initial work on this thesis studied  the effectiveness of classical and standard
search heuristics,  such as  Simulated Annealing,  on autotuning  problems.  The
first target autotuning domain was the set  of parameters of a compiler for CUDA
programs.  The search heuristics for this  case study were implemented using the
OpenTuner framework\nbsp\cite{ansel2014opentuner}, and  consisted of an ensemble
of  search  heuristics  coordinated  by a  Multi-Armed  Bandit  algorithm.   The
autotuner  searched  for a  set  of  compilation  parameters that  optimized  17
heterogeneous  GPU  kernels, from  a  set  of approximately  $10^{23}$  possible
combinations of all  parameters.  With 1.5h autotuning runs we  have achieved up
to  $4\times$  speedup  in  comparison   with  the  CUDA  compiler's  high-level
optimizations.   The  compilation  and  execution  times  of  programs  in  this
autotuning domain are relatively fast, and were in the order of a few seconds to
a minute.  Since measurement costs are relatively small, search heuristics could
find  good optimizations  using  as  many measurements  as  needed.  A  detailed
description      of      this      work       is      available      in      our
paper\nbsp\cite{bruel2017autotuning}   published   in    the   /Concurrency   and
Computation: Practice and Experience/ journal.

The  next  case  study  was  developed  in  collaboration  with  /Hewlett-Packard
Enterprise/,  and  consisted of  applying  the  same heuristics-based  autotuning
approach to the  configuration of parameters involved in the  generation of FPGA
hardware specification  from source  code in  the C  language, a  process called
/High-Level Synthesis/ (HLS).  The main difference from our work with GPU compiler
parameters was the time to obtain  the hardware specification, which could be in
the order of hours for a single kernel.

In this  more complex  scenario, we  achieved up  to $2\times$  improvements for
different hardware metrics using  conventional search algorithms.  These results
were obtained in a simple HLS benchmark, for which compilation times were in the
order of  minutes.  The  search space was  composed of  approximately $10^{123}$
possible  configurations, which  is much  larger than  the search  space in  our
previous work with GPUs. Search space size and the larger measurement cost meant
that  we  did  not  expect  the  heuristics-based  approach  to  have  the  same
effectiveness   as  in   the   GPU   compiler  case   study.    This  work   was
published\nbsp\cite{bruel2017autotuninghls}  at  the  2017  /IEEE  International
Conference on ReConFigurable Computing and FPGAs/.

Approaches using  classical machine  learning and optimization  techniques would
not scale  to industrial-level  HLS, where  each compilation  can take  hours to
complete.  Search space properties also  increase the complexity of the problem,
in  particular  its  structure  composed of  binary,  factorial  and  continuous
variables with potentially complex interactions.   Our results on autotuning HLS
for  FPGAs  corroborate   the  conclusion  that  the   empirical  autotuning  of
expensive-to-evaluate functions, such as those  that appear on the autotuning of
HLS, require a more parsimonious  and transparent approach, that can potentially
be achieved using  the DoE methodology.  The next section  describes our work on
applying the DoE methodology to autotuning.

The  main contribution  of this  thesis is  a strategy  to apply  the Design  of
Experiments methodology to autotuning problems.  The strategy is based on linear
regression  and  its  extensions,  Analysis of  Variance  (ANOVA),  and  Optimal
Design. The strategy  requires the formulation of initial  assumptions about the
target autotuning problem, which are  refined with data collected by efficiently
selected experiments. The main objectives  are to identify relationships between
parameters, suggesting regions for further experimentation in a transparent way,
that is, in a  way that is supported by statistical  tests of significance.  The
effectiveness of the proposed strategy, and its ability to explain optimizations
it  finds,  are  evaluated  on   autotuning  problems  in  the  source-to-source
transformation domain. The initial stages of this work resulted in a publication
on IEEE/ACM CCGrid\nbsp{}\cite{bruel2019autotuning}.

Further efforts in this direction were  dedicated to describe precisely what can
be learned about  search spaces from the application of  the methodology, and to
refine  the  differentiation  of  the approach  for  the  sometimes  conflicting
objectives of model  assessment and prediction. These  discussions are presented
in Chapter\nbsp{}[[A Design of Experiments Methodology]].

Because the  Design of Experiments  methodology requires the specification  of a
class of initial performance models, the methodology can sometimes achieve worse
prediction  capabilities  when  there  is considerable  uncertainty  on  initial
assumptions about the  underlying relationships.  Chapter\nbsp{}[[Gaussian Process
Regression: A  more Flexible Method]]  describes the application to  autotuning of
Gaussian Process Regression, an approach  that trades some explanatory power for
a much  larger and  more flexible  class of underlying  models. We  evaluate the
performance of this approach on the source-to-source transformation problem from
Chapter\nbsp{}[[A  Design of  Experiments  Methodology]], and  on larger  autotuning
problem on the quantization of /Deep Neural Network/ (DNN) layers.
*** Text Structure
The  remainder of  this thesis  is  organized as  follows.  Chapter\nbsp{}[[Efforts  for
Reproducible Science]] describes the efforts made over the duration of the work on
this thesis  to establish a  workflow that promotes reproducible  science, using
computational  documents  and  versioning   for  code,  results,  and  analyses.
Chapter\nbsp{}[[Methods for  Function Minimization]]  presents background for  the derived
from search heuristics, mathematical optimization, and statistical learning, and
Chapter\nbsp{}[[Application   of  Function   Minimization  Methods   to  Autotuning]]   to
autotuning.   Chapter\nbsp{}[[A  Design of  Experiments  Methodology]]  describes the  DoE
methodology,  and  its  application  to  autotuning  problems,  and  the  method
limitations   encountered   during   this  thesis.    Chapter\nbsp{}[[Gaussian   Process
Regression: A  more Flexible  Method]] describes  the Gaussian  Process Regression
method, and its application  to autotuning problems.  Chapter\nbsp{}[[Conclusion]] resumes
and  concludes the  high-level  discussions of  this chapter,  in  light of  the
results of the excursions performed in this thesis.
** Efforts for Reproducible Science
*** Introduction
*** Computational Documents with Emacs and Org mode
*** Versioning for Code, Text, and Data
* Background
** Methods for Function Minimization
*** Introduction
The    effectiveness   of    search    heuristics   on    autotuning   can    be
limited\nbsp{}\cite{seymour2008comparison,balaprakash2011can,balaprakash2012experimental},
between other factors, by underlying hypotheses  about the search space, such as
the  reachability of  the  global optimum  and the  smoothness  of search  space
surfaces, which  are frequently not  respected. The derivation  of relationships
between  parameters  and  performance  from search  heuristic  optimizations  is
greatly hindered,  if not rendered impossible,  by the biased way  these methods
explore  parameters.   Some  parametric  learning methods,  such  as  Design  of
Experiments,  are  not widely  applied  to  autotuning.  These  methods  perform
structured  parameter  exploration,  and  can  be used  to  build  and  validate
performance     models,     generating    transparent     and     cost-effective
optimizations\nbsp{}\cite{mametjanov2015autotuning,bruel2019autotuning}.         Other
methods  from  the parametric  family  are  more  widely  used, such  as  Bandit
Algorithms\nbsp{}\cite{xu2017parallel}.   Nonparametric  learning   methods,  such  as
Decision    Trees\nbsp{}\cite{balaprakash2016automomml}     and    Gaussian    Process
Regression\nbsp{}\cite{parsa2019pabo}, are able  to reduce model bias  greatly, at the
expense  of increased  prediction  variance.  Figure\nbsp{}\ref{fig:tree}  categorizes
some autotuning  methods according to some  of the key hypotheses  and branching
questions underlying each method.
**** A Classification Tree of Methods for Function Minimization
*** Search Heuristics
**** Implicit Underlying Hypotheses about Search Spaces
**** Optimization without Surrogate Models
**** Classification of Search Heuristics
*** Statistical Learning
**** Introduction
***** Optimization with Surrogate Models
***** The Bias-Variance Trade-Off
***** Inference and prediction
***** Supervised and Unsupervised Learning
***** Parametric and Nonparametric Statistics
**** Supervised Methods for Regression and Classification
- KNN, Linear Regression, Logistic Regression, Neural Networks
**** Model Assessment
- Train and Test Sets
- Train and Test Mean Squared Error
- Cross Validation, Bootstrapping
- Information Criteria for Model Assessment
  - Mallow's C_p and AIC
  - BIC
**** Choosing Experiments $X$: An Unposed Question
**** Linear Regression
- Ridge, Lasso
***** Ordinary Least Squares: Gauss-Markov (BLUE)
***** Transformations of $X$
**** Classification of Statistical Learning Methods
*** Design of Experiments
**** Introduction
***** Posing the Question of Choosing Experiments $X$
****** Reducing Experimental Cost
****** Improving Model Inference and Prediction
**** Screening
**** Factorial Designs
**** Space-Filling Designs
**** Optimal Design
***** Mixing Categorical and Numerical Factors
***** Optimality Criteria
***** Exchange Algorithms for Optimal Design Construction
**** Inference with Analysis of Variance (ANOVA)
**** Sampling under Search Space Constraints
**** Classification of Design of Experiments Methods
*** Gaussian Process Regression
**** Introduction
**** Sampling Functions from Multidimensional Gaussian Distributions
**** Nonparametric Modeling with Covariance Kernels
**** Sensitivity Analysis with Sobol Indices
**** Other Nonparametric Methods
**** Classification of Nonparametric Methods
*** Revisiting the Classification Tree
** Application of Function Minimization Methods to Autotuning
*** Search Heuristics
**** OpenTuner: Leveraging Ensembles of Heuristics
***** The Multi-Armed Bandit, Area Under the Curve Approach
*** Statistical Learning
**** Software for Autotuning with Statistical Learning
***** The /R/ Language
**** Results using Statistical Learning
***** Linear Regression and Design of Experiments
***** Methods based on Machine Learning
*** Design of Experiments
**** Inference for Autotuning under a Tight Budget
***** Fractional Factorial Designs for Chip Design on FPGAs
*** Gaussian Process Regression
* Experimental Validation
** Search Heuristics
*** Results and Limits of Analysis
**** GPU Compiler Parameters
**** High-Level Synthesis for FPGAs
**** Limitations of Analyzing Results from Search Heuristics
** A Design of Experiments Methodology
*** Inference for Autotuning under a Tight Budget
**** Screening for Main Effects of GPU Compiler Parameters
**** Optimal Designs for a GPU Laplacian Kernel
**** Space-Filling Designs for Exploring Quantization of DNN Layers
*** Prediction and Inference for Source-to-Source Transformation
**** Results with SPAPT Kernels
** Gaussian Process Regression: A more Flexible Method
*** Expressing Structure with Covariance Kernels?
*** Results with Quantization of Deep Neural Network Layers
** Conclusion
#+begin_export latex
\bibliographystyle{IEEEtran}
\bibliography{bibliography/references}
#+end_export
