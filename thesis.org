#+STARTUP: overview indent inlineimages logdrawer
#+TITLE: Towards Transparent and Parsimonious
#+TITLE: Methods for Automatic Performance Tuning
#+AUTHOR:      Pedro Bruel

#+LANGUAGE:    en
#+TAGS: noexport(n) ignore(i)
#+OPTIONS:   H:5 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Headers and Configuration                                 :noexport:ignore:
** Emacs Optional Configuration
#+begin_src emacs-lisp :exports none :eval no-export
;; Pour configurer les subdivisions de la classe book (indiquer : #+LaTeX_CLASS: book)
(with-eval-after-load "ox-latex"
(add-to-list 'org-latex-classes
             '("book"
               "\\documentclass{book}"
               ("\\part{%s}" . "\\part*{%s}")
               ("\\chapter{%s}" . "\\chapter*{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}"))))
#+end_src

#+RESULTS:
| book    | \documentclass{book}                 | (\part{%s} . \part*{%s})       | (\chapter{%s} . \chapter*{%s})       | (\section{%s} . \section*{%s})             | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) |
| beamer  | \documentclass[presentation]{beamer} | (\section{%s} . \section*{%s}) | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) |                                      |                                            |
| article | \documentclass{article}              |                                |                                      |                                            |                                      |                                            |
** Latex
#+LATEX_CLASS: book
#+LATEX_CLASS_OPTIONS: [11pt,twoside,openany,a4paper]

:latex_headers:
#+LATEX_HEADER: \usepackage[a4paper]{geometry}
#+LATEX_HEADER: \geometry{
#+LATEX_HEADER:   top=32mm,
#+LATEX_HEADER:   bottom=28mm,
#+LATEX_HEADER:   left=24mm,
#+LATEX_HEADER:   right=29mm,
#+LATEX_HEADER:   textwidth=157mm, % 210-24-34
#+LATEX_HEADER:   textheight=245mm, % 297-32-28
#+LATEX_HEADER:   vmarginratio=8:7, % 32:28
#+LATEX_HEADER:   hmarginratio=12:17, % 24:34
#+LATEX_HEADER:   % Com geometry, esta medida não é tão relevante; basta garantir que ela
#+LATEX_HEADER:   % seja menor que "top" e que o texto do cabeçalho caiba nela.
#+LATEX_HEADER:   headheight=25.4mm,
#+LATEX_HEADER:   % distância entre o início do texto principal e a base do cabeçalho;
#+LATEX_HEADER:   % ou seja, o cabeçalho "invade" a margem superior nessa medida. Essa
#+LATEX_HEADER:   % é a medida que determina a posição do cabeçalho
#+LATEX_HEADER:   headsep=11mm,
#+LATEX_HEADER:   footskip=10mm,
#+LATEX_HEADER:   marginpar=20mm,
#+LATEX_HEADER:   marginparsep=5mm,
#+LATEX_HEADER: }
#+LATEX_HEADER: \widowpenalty=10000
#+LATEX_HEADER: \clubpenalty=10000
#+LATEX_HEADER: \usepackage{tocbibind}
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX:HEADER: \usepackage[inline]{enumitem}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amssymb,amsthm}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{newpxtext}
#+LATEX_HEADER: \usepackage{newpxmath}
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \usepackage{titling}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-qtree}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstdefinelanguage{Julia}%
#+LATEX_HEADER:   {morekeywords={abstract,struct,break,case,catch,const,continue,do,else,elseif,%
#+LATEX_HEADER:       end,export,false,for,function,immutable,mutable,using,import,importall,if,in,%
#+LATEX_HEADER:       macro,module,quote,return,switch,true,try,catch,type,typealias,%
#+LATEX_HEADER:       while,<:,+,-,::,/},%
#+LATEX_HEADER:    sensitive=true,%
#+LATEX_HEADER:    alsoother={$},%
#+LATEX_HEADER:    morecomment=[l]\#,%
#+LATEX_HEADER:    morecomment=[n]{\#=}{=\#},%
#+LATEX_HEADER:    morestring=[s]{"}{"},%
#+LATEX_HEADER:    morestring=[m]{'}{'},%
#+LATEX_HEADER: }[keywords,comments,strings]%
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\tiny,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=b,
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   numbers=left,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=1,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}
#+LATEX_HEADER: \onehalfspacing

#+LATEX_HEADER: \setlength{\parskip}{0.5em}
#+LATEX_HEADER: \usepackage[pagestyles,raggedright]{titlesec}
# #+LATEX_HEADER: \titleformat{\chapter}[display]{\normalfont\bfseries}{}{0pt}{\huge}

#+LATEX_HEADER: \usepackage[fit]{truncate}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \fancyhf{}

#+LATEX_HEADER: \fancypagestyle{plain}{%
#+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}
#+LATEX_HEADER: \fancyfoot[RO,LE]{\thepage}
#+LATEX_HEADER: \fancyhead[RO]{\nouppercase{\truncate{\headwidth}{\rightmark}}}
#+LATEX_HEADER: \fancyhead[LE]{\nouppercase{\truncate{\headwidth}{\leftmark}}}
#+LATEX_HEADER: }

#+LATEX_HEADER: \pagestyle{plain}

#+LATEX_HEADER: \hypersetup{
#+LATEX_HEADER:     colorlinks=true,
#+LATEX_HEADER:     linkcolor={red!50!black},
#+LATEX_HEADER:     citecolor={blue!50!black},
#+LATEX_HEADER:     urlcolor={blue!80!black}
#+LATEX_HEADER: }
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \newcommand\fs@ruled@notop{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
#+LATEX_HEADER:   %\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}% <----removed
#+LATEX_HEADER:   \def\@fs@pre{}%
#+LATEX_HEADER:   %\def\@fs@post{\kern2pt\hrule\relax}%
#+LATEX_HEADER:   \def\@fs@post{}%
#+LATEX_HEADER:   %\def\@fs@mid{\kern2pt\hrule\kern2pt}%
#+LATEX_HEADER:   \def\@fs@mid{}%
#+LATEX_HEADER:   \let\@fs@iftopcapt\iftrue}
#+LATEX_HEADER: \renewcommand\fst@algorithm{\fs@ruled@notop}
#+LATEX_HEADER: \makeatother
:end:


* Early Drafts                                                     :noexport:
** First Draft
1. Introduction
   1. Autotuning
      1. Algorithm Selection Problem?
   2. Overview of Autotuning Methods (taxonomy/decision tree)
   3. Search Heuristics
      - Introduction
      - OpenTuner
      - Autotuning GPU compiler parameters
      - Autotuning High Level Synthesis for FPGAs
   4. Statistical Learning
      - Parametric, nonparametric
   5. Related Work
      - Literature Review
2. Design of Experiments
   1. Introduction
      1. Linear Regression
   2. Screening
      1. Main effects
      2. Example with CUDA flags
   3. Factorial Designs
      1. Example?
   4. Optimal Design
      1. Properties of the BLUE, Information Matrix
      2. Variance-optimizing criteria
      3. Example on Laplacian GPU
   5. Autotuning SPAPT Kernels
      - Mixing factor types
      - Sampling with Constraints
      - Heteroscedasticity
3. Gaussian Process Regression
   1. Introduction
      1. Bayesian Linear Model (Rasmussen's Book)
      2. EGO
   2. Revisiting SPAPT kernels
   3. Quantization for Deep Neural Networks
4. Conclusion
   - Expressing structure with kernels? (Duvenaud's thesis)
   - Performance of the Federov Algorithm for D-Optimal design construction?
** Structure Draft
- Course on performance optimization for HPC, and why it's hard
- Difficulty to optimize programs comes from complexity in:
  - Computer architecture
    - Pursuit of doubling performance, fitting more transistors,
      (Moore's Law), and the end of frequency and power
      scaling (Dennard's),
      mean that we need parallel architectures, which are more complex
  - Software
    - Parallel architectures are harder to program efficiently

** Underlying Hypotheses of Autotuning Methods
:PROPERTIES:
:EXPORT_FILE_NAME: hipotheses.pdf
:END:
*** Introduction                                                 :noexport:
Given  a program  with $X  \in \mathcal{X}$  configurable parameters,  we want  to
choose the best parameter values according  to a performance metric given by the
function  $f(X)$.   Autotuning methods  attempt  find  the $X_{*}$  that  minimizes
$f(\cdot)$.   Despite  their different  approaches,  autotuning  methods share  some
common hypotheses:

- There is no knowledge about the global optimal configuration
- There could be some problem-specific knowledge to exploit
- Measuring the effects of a choice of parameter values is possible but costly

Each  autotuning method  has  assumptions that  justify  its implementation  and
usage. Some of  these hypotheses are explicit,  such as the ones  that come from
the  linear model.   Others are  implicit,  such as  the ones  that support  the
implementation and the justification of optimization heuristics.
*** Overview of Autotuning Methods
:PROPERTIES:
:EXPORT_TITLE:
:EXPORT_FILE_NAME: tree.pdf
:END:
#+begin_export latex
\begin{sidewaysfigure}[t]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      for tree={%
        anchor = north,
        align = center,
        l sep+=1em
      },
      [{Minimize $f: \mathcal{X} \to \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
        draw,
        [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
          draw,
          color = NavyBlue
          [{Search Heuristics},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{\textbf{Random} \textbf{Sampling}}, draw]
            [{Reachable Optima},
              draw,
              color = BurntOrange
              [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
                draw,
                color = BurntOrange
                [{Strong $corr(f(X),d(X,X_{*}))$?},
                  draw,
                  color = NavyBlue
                  [{More Global},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                    [, phantom]]
                  [{More Local},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{High local optima density?},
                      draw,
                      color = NavyBlue
                      [{Exploit Steepest Descent},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                        [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                        [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                      [{Allows\\exploration},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                        [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                        [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
                [,phantom]]
              [,phantom]]]
          [{Statistical Learning},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{Parametric Learning},
              draw,
              color = BurntOrange
              [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                draw,
                color = BurntOrange
                [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                [, phantom]]
              [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                draw,
                color = BurntOrange
                [, phantom]
                [{Check for model adequacy?},
                  draw,
                  alias = adequacy,
                  color = NavyBlue
                  [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                    draw,
                    alias = interactions,
                    color = NavyBlue,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                      draw
                      [, phantom]
                      [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                        edge = {-stealth, ForestGreen, semithick},
                        edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                        draw,
                        alias = estimate,
                        color = ForestGreen]]
                    [{\textbf{Optimal} \textbf{Design}},
                      draw,
                      alias = optimal,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [{\textbf{Space-filling} \textbf{Designs}},
                    draw,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{Model selection},
                      edge = {-stealth, ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      alias = selection,
                      color = ForestGreen]]]]]
            [{Nonparametric Learning},
              draw,
              color = BurntOrange
              [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
              [{\textbf{Gaussian} \textbf{Process Regression}},
                alias = gaussian,
                draw]
              [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
      \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
      \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
      \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
    \end{forest}
  }
  \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
    denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
    denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
    denote algorithm choices, and \textbf{bold} boxes denote methods.}
\end{sidewaysfigure}
#+end_export

*** Previous Attempts                                            :noexport:
#+begin_export latex
\forestset{linebreaks/.style={for tree={align = center}}}
\begin{sidewaysfigure}
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      linebreaks
      [{Minimize $f: \mathcal{X} \to \mathbb{R}$,\\ $Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$}
        [{Does not construct\\estimate $Y = \hat{f}(\cdot, \theta{}(X))$}
          [{Reachable\\optima}
            [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$}
              [{Strong\\$corr(f(X),d(X,X_{*}))$}
                [{Low local\\optima density}
                  [{\textbf{Greedy}\\\textbf{Search}}, draw]
                  [{Estimate $f^{\prime}(X)$}
                    [{\textbf{Gradient}\\\textbf{Descent}}, draw]]]
                [{Introduce a ``population''\\$\mathbf{X} = (X_1,\dots,X_n)$}
                  [{Combination, mutation,\\within $\mathbf{X}$}
                    [{\textbf{Genetic}\\\textbf{Algorithms}}, draw]]
                  [{\textbf{Ant}\\\textbf{Colony}}, draw]]]
              [{Weaker\\$corr(f(X),d(X,X_{*}))$}
                [{Accept\\worst $f(X)$}
                  [{\textbf{Simulated}\\\textbf{Annealing}}, draw]]
                [{Avoid\\recent $X$}
                  [{\textbf{Tabu}\\\textbf{Search}}, draw]]]]]
          [{\textbf{Random}\\\textbf{Sampling}}, draw]]
        [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$}
          [{Parametric\\Learning}
            [{$\hat{f}(X) \approx f_1(X_1) + \dots + f_k(X_k)$}
              [{\textbf{Independent}\\\textbf{Bandit}}, draw]]
            [{$\hat{f}(X) = \mathcal{B}(logit(\mathcal{M}(X)\theta(X) + \varepsilon))$}
              [{\textbf{Logistic}\\\textbf{Regression}}, draw]]
            [{$\hat{f}(X) = \mathcal{M}(X)\theta(X) + \varepsilon$}
              [{\textbf{Linear}\\\textbf{Regression}}, draw]
              [{Measure\\properties of $X$}
                [{Independance\\of effects}
                  [{\textbf{Screening}}, draw]]
                [{Homoscedasticity of $\varepsilon$}
                  [{\textbf{Optimal}\\\textbf{Design}}, draw]]]]]
          [{Nonparametric\\Learning}
            [{Splitting\\rules on $X$}
              [{\textbf{Decision}\\\textbf{Trees}}, draw]]
            [{$\hat{f} = \mathcal{GP}(X; \mathcal{K})$}
              [{\textbf{Gaussian}\\\textbf{Process Regression}}, draw]]
            [{\textbf{Neural}\\\textbf{Networks}}, draw]
            [{\textbf{Multi-armed}\\\textbf{Bandit (?)}}, draw]]]]
    \end{forest}
  }
  \caption{Some hypothesis of some autotuning methods}
\end{sidewaysfigure}

#+end_export

#+begin_export latex
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\begin{table}[ht]
  \center
  \begin{tabular}{@{}p{0.3\textwidth}p{0.5\textwidth}@{}}
    \toprule
    Method &  Hypotheses \\ \midrule
    Metaheuristics & \tabitem There are similarities between natural fenomena and the target problem \\
    & \tabitem Gradual changes in configurations produce gradual changes in performance \\
    & \tabitem The optimal configuration is ``reachable'', by small changes, from non-optimal configurations  \\
    \addlinespace \\
    Machine Learning & \tabitem As more samples are obtained, decreases in ``out-of-sample error'' imply decreases ``in-sample error'' \\
    & \tabitem \textbf{TODO} What are the classes of models? \\
    \addlinespace \\
    Design of Experiments & \tabitem There is ``exploitable search space structure''\\
    & \tabitem Linear model: Response $\bm{Y}$ is an ``unobservable function'' of parameters $\bm{X}$: \\
    & \hspace{0.15\textwidth} $f(\bm{X}) = \bm{Y} = \bm{X\beta} + \bm{\varepsilon}$ \\
    & \tabitem Optimal Design: Variance of estimator $\hat{\bm{\beta}}$ is proportional to $\bm{X}$: \\
    & \hspace{0.15\textwidth} $\bm{\hat{\beta}} = \left(\bm{X}^{\intercal}\bm{X}\right)^{-1}\bm{X}^{\intercal}\bm{Y}$ \\
    \addlinespace \\
    Gaussian Process Regression & \tabitem Response $\bm{Y}$ is a sample from a multidimensional Gaussian distribution, with mean $m(\bf{X})$ and variance $k(\bm{X}, \bm{X}^{\intercal})$: \\
    & \hspace{0.1\textwidth} $\bm{Y} = f(\bm{X}) \sim \mathcal{N}(m(\bm{X}), k(\bm{X}, \bm{X}^{\intercal}))$ \\
    & \tabitem Predictions $\bm{Y_{*}}$ can be made conditioning distribution to observed data\\ \bottomrule
  \end{tabular}%
\end{table}
#+end_export

#+begin_export latex
\resizebox{!}{\textheight}{%
  \begin{tikzpicture}[rotate = -90]
    \begin{scope}
      \tikzset{every tree node/.style = {align = center}}
      \tikzset{level 1+/.style={level distance = 40pt}}
      \Tree [.\node(n0){Minimize $f: X \to \mathbb{R}$ \\ $f(X) = f^{*}(X) + \varepsilon = m$};
        [.{Does not construct \\ estimate $\hat{f}(X; \theta)$}
          [.{Reachability of \\ optima}
            [.{\textbf{Greedy} \\ \textbf{Search}} ]
            [.{$d(x_i, x_j) \to 0$ $\implies$ \\ $d(f(x_i), f(x_j)) \to 0$}
              [.{Abundance of \\ local optima}
                [.{\textbf{Simulated} \\ \textbf{Annealing}} ]]
              [.{Closeness of a \\ ``population'' of $X$}
                [.{\textbf{Genetic} \\ \textbf{Algorithms}} ]]]]
          [.{\textbf{Random} \\ \textbf{Sampling}} ] ]
        [.\node(r1){Constructs surrogate \\ estimate $\hat{f}(X; \theta)$};
          [.{Explicit, variable \\ models of $\theta$}
            [.{$\hat{f} = M(X)\theta + \varepsilon$}
              [.{Independance \\ of effects}
                [.{\textbf{Screening}} ] ]
              [.{Homoscedasticity}
                [.{\textbf{Optimal} \\ \textbf{Design}} ] ] ] ]
          [.{Implicit, fixed \\ models of $\theta$}
            [.{\textbf{Neural Networks}} ] ]
          [.{Samples \\ functions}
            [.{$\hat{f} = \mathcal{GP}(X; \theta, \mathcal{K})$}
              [.{\textbf{Gaussian Process} \\ \textbf{Regression}} ] ] ] ] ]
    \end{scope}
    % \begin{scope}[thick]
    %   \draw [color = orange] (n0) to [bend left = 2] (r1);
    %   \draw [color = green] (n0) to [bend right = 2] (r1);
    % \end{scope}
  \end{tikzpicture}
}
#+end_export

* Final Draft and Work List                                        :noexport:
** Introduction
General presentation of the manuscript
** Context
*** Observation: Historical Trends in Hardware Design
*** Consequences for compilers and application developers: Generating optimized code has become increasingly difficult
*** Autotuning approaches and difficulties
- Describe existing approaches (opentuner, etc.) and provide examples
  of autotuning problems with their dimension

All this leads to explain the current state of the technology and
what is possibly "wrong" with current approaches.
- Huge dimension and unclear geometry
- Many black/box search heuristics whose effectiveness is difficult to
  evaluate/interpret
** Optimization (in Autotuning context)
*** Optimization Methods
**** Methods Based on Derivatives (Local Descent)
***** Gradient based
- Classical hypothesis: convex
- convergence difficulties
- path of GD on 3 booth versions?
- mention very high cost of estimating derivatives
- make hypotheses more clear and explicit
  - explain why hypothesis are restrictive for autotuning problems
- Figure with Restarts
- [ ] Mention stochastic gradient descent -> used in NNs
***** Gradient + Hessian based
- Taylor expansion: Information about derivatives at a point --> (approximation)
  information about the function around a point
- hessian definition
- fast convergence
  - Example: converging in 1 step for the booth function
***** This requires too strong hypothesis, hence the needs for more "general" methods
- high cost of estimating derivatives
**** Stochastic Methods (Derivative-Free)
***** Single-State
- compute neighborhood of x --> perturb x
  - random walk
  - greedy random walk
  - best random walk
- probability distribution for acceptance
  - bio-inspired simulated annealing
***** Population-Based methods
- GAs
  - Colors for generations, uniform points
- PSO
- +Ant colony?+
**** Mini-conclusion
- Most of these methods are not parsimonious. Require many estimates
  of f, of \nabla f and even sometimes of \Delta f! Stochastic methods are even
  worse as they need to explore whereas descent based methods head to
  the optimum more directly.
*** Learning: Building a Surrogate
**** Statistical Learning: Linear Regression
- the model
- how to fit
- "Model quality" +how to check whether the model is correct or not ?+
- how to interpret the significance (LM-CI, ANOVA). May hint to good
  values for optimization

Limitation: simple model with shape constraits, cannot "fit"
everything

***** Fitting the Linear Model to Data
- [X] paragraph: Computing $\hat{\beta}$: OLS
- [X] 1/2  paragraph: Projection  matrix of  $\mathbf{y}$, into  column space  of
  $\mathbf{X}$
  - Use linear map of columns of $\mathbf{X}$ to describe $\mathbf{y}$
- [X]  1/2 paragraph: If $\varepsilon$  is normally distributed, equal  to Maximum
  Likelihood Estimator
- [X] paragraph: Natural Extension: Transformations of $\mathbf{X}
  - Basis function sets
  - Linear terms, quadratic terms, interactions
- +[ ] Example on "slice" of multidimensional function? (from pres. at Argonne)+
***** Quality of Fit Metrics
- [X] 2 paragraphs: Example on Booth's function (3d?)
- [X] paragraph: MSE
- [X] paragraph: Train and Test Sets?
- +[ ] paragraph: Cross Validation?+
- [X] paragraph: Bias-Variance Trade-Off
  - number of parameters -> complexity
  - as we change the training set, for the same point x_0:
    - bias: distance between \hat{f}(x_0) and f(x_0)
    - variance: distance between \hat{f}_1(x_0) and \hat{f}_2(x_0)
  - complexity reduces  bias, but increases variance
- [ ] paragraph: Model space figure from ESLII?
- +[ ] paragraph: Information Criteria for Model Assessment+
  - Mallow's C_p and AIC
  - BIC?
***** Interpreting Significance
- [X] OLS standard deviation and Confidence Intervals
  - Coefficients as main effects, interactions, ...
  - p-values: may mislead
- [-] ANOVA
  - [X] 1, 2? paragraphs: Rationale
    - Group observations:
      - Group by factor levels: One-way
        - Group by factor levels and factors: Two-way
    - Compute separate group means:
      - Partition of the sums of squares
      - Compute partitioned MSEs
    - Test the differences between group means:
      - Using the F-Test
  - [X] 1 paragraph: Formal Hypotheses
    - Uncorrelated observations
    - Normally distributed residuals
    - Homoscedasticity (identical group variances)
  - [ ] 1 paragraph: Computing ANOVA
    - As a special case of the linear model
      - Group observations by factor levels and factor
      - Form model matrix with indicator variables for group membership
        - Add interactions
      - Assume sums of in-group coefficients are zero
      - Generate "power set" of all groups of model terms
      - Compare resulting models with F-tests

**** Gaussian Process
- Sampling Functions from Multidimensional Gaussian Distributions
- Nonparametric Modeling with Covariance Kernels
- Sensitivity Analysis with Sobol Indices
- Other Nonparametric Methods

- the model
- how to fit
- provides mean estimates with confidence estimation
- how to interpret the significance (sobol indices) but quite costly

***** Quality of Fit
- [X] Introduction
  - 3 trend model types
  - 10 samples -> fit a GP
  - Compute MSE (assess quality)
- [X] Model Trends
  - Guide surrogate mean outside measurements
- [X] MSE
  - mean of observations
  - on training set: Leave One Out Cross Validation
  - on testing set
  - on the real function
- [X] Kernel Hyperparameters
  - lengthscale
  - optimized during fit: MSE, MAP
- [X] Other assessment metrics
  - MAP?
- [ ] Three levels of inference (Rasmussen Chap. 5)
  - Hypothesis, hyperparameters, parameters

***** Inference: Sensitivity Analysis
- [ ] Sobol indices (brief)
  - variance-based sensitivity analysis
  - variance decomposition for a general objective function of $p$ factors
    - for each factor $x_i$, what is the variance of the expected value
      of the objective function, given a specific value of the factor?
    - estimators

**** Mini-Conclusion
Two big classes of models. Generality vs. interpretability. Yet
everything we mentioned assumes X is given, sampled from
observations. In our contects, We can choose which X to test, either
to test the model, or to improve its quality, or to find a "good"
value in our space $\mathcal{X}$.
*** Design of Experiments
**** Estimating Linear Effects
- A Note on Terminology
  - [ ] Factors, levels, designs, ...

- [ ] Figure with 6 panels, with fits for:
  - close / apart experiments
  - apart experiments: low noise / high noise
  - apart experiments: heteroscedastic / homoscedastic

- [ ] Linear effects (one-dimensional example):
  - Simplest hypothesis, other than the mean
  - Experiments "too close" ~> high prediction variance
  - Picking the extremes [-1, 1] decreases estimator variance

- [ ] Effects of noise
  - [ ] Too much noise can invert relationships
  - [ ] Heteroscedasticity

***** 2-Level Factorial Designs
- [ ] Going multivariate
  - 2-level Full-Factorial designs have  complete information on linear effects,
    plus interactions
  - 2-level  Fractional Factorial:  add more  factors without  increasing design
    size too much, but confound with interactions

- [ ] Going multilevel
  - Picking random levels from multi-level factors
  - Just mention this, no need to expand on it

**** Screening
Super efficient but very limited
- adapt example from ccgrid
**** Optimal designs
A  "flexible"  screening:  allows  to include  non-linear  terms,  interactions,
etc. if needed.

Awesome but if a parameter was not included  in the model or if the model is too
simple (e.g. only comprised a linear term  where a quadratic one would have been
needed,  or an  important interaction  was not  included), we  won't be  able to
detect it (lack of fit).
**** Space-filling Designs
Not very efficient for parameter estimate but good to evaluate the
lack of fit.

Also good for variance minimization in GP.
**** Mini-conclusion
- Designs to obtain good-quality parameter estimates
  - Screening and D-opt for LM
  - SFD for GP
- Designs to test the model quality (lack of fit)
  - SFD fo LM

If model  based, parameter  significance and  estimation can  be used  to reduce
dimension and  guide the optimization. With  this DoE approach, we  have a clear
separation between the sampling phase and  the interpretation phase. But what if
no parameter really appears significant anymore ?
*** Online Learning: the exploration / exploitation trade-off
**** Bandits: simple (discrete choice, optimize regret = \sum_t R_t), UCB
**** EI for GP (continuous choice, optimize EI = \max_t R_t)
Also mention GP-UCB and contrast with EGO. There are also variants for
Linear Model (LinUCB).
**** Mini-conclusion
These methods seamlessly mix exploration and exploitation but the
overal objective function is generally the regret, which makes sense
for a self-optimizing system (e.g. facebook) but not in an autotuning
context (where EI is more meaningful).
*** Summary and Proposal
- use glassbox (DoE based) approach to perform the optimization,
  always try to interpret the results
- 2 big methods based on different exploration/exploitation strategy:
  1. Evaluate parameter significance and reduce dimension (two phases,
     iterative)
  2. Expected Improvement (first a general exploration phase with a
     SFD, then seamlessly mix exploration and exploitation)
  Possibly combinations of both approaches could be used back and
  forth depending on the specific information we learn on the use
  case.
- In this thesis, we evaluate these DoE-based approaches for several
  autotuning use cases and try to compare them with approaches that
  had beed previously proposed fot these use cases.
** Evaluation
*** Reproducible Research Methodology
- Tools and such
- Explain difficulty of finding a needle in a haystack:
  - how to know whether we found the optimal value ?
  - how to know how far we are from the optimal value ?
  - how to know whether there is anything to find ?
  - how to know whether the geometry hypothesis we make are sound in
    an unknown space ?
  - ...
*** TODO Use Case 1: GPU compiling flags
:LOGBOOK:
- State "TODO"       from              [2021-03-25 Thu 20:27]
:END:
- mix binaire/numérique, opentuner (multi-armed to select the right
  stochastic descent algorithm)
- Tried to use clustering to identify significant parameters
- On the interest of using a Screening design.
  - Once the significant parameters are, they can easily be
    fixed. Are there the same as the ones found by opentuner.
- [ ] Separate new GPUs from older ones, into new figure

*** TODO Use Case 2: Kernel GPU Steven
:LOGBOOK:
- State "TODO"       from              [2021-03-25 Thu 20:28]
:END:
- [X] What is the Laplacian kernel?
- [X] How was the model chosen?

- [X] Base strategies (RS, GAs, Local Descent, ...)
  - Good results too, especially compared to
    RS, but with high maximum variability
  - LM, RQ: Find the minimum, but get it
    wrong some of the time

- [X] D-opt based approach \to excellent results
  - Able to consistently find the optimum
  - Motivated experiments on the next chapter

- [X] Describe actual steps of the algorithm
  - Give a more concrete example

- [X] GP-EI based approach
  - Improves upon LM, RQ, but has more
    variability than DLMT

- [ ] Add design points to Figure 12.6

*** Use Case 3: SPAPT
- Base strategies (RS because equivalent to other classical strategies
  s.a. GAs and others)

- D-opt based approach \to not really impressive compared to RS but
  maybe there is nothing to find.
  - Ability to interpret = unclear. Nothing to see or model too simple ?

**** Identifying significant factors
- [X] On ensemble figure: Invert RQ - Extra Steps
- [X] Describe the 4 experiment variations
- [X] Remove repeated figures
  - We can make all points with only the first set of figures

- [X] We can ID factors' significance
  - OMP had the largest impact

- [X] We can show that fixing factors effectively decreases performance
  - D-Optimal designs on restricted subspaces find the best points often
  - The model fits, not so much

- We can see which factors were responsible

- After fixing OMP, we can see impacts of other factors more clearly

- Quantile regression fits:
  - Didn't seem to change predictions by a lot

- For the DGEMV kernel:
  - Peak performance is still 20 times faster than the best point we found
  - [-] Describe Roofline computations, CPU specs?
    - [X] Generate real Roofline plots
      - Hypothesis to explain slow kernel: cache usage and vectorization
    - [ ] Add reference to ERT code/paper
    - [ ] Add reference to ERT experiments code
    - [X] Check kernel data type (double)
    - Compare with MKL
  - [ ] Add best/achieved figure (like 13.3) for =-march=native=

*** Use Case 4: FPGA
- 100 of numerical parameters
- several metrics to optimize \to weighted combination
- OpenTuner with heuristic and bandits gave "good" results
  (improvement over the default one)
- +DoE+ because no more access to the code. Ideally GP-EI. But we could
  see that the OpenTuner exploration makes it very hard to interprete
  the geometry.
*** Use Case 5: Bit Quantization in Neural Nets
- 54 discrete numerical parameters
- several objective functions
- RL
- GP-EI

** Conclusion and Perspectives
* Cloning Repositories with Data                                   :noexport:
** Adding Sub-modules
*** Bibliography
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
git submodule add --depth=1 git@github.com:phrb/bibliography.git
#+end_SRC

*** DLMT and ANOVA Experiments
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
git submodule add --depth=1 git@github.com:phrb/dopt_anova_experiments.git
#+end_SRC

#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
git submodule add --depth=1 git@github.com:phrb/dlmt_spapt_experiments.git
#+end_SRC

#+RESULTS:

*** CCGRID2019 Paper
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
git submodule add --depth=1 git@github.com:phrb/ccgrid19.git
#+end_SRC

*** PhD Journal
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
git submodule add --depth=1 git@github.com:phrb/journal.git
#+end_SRC
*** HAQ Experiments
- TODO (cleanup repo)
- https://github.com/phrb/haq-autotuning-experiments
*** HPE 2019 Journal
- TODO (cleanup repo, make public)
- https://github.com/phrb/hpe-2019-journal

** Initializing Sub-modules
#+begin_SRC shell :results output :session *Shell* :eval no-export :exports results
git submodule init
#+end_SRC

* Generating Figures                                               :noexport:
** Historical Trends (Part I)
*** Introduction
**** 49 Years of Processor Data
***** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)
df_freq <- read.csv("data/wiki_data/frequency.csv", header = TRUE)
df_transistor <- read.csv("data/wiki_data/transistor_count.csv", header = TRUE)
#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_freq)
#+end_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  12 variables:
 $ date               : int  1971 1972 1972 1972 1972 1973 1973 1973 1974 1974 ...
 $ name               : chr  "4004" "PPS-25" "μPD700" "8008" ...
 $ designer           : chr  "Intel" "Fairchild" "NEC" "Intel" ...
 $ max_clock_khz      : int  740 400 NA 500 200 NA NA NA 715 NA ...
 $ max_clock_mhz      : num  NA NA NA NA NA 2 1 1 NA 2 ...
 $ max_clock_ghz      : num  NA NA NA NA NA NA NA NA NA NA ...
 $ process_micro_m    : num  10 NA NA 10 NA 7.5 6 NA NA 6 ...
 $ process_nm         : int  NA NA NA NA NA NA NA NA NA NA ...
 $ chips              : int  1 2 1 1 1 1 1 1 3 1 ...
 $ transistor_count   : int  2250 NA NA 3500 NA 2500 2800 NA NA 6000 ...
 $ transistor_millions: num  NA NA NA NA NA NA NA NA NA NA ...
 $ logical_cores      : int  1 1 1 1 1 1 1 1 1 1 ...
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_transistor)
#+end_SRC

#+RESULTS:
: 'data.frame':	151 obs. of  6 variables:
:  $ name            : chr  "Intel 4004 " "Intel 8008 " "Toshiba TLCS-12 " "Intel 4040 " ...
:  $ transistor_count: num  2250 3500 11000 3000 4100 ...
:  $ date            : int  1971 1972 1973 1974 1974 1974 1974 1975 1976 1976 ...
:  $ designer        : chr  "Intel" "Intel" "Toshiba" "Intel" ...
:  $ process_nm      : int  10000 10000 6000 10000 6000 6000 8000 8000 5000 4000 ...
:  $ area_mm         : num  12 14 32 12 16 20 11 21 27 18 ...

***** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/49_years_processor_data.pdf" :width 10 :height 5 :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")

point_alpha = 0.9
line_alpha = 0.4
point_size = 2

shapes = c(0, 1, 2, 5)

ggplot() +
    # geom_line(data = df_transistor,
    #           size = point_size,
    #           stat = "smooth",
    #           method = "lm",
    #           alpha = line_alpha,
    #           formula = y ~ x + I(x ^ 2),
    #           aes(x = date,
    #               y = process_nm,
    #               color = "Process (nanometers)"),
    #           show.legend = FALSE) +
    # geom_line(data = df_freq,
    #           size = point_size,
    #           stat = "smooth",
    #           method = "lm",
    #           alpha = line_alpha,
    #           formula = y ~ x + I(x ^ 2) + I(x ^ 3),
    #           aes(x = date,
    #               y = logical_cores,
    #               shape = "Logical Cores (Count)",
    #               color = "Logical Cores (Count)"),
    #           show.legend = FALSE) +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_micro_m * 1e3,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = logical_cores,
                   shape = "Logical Cores (Count)",
                   color = "Logical Cores (Count)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_khz * 1e-3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_mhz,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_ghz * 1e3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_millions * 1e3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    xlab("Year") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 18) +
    theme(axis.title.y = element_blank(),
          legend.position = c(0.14, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 14),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/49_years_processor_data.pdf]]
**** TOP500
***** Loading Data and Packages
Load the /csv/:

#+begin_SRC R :results output :session *R* :exports code :eval no-export
library(dplyr)
library(tidyr)
library(ggplot2)

df <- read.csv("./data/top500/TOP500_history.csv")
#+end_SRC

#+RESULTS:
***** Looking at Data
****** Column Names
We  have many  columns  filled with  `NA`s,  due to  how  metrics were  measured
differently over the years. There's data from 1993 to 2019!

#+begin_SRC R :results output :session *R* :exports both :eval no-export
names(df)
#+end_SRC

#+RESULTS:
#+begin_example
 [1] "Year"                            "Month"
 [3] "Day"                             "Rank"
 [5] "Site"                            "Manufacturer"
 [7] "Computer"                        "Country"
 [9] "Processors"                      "RMax"
[11] "RPeak"                           "Nmax"
[13] "Nhalf"                           "Processor.Family"
[15] "Processor"                       "Processor.Speed..MHz."
[17] "System.Family"                   "Operating.System"
[19] "Architecture"                    "Segment"
[21] "Application.Area"                "Interconnect.Family"
[23] "Interconnect"                    "Region"
[25] "Continent"                       "Power"
[27] "System.Model"                    "Total.Cores"
[29] "Measured.Size"                   "Processor.Cores"
[31] "Accelerator"                     "Name"
[33] "Accelerator.Cores"               "Efficiency...."
[35] "Mflops.Watt"                     "Processor.Technology"
[37] "OS.Family"                       "Cores.per.Socket"
[39] "Processor.Generation"            "Previous.Rank"
[41] "First.Appearance"                "First.Rank"
[43] "Accelerator.Co.Processor.Cores"  "Accelerator.Co.Processor"
[45] "Power.Source"                    "Rmax..TFlop.s."
[47] "Rpeak..TFlop.s."                 "HPCG..TFlop.s."
[49] "Power..kW."                      "Power.Effeciency..GFlops.Watts."
[51] "Site.ID"                         "System.ID"
#+end_example

****** Achieved and Theoretical Performance
#+begin_SRC R :results graphics output :session *R* :file "./img/top500_rmax_rpeak.pdf" :width 10 :height 5 :exports both :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")

point_size = 2.8
shapes = c(0, 1, 2, 5)

plot_df <- df %>%
    filter(Rank <= 1) %>%
    mutate(RMaxT = coalesce(RMax / 1e3, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak / 1e3, Rpeak..TFlop.s.),
           Power = coalesce(Power, Power..kW.)) %>%
    select(Rank,
           Year,
           Power,
           RMaxT,
           RPeakT) %>%
    distinct(Rank, Year, .keep_all = TRUE) %>%
    mutate(Ratio = RMaxT / RPeakT) %>%
    filter(is.finite(Ratio) & Ratio <= 1.0)

ggplot() +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RMaxT,
                   shape = "RMax",
                   color = "RMax")) +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RPeakT,
                   shape = "RPeak",
                   color = "RPeak")) +
    # geom_point(data = plot_df,
    #            size = point_size,
    #            aes(x = Year,
    #                y = Power,
    #                shape = "Power (kW)",
    #                color = "Power (kW)")) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    ylab("Tflops/s") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 20) +
    theme(legend.position = c(0.06, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 16),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/top500_rmax_rpeak.pdf]]

****** Accelerator Core Count
#+begin_SRC R :results graphics output :session *R* :file "./img/top500_accelerator_cores.pdf" :width 10 :height 5 :exports both :eval no-export
library(ggplot2)
library(extrafont)
library(scales)
library(tidyr)

loadfonts(device = "postscript")

point_size = 2.8
shapes = c(0, 1, 2, 5)

plot_df <- df %>%
    filter(Rank <= 1) %>%
    mutate(Accelerators = na_if(Accelerator.Co.Processor.Cores, 0),
           Cores = coalesce(Processors, Total.Cores) -
               replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    select(Rank,
           Year,
           Accelerators,
           Cores) %>%
    distinct(Rank, Year, .keep_all = TRUE)

ggplot() +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = Cores,
                   shape = "Processor",
                   color = "Processor")) +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = Accelerators,
                   shape = "Accelerator",
                   color = "Accelerator")) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    ylab("Cores") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 20) +
    theme(legend.position = c(0.09, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 16),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/top500_accelerator_cores.pdf]]
****** Other Plots
******* Processor Clock
Supercomputer  clock  explosion  and  range  broadening.  Even  top-tier  clocks
stagnate after 2008.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_processors_clock.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Processor.Speed..MHz. / 1000,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
                                        #scale_y_log10() +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Processor Clock (GHz)") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_processors_clock.pdf]]

******* Processors
Core count sustained  exponential increase.  Although top-tier  core count still
increases, range  broadening around  2012 can be  explained by  introduction and
ubiquity of accelerator cores on all tiers.

#+begin_SRC R :results graphics output :session *R* :file "./img/top500_total_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)
library(tidyr)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores) - replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    select(Rank, Year, AllCores, Accelerator.Co.Processor.Cores) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("AllCores",
                                    "Accelerator.Co.Processor.Cores"),
                         labels = c("Processor Cores",
                                    "Accelerator Cores"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Core Count") +
    scale_y_log10() +
    # annotation_logticks(sides = "l") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.67, 0.08),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 4)
#+end_SRC

#+RESULTS:
[[file:./img/top500_total_cores.pdf]]
******* RPeak and RMax
Sustained increase of theoretical peak and  achieved max performance on HPL and,
most recently,  on the  HPCG benchmark.  RPeak does not  guarantee rank  on some
cases.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rpeak.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(RMax = RMax / 1e3,
           RPeak = RPeak / 1e3,
           RMaxT = coalesce(RMax, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak, Rpeak..TFlop.s.)) %>%
    select(Rank,
           Year,
           RMaxT,
           RPeakT,
           HPCG..TFlop.s.) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakT",
                                    "RMaxT",
                                    "HPCG..TFlop.s."),
                         labels = c("RPeak (HPL)",
                                    "RMax (HPL)",
                                    "RMax (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance (TFlops/s)") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.83, 0.09),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 3)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rpeak.pdf]]
******* RMax / Cores
Ratio of performance and core count, for HPL and HPCG. Is this sustained increase due only to accelerator cores, or are there other engineering and software advances?
#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rmax_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores)) %>%
    mutate(RMax = (RMax / 1e3) / AllCores,
           RPeak = (RPeak / 1e3) / AllCores,
           Rmax..TFlop.s. = Rmax..TFlop.s. / AllCores,
           Rpeak..TFlop.s. = Rpeak..TFlop.s. / AllCores,
           RMaxC = coalesce(RMax, Rmax..TFlop.s.),
           RPeakC = coalesce(RPeak, Rpeak..TFlop.s.),
           HPCGC = HPCG..TFlop.s. / AllCores) %>%
    select(Rank,
           Year,
           RMaxC,
           RPeakC,
           HPCGC) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakC",
                                    "RMaxC",
                                    "HPCGC"),
                         labels = c("RPeak / Cores (HPL)",
                                    "RMax / Cores (HPL)",
                                    "RMax / Cores (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance / Core Count") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.85, 0.1),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          strip.text.x = element_text(size = 28),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 5)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rmax_cores.pdf]]

******* NMax
Exponential increase of problem size to reach max performance. Why is there
range broadening after 2011?

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_nmax.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Nmax,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Problem Size to Reach RMax") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_nmax.pdf]]

**** Search Spaces
***** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(extrafont)

df_search_spaces <- read.csv("data/search_spaces/search_spaces.csv")

loadfonts(device = "postscript")
#+end_SRC

#+RESULTS:
#+begin_example

Akaash already registered with postscriptFonts().
AkrutiMal1 already registered with postscriptFonts().
AkrutiMal2 already registered with postscriptFonts().
AkrutiTml1 already registered with postscriptFonts().
AkrutiTml2 already registered with postscriptFonts().
Anonymice Powerline already registered with postscriptFonts().
Arimo for Powerline already registered with postscriptFonts().
Bitstream Vera Sans already registered with postscriptFonts().
Bitstream Vera Sans Mono already registered with postscriptFonts().
Bitstream Vera Serif already registered with postscriptFonts().
Cousine for Powerline already registered with postscriptFonts().
IBM 3270 already registered with postscriptFonts().
IBM 3270 Narrow already registered with postscriptFonts().
IBM 3270 Semi-Narrow already registered with postscriptFonts().
DejaVu Math TeX Gyre already registered with postscriptFonts().
DejaVu Sans already registered with postscriptFonts().
DejaVu Sans Light already registered with postscriptFonts().
DejaVu Sans Condensed already registered with postscriptFonts().
DejaVu Sans Mono already registered with postscriptFonts().
DejaVu Sans Mono for Powerline already registered with postscriptFonts().
DejaVu Serif already registered with postscriptFonts().
DejaVu Serif Condensed already registered with postscriptFonts().
Droid Arabic Kufi already registered with postscriptFonts().
Droid Arabic Naskh already registered with postscriptFonts().
Droid Naskh Shift Alt already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans. Skipping setup for this font.
Droid Sans Arabic already registered with postscriptFonts().
Droid Sans Armenian already registered with postscriptFonts().
Droid Sans Devanagari already registered with postscriptFonts().
Droid Sans Ethiopic already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Fallback. Skipping setup for this font.
Droid Sans Georgian already registered with postscriptFonts().
Droid Sans Hebrew already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Mono. Skipping setup for this font.
Droid Sans Mono Dotted for Powerline already registered with postscriptFonts().
Droid Sans Mono Slashed for Powerline already registered with postscriptFonts().
Droid Sans Tamil already registered with postscriptFonts().
Droid Sans Thai already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Serif. Skipping setup for this font.
Font Awesome 5 Brands Regular already registered with postscriptFonts().
Font Awesome 5 Free Regular already registered with postscriptFonts().
Font Awesome 5 Free Solid already registered with postscriptFonts().
Gargi-1.2b already registered with postscriptFonts().
Goha-Tibeb Zemen already registered with postscriptFonts().
Go Mono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for GurbaniBoliLite. Skipping setup for this font.
Hack already registered with postscriptFonts().
Inconsolata Black already registered with postscriptFonts().
Inconsolata already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Inconsolata for Powerline. Skipping setup for this font.
Inconsolata Condensed already registered with postscriptFonts().
Inconsolata Condensed Black already registered with postscriptFonts().
Inconsolata Condensed Bold already registered with postscriptFonts().
Inconsolata Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Condensed Light already registered with postscriptFonts().
Inconsolata Condensed Medium already registered with postscriptFonts().
Inconsolata Condensed SemiBold already registered with postscriptFonts().
Inconsolata Expanded already registered with postscriptFonts().
Inconsolata Expanded Black already registered with postscriptFonts().
Inconsolata Expanded Bold already registered with postscriptFonts().
Inconsolata Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Expanded Light already registered with postscriptFonts().
Inconsolata Expanded Medium already registered with postscriptFonts().
Inconsolata Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed already registered with postscriptFonts().
Inconsolata Extra Condensed Black already registered with postscriptFonts().
Inconsolata Extra Condensed Bold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Extra Condensed Light already registered with postscriptFonts().
Inconsolata Extra Condensed Medium already registered with postscriptFonts().
Inconsolata Extra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Extra Expanded already registered with postscriptFonts().
Inconsolata Extra Expanded Black already registered with postscriptFonts().
Inconsolata Extra Expanded Bold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Extra Expanded Light already registered with postscriptFonts().
Inconsolata Extra Expanded Medium already registered with postscriptFonts().
Inconsolata Extra Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraLight already registered with postscriptFonts().
Inconsolata Light already registered with postscriptFonts().
Inconsolata Medium already registered with postscriptFonts().
Inconsolata SemiBold already registered with postscriptFonts().
Inconsolata Semi Condensed already registered with postscriptFonts().
Inconsolata Semi Condensed Black already registered with postscriptFonts().
Inconsolata Semi Condensed Bold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Semi Condensed Light already registered with postscriptFonts().
Inconsolata Semi Condensed Medium already registered with postscriptFonts().
Inconsolata Semi Condensed SemiBold already registered with postscriptFonts().
Inconsolata Semi Expanded already registered with postscriptFonts().
Inconsolata Semi Expanded Black already registered with postscriptFonts().
Inconsolata Semi Expanded Bold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Semi Expanded Light already registered with postscriptFonts().
Inconsolata Semi Expanded Medium already registered with postscriptFonts().
Inconsolata Semi Expanded SemiBold already registered with postscriptFonts().
Inconsolata Ultra Condensed already registered with postscriptFonts().
Inconsolata Ultra Condensed Black already registered with postscriptFonts().
Inconsolata Ultra Condensed Bold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Condensed Light already registered with postscriptFonts().
Inconsolata Ultra Condensed Medium already registered with postscriptFonts().
Inconsolata Ultra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Ultra Expanded already registered with postscriptFonts().
Inconsolata Ultra Expanded Black already registered with postscriptFonts().
Inconsolata Ultra Expanded Bold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Expanded Light already registered with postscriptFonts().
Inconsolata Ultra Expanded Medium already registered with postscriptFonts().
Inconsolata Ultra Expanded SemiBold already registered with postscriptFonts().
Liberation Mono already registered with postscriptFonts().
Liberation Sans already registered with postscriptFonts().
Liberation Serif already registered with postscriptFonts().
Ligconsolata already registered with postscriptFonts().
Likhan already registered with postscriptFonts().
Literation Mono Powerline already registered with postscriptFonts().
malayalam already registered with postscriptFonts().
MalOtf already registered with postscriptFonts().
Meslo LG L DZ for Powerline already registered with postscriptFonts().
Meslo LG L for Powerline already registered with postscriptFonts().
Meslo LG M DZ for Powerline already registered with postscriptFonts().
Meslo LG M for Powerline already registered with postscriptFonts().
Meslo LG S DZ for Powerline already registered with postscriptFonts().
Meslo LG S for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for monofur for Powerline. Skipping setup for this font.
More than one version of regular/bold/italic found for Mukti Narrow. Skipping setup for this font.
Noto Kufi Arabic already registered with postscriptFonts().
Noto Kufi Arabic Medium already registered with postscriptFonts().
Noto Kufi Arabic Semi bold already registered with postscriptFonts().
Noto Mono for Powerline already registered with postscriptFonts().
Noto Music already registered with postscriptFonts().
Noto Naskh Arabic already registered with postscriptFonts().
Noto Naskh Arabic UI already registered with postscriptFonts().
Noto Nastaliq Urdu already registered with postscriptFonts().
Noto Sans Black already registered with postscriptFonts().
Noto Sans already registered with postscriptFonts().
Noto Sans Light already registered with postscriptFonts().
Noto Sans Medium already registered with postscriptFonts().
Noto Sans Thin already registered with postscriptFonts().
Noto Sans Adlam already registered with postscriptFonts().
Noto Sans Adlam Unjoined already registered with postscriptFonts().
Noto Sans AnatoHiero already registered with postscriptFonts().
Noto Sans Arabic Blk already registered with postscriptFonts().
Noto Sans Arabic already registered with postscriptFonts().
Noto Sans Arabic Light already registered with postscriptFonts().
Noto Sans Arabic Med already registered with postscriptFonts().
Noto Sans Arabic Thin already registered with postscriptFonts().
Noto Sans Arabic UI Bk already registered with postscriptFonts().
Noto Sans Arabic UI already registered with postscriptFonts().
Noto Sans Arabic UI Lt already registered with postscriptFonts().
Noto Sans Arabic UI Md already registered with postscriptFonts().
Noto Sans Arabic UI Th already registered with postscriptFonts().
Noto Sans Armenian Blk already registered with postscriptFonts().
Noto Sans Armenian already registered with postscriptFonts().
Noto Sans Armenian Light already registered with postscriptFonts().
Noto Sans Armenian Med already registered with postscriptFonts().
Noto Sans Armenian Thin already registered with postscriptFonts().
Noto Sans Avestan already registered with postscriptFonts().
Noto Sans Bamum already registered with postscriptFonts().
Noto Sans Bassa Vah already registered with postscriptFonts().
Noto Sans Batak already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Blk. Skipping setup for this font.
Noto Sans Bengali already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Thin. Skipping setup for this font.
Noto Sans Bengali UI already registered with postscriptFonts().
Noto Sans Bhaiksuki already registered with postscriptFonts().
Noto Sans Brahmi already registered with postscriptFonts().
Noto Sans Buginese already registered with postscriptFonts().
Noto Sans Buhid already registered with postscriptFonts().
Noto Sans CanAborig Bk already registered with postscriptFonts().
Noto Sans CanAborig already registered with postscriptFonts().
Noto Sans CanAborig Lt already registered with postscriptFonts().
Noto Sans CanAborig Md already registered with postscriptFonts().
Noto Sans CanAborig Th already registered with postscriptFonts().
Noto Sans Carian already registered with postscriptFonts().
Noto Sans CaucAlban already registered with postscriptFonts().
Noto Sans Chakma already registered with postscriptFonts().
Noto Sans Cham Blk already registered with postscriptFonts().
Noto Sans Cham already registered with postscriptFonts().
Noto Sans Cham Light already registered with postscriptFonts().
Noto Sans Cham Med already registered with postscriptFonts().
Noto Sans Cham Thin already registered with postscriptFonts().
Noto Sans Cherokee Blk already registered with postscriptFonts().
Noto Sans Cherokee already registered with postscriptFonts().
Noto Sans Cherokee Light already registered with postscriptFonts().
Noto Sans Cherokee Med already registered with postscriptFonts().
Noto Sans Cherokee Thin already registered with postscriptFonts().
Noto Sans Coptic already registered with postscriptFonts().
Noto Sans Cuneiform already registered with postscriptFonts().
Noto Sans Cypriot already registered with postscriptFonts().
Noto Sans Deseret already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Bk. Skipping setup for this font.
Noto Sans Devanagari already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Lt. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Md. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Th. Skipping setup for this font.
Noto Sans Devanagari UI already registered with postscriptFonts().
Noto Sans Display Black already registered with postscriptFonts().
Noto Sans Display already registered with postscriptFonts().
Noto Sans Display Light already registered with postscriptFonts().
Noto Sans Display Medium already registered with postscriptFonts().
Noto Sans Display Thin already registered with postscriptFonts().
Noto Sans Duployan already registered with postscriptFonts().
Noto Sans EgyptHiero already registered with postscriptFonts().
Noto Sans Elbasan already registered with postscriptFonts().
Noto Sans Ethiopic Blk already registered with postscriptFonts().
Noto Sans Ethiopic already registered with postscriptFonts().
Noto Sans Ethiopic Light already registered with postscriptFonts().
Noto Sans Ethiopic Med already registered with postscriptFonts().
Noto Sans Ethiopic Thin already registered with postscriptFonts().
Noto Sans Georgian Blk already registered with postscriptFonts().
Noto Sans Georgian already registered with postscriptFonts().
Noto Sans Georgian Light already registered with postscriptFonts().
Noto Sans Georgian Med already registered with postscriptFonts().
Noto Sans Georgian Thin already registered with postscriptFonts().
Noto Sans Glagolitic already registered with postscriptFonts().
Noto Sans Gothic already registered with postscriptFonts().
Noto Sans Grantha already registered with postscriptFonts().
Noto Sans Gujarati already registered with postscriptFonts().
Noto Sans Gujarati UI already registered with postscriptFonts().
Noto Sans Gurmukhi Black already registered with postscriptFonts().
Noto Sans Gurmukhi already registered with postscriptFonts().
Noto Sans Gurmukhi Light already registered with postscriptFonts().
Noto Sans Gurmukhi Medium already registered with postscriptFonts().
Noto Sans Gurmukhi Thin already registered with postscriptFonts().
Noto Sans Gurmukhi UI Black already registered with postscriptFonts().
Noto Sans Gurmukhi UI already registered with postscriptFonts().
Noto Sans Gurmukhi UI Light already registered with postscriptFonts().
Noto Sans Gurmukhi UI Medium already registered with postscriptFonts().
Noto Sans Gurmukhi UI Thin already registered with postscriptFonts().
Noto Sans HanifiRohg already registered with postscriptFonts().
Noto Sans Hanunoo already registered with postscriptFonts().
Noto Sans Hatran already registered with postscriptFonts().
Noto Sans Hebrew Blk already registered with postscriptFonts().
Noto Sans Hebrew already registered with postscriptFonts().
Noto Sans Hebrew Light already registered with postscriptFonts().
Noto Sans Hebrew Med already registered with postscriptFonts().
Noto Sans Hebrew Thin already registered with postscriptFonts().
Noto Sans ImpAramaic already registered with postscriptFonts().
Noto Sans Indic Siyaq Numbers already registered with postscriptFonts().
Noto Sans InsPahlavi already registered with postscriptFonts().
Noto Sans InsParthi already registered with postscriptFonts().
Noto Sans Javanese already registered with postscriptFonts().
Noto Sans Kaithi already registered with postscriptFonts().
Noto Sans Kannada Black already registered with postscriptFonts().
Noto Sans Kannada already registered with postscriptFonts().
Noto Sans Kannada Light already registered with postscriptFonts().
Noto Sans Kannada Medium already registered with postscriptFonts().
Noto Sans Kannada Thin already registered with postscriptFonts().
Noto Sans Kannada UI Black already registered with postscriptFonts().
Noto Sans Kannada UI already registered with postscriptFonts().
Noto Sans Kannada UI Light already registered with postscriptFonts().
Noto Sans Kannada UI Medium already registered with postscriptFonts().
Noto Sans Kannada UI Thin already registered with postscriptFonts().
Noto Sans Kayah Li already registered with postscriptFonts().
Noto Sans Kharoshthi already registered with postscriptFonts().
Noto Sans Khmer Black already registered with postscriptFonts().
Noto Sans Khmer already registered with postscriptFonts().
Noto Sans Khmer Light already registered with postscriptFonts().
Noto Sans Khmer Medium already registered with postscriptFonts().
Noto Sans Khmer Thin already registered with postscriptFonts().
Noto Sans Khmer UI Black already registered with postscriptFonts().
Noto Sans Khmer UI already registered with postscriptFonts().
Noto Sans Khmer UI Light already registered with postscriptFonts().
Noto Sans Khmer UI Medium already registered with postscriptFonts().
Noto Sans Khmer UI Thin already registered with postscriptFonts().
Noto Sans Khojki already registered with postscriptFonts().
Noto Sans Khudawadi already registered with postscriptFonts().
Noto Sans Lao Blk already registered with postscriptFonts().
Noto Sans Lao already registered with postscriptFonts().
Noto Sans Lao Light already registered with postscriptFonts().
Noto Sans Lao Med already registered with postscriptFonts().
Noto Sans Lao Thin already registered with postscriptFonts().
Noto Sans Lao UI Blk already registered with postscriptFonts().
Noto Sans Lao UI already registered with postscriptFonts().
Noto Sans Lao UI Light already registered with postscriptFonts().
Noto Sans Lao UI Med already registered with postscriptFonts().
Noto Sans Lao UI Thin already registered with postscriptFonts().
Noto Sans Lepcha already registered with postscriptFonts().
Noto Sans Limbu already registered with postscriptFonts().
Noto Sans Linear A already registered with postscriptFonts().
Noto Sans Linear B already registered with postscriptFonts().
Noto Sans Lisu already registered with postscriptFonts().
Noto Sans Lycian already registered with postscriptFonts().
Noto Sans Lydian already registered with postscriptFonts().
Noto Sans Mahajani already registered with postscriptFonts().
Noto Sans Malayalam Black already registered with postscriptFonts().
Noto Sans Malayalam already registered with postscriptFonts().
Noto Sans Malayalam Light already registered with postscriptFonts().
Noto Sans Malayalam Medium already registered with postscriptFonts().
Noto Sans Malayalam Thin already registered with postscriptFonts().
Noto Sans Malayalam UI Black already registered with postscriptFonts().
Noto Sans Malayalam UI already registered with postscriptFonts().
Noto Sans Malayalam UI Light already registered with postscriptFonts().
Noto Sans Malayalam UI Medium already registered with postscriptFonts().
Noto Sans Malayalam UI Thin already registered with postscriptFonts().
Noto Sans Mandaic already registered with postscriptFonts().
Noto Sans Manichaean already registered with postscriptFonts().
Noto Sans Marchen already registered with postscriptFonts().
Noto Sans Math already registered with postscriptFonts().
Noto Sans Mayan Numerals already registered with postscriptFonts().
Noto Sans MeeteiMayek already registered with postscriptFonts().
Noto Sans Mende Kikakui already registered with postscriptFonts().
Noto Sans Meroitic already registered with postscriptFonts().
Noto Sans Miao already registered with postscriptFonts().
Noto Sans Modi already registered with postscriptFonts().
Noto Sans Mongolian already registered with postscriptFonts().
Noto Sans Mono Black already registered with postscriptFonts().
Noto Sans Mono already registered with postscriptFonts().
Noto Sans Mono Light already registered with postscriptFonts().
Noto Sans Mono Medium already registered with postscriptFonts().
Noto Sans Mono Thin already registered with postscriptFonts().
Noto Sans Mro already registered with postscriptFonts().
Noto Sans Multani already registered with postscriptFonts().
Noto Sans Myanmar Blk already registered with postscriptFonts().
Noto Sans Myanmar already registered with postscriptFonts().
Noto Sans Myanmar Light already registered with postscriptFonts().
Noto Sans Myanmar Med already registered with postscriptFonts().
Noto Sans Myanmar Thin already registered with postscriptFonts().
Noto Sans Myanmar UI Black already registered with postscriptFonts().
Noto Sans Myanmar UI already registered with postscriptFonts().
Noto Sans Myanmar UI Light already registered with postscriptFonts().
Noto Sans Myanmar UI Medium already registered with postscriptFonts().
Noto Sans Myanmar UI Thin already registered with postscriptFonts().
Noto Sans Nabataean already registered with postscriptFonts().
Noto Sans Newa already registered with postscriptFonts().
Noto Sans NewTaiLue already registered with postscriptFonts().
Noto Sans N'Ko already registered with postscriptFonts().
Noto Sans Ogham already registered with postscriptFonts().
Noto Sans Ol Chiki already registered with postscriptFonts().
Noto Sans OldHung already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Noto Sans Old Italic. Skipping setup for this font.
Noto Sans OldNorArab already registered with postscriptFonts().
Noto Sans Old Permic already registered with postscriptFonts().
Noto Sans OldPersian already registered with postscriptFonts().
Noto Sans OldSogdian already registered with postscriptFonts().
Noto Sans OldSouArab already registered with postscriptFonts().
Noto Sans Old Turkic already registered with postscriptFonts().
Noto Sans Oriya already registered with postscriptFonts().
Noto Sans Oriya UI already registered with postscriptFonts().
Noto Sans Osage already registered with postscriptFonts().
Noto Sans Osmanya already registered with postscriptFonts().
Noto Sans Pahawh Hmong already registered with postscriptFonts().
Noto Sans Palmyrene already registered with postscriptFonts().
Noto Sans PauCinHau already registered with postscriptFonts().
Noto Sans PhagsPa already registered with postscriptFonts().
Noto Sans Phoenician already registered with postscriptFonts().
Noto Sans PsaPahlavi already registered with postscriptFonts().
Noto Sans Rejang already registered with postscriptFonts().
Noto Sans Runic already registered with postscriptFonts().
Noto Sans Samaritan already registered with postscriptFonts().
Noto Sans Saurashtra already registered with postscriptFonts().
Noto Sans Sharada already registered with postscriptFonts().
Noto Sans Shavian already registered with postscriptFonts().
Noto Sans Siddham already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Blk. Skipping setup for this font.
Noto Sans Sinhala already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Thin. Skipping setup for this font.
Noto Sans Sinhala UI already registered with postscriptFonts().
Noto Sans SoraSomp already registered with postscriptFonts().
Noto Sans Sundanese already registered with postscriptFonts().
Noto Sans Syloti Nagri already registered with postscriptFonts().
Noto Sans Symbols Blk already registered with postscriptFonts().
Noto Sans Symbols already registered with postscriptFonts().
Noto Sans Symbols Light already registered with postscriptFonts().
Noto Sans Symbols Med already registered with postscriptFonts().
Noto Sans Symbols Thin already registered with postscriptFonts().
Noto Sans Symbols2 already registered with postscriptFonts().
Noto Sans Syriac Black already registered with postscriptFonts().
Noto Sans Syriac already registered with postscriptFonts().
Noto Sans Syriac Thin already registered with postscriptFonts().
Noto Sans Tagalog already registered with postscriptFonts().
Noto Sans Tagbanwa already registered with postscriptFonts().
Noto Sans Tai Le already registered with postscriptFonts().
Noto Sans Tai Tham already registered with postscriptFonts().
Noto Sans Tai Viet already registered with postscriptFonts().
Noto Sans Takri already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Blk. Skipping setup for this font.
Noto Sans Tamil already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Thin. Skipping setup for this font.
Noto Sans Tamil Supplement already registered with postscriptFonts().
Noto Sans Tamil UI already registered with postscriptFonts().
Noto Sans Telugu Black already registered with postscriptFonts().
Noto Sans Telugu already registered with postscriptFonts().
Noto Sans Telugu Light already registered with postscriptFonts().
Noto Sans Telugu Medium already registered with postscriptFonts().
Noto Sans Telugu Thin already registered with postscriptFonts().
Noto Sans Telugu UI Black already registered with postscriptFonts().
Noto Sans Telugu UI already registered with postscriptFonts().
Noto Sans Telugu UI Light already registered with postscriptFonts().
Noto Sans Telugu UI Medium already registered with postscriptFonts().
Noto Sans Telugu UI Thin already registered with postscriptFonts().
Noto Sans Thaana Black already registered with postscriptFonts().
Noto Sans Thaana already registered with postscriptFonts().
Noto Sans Thaana Light already registered with postscriptFonts().
Noto Sans Thaana Medium already registered with postscriptFonts().
Noto Sans Thaana Thin already registered with postscriptFonts().
Noto Sans Thai Blk already registered with postscriptFonts().
Noto Sans Thai already registered with postscriptFonts().
Noto Sans Thai Light already registered with postscriptFonts().
Noto Sans Thai Med already registered with postscriptFonts().
Noto Sans Thai Thin already registered with postscriptFonts().
Noto Sans Thai UI Blk already registered with postscriptFonts().
Noto Sans Thai UI already registered with postscriptFonts().
Noto Sans Thai UI Light already registered with postscriptFonts().
Noto Sans Thai UI Med already registered with postscriptFonts().
Noto Sans Thai UI Thin already registered with postscriptFonts().
Noto Sans Tibetan already registered with postscriptFonts().
Noto Sans Tifinagh already registered with postscriptFonts().
Noto Sans Tirhuta already registered with postscriptFonts().
Noto Sans Ugaritic already registered with postscriptFonts().
Noto Sans Vai already registered with postscriptFonts().
Noto Sans WarangCiti already registered with postscriptFonts().
Noto Sans Yi already registered with postscriptFonts().
Noto Serif Black already registered with postscriptFonts().
Noto Serif already registered with postscriptFonts().
Noto Serif Light already registered with postscriptFonts().
Noto Serif Medium already registered with postscriptFonts().
Noto Serif Thin already registered with postscriptFonts().
Noto Serif Ahom already registered with postscriptFonts().
Noto Serif Armenian Bk already registered with postscriptFonts().
Noto Serif Armenian already registered with postscriptFonts().
Noto Serif Armenian Lt already registered with postscriptFonts().
Noto Serif Armenian Md already registered with postscriptFonts().
Noto Serif Armenian Th already registered with postscriptFonts().
Noto Serif Balinese already registered with postscriptFonts().
Noto Serif Bengali Black already registered with postscriptFonts().
Noto Serif Bengali already registered with postscriptFonts().
Noto Serif Bengali Light already registered with postscriptFonts().
Noto Serif Bengali Medium already registered with postscriptFonts().
Noto Serif Bengali Thin already registered with postscriptFonts().
Noto Serif Devanagari Black already registered with postscriptFonts().
Noto Serif Devanagari already registered with postscriptFonts().
Noto Serif Devanagari Light already registered with postscriptFonts().
Noto Serif Devanagari Medium already registered with postscriptFonts().
Noto Serif Devanagari Thin already registered with postscriptFonts().
Noto Serif Display Black already registered with postscriptFonts().
Noto Serif Display already registered with postscriptFonts().
Noto Serif Display Light already registered with postscriptFonts().
Noto Serif Display Medium already registered with postscriptFonts().
Noto Serif Display Thin already registered with postscriptFonts().
Noto Serif Dogra already registered with postscriptFonts().
Noto Serif Ethiopic Bk already registered with postscriptFonts().
Noto Serif Ethiopic already registered with postscriptFonts().
Noto Serif Ethiopic Lt already registered with postscriptFonts().
Noto Serif Ethiopic Md already registered with postscriptFonts().
Noto Serif Ethiopic Th already registered with postscriptFonts().
Noto Serif Georgian Bk already registered with postscriptFonts().
Noto Serif Georgian already registered with postscriptFonts().
Noto Serif Georgian Lt already registered with postscriptFonts().
Noto Serif Georgian Md already registered with postscriptFonts().
Noto Serif Georgian Th already registered with postscriptFonts().
Noto Serif Gujarati Black already registered with postscriptFonts().
Noto Serif Gujarati already registered with postscriptFonts().
Noto Serif Gujarati Light already registered with postscriptFonts().
Noto Serif Gujarati Medium already registered with postscriptFonts().
Noto Serif Gujarati Thin already registered with postscriptFonts().
Noto Serif Gurmukhi Black already registered with postscriptFonts().
Noto Serif Gurmukhi already registered with postscriptFonts().
Noto Serif Gurmukhi Light already registered with postscriptFonts().
Noto Serif Gurmukhi Medium already registered with postscriptFonts().
Noto Serif Gurmukhi Thin already registered with postscriptFonts().
Noto Serif Hebrew Blk already registered with postscriptFonts().
Noto Serif Hebrew already registered with postscriptFonts().
Noto Serif Hebrew Light already registered with postscriptFonts().
Noto Serif Hebrew Med already registered with postscriptFonts().
Noto Serif Hebrew Thin already registered with postscriptFonts().
Noto Serif Kannada Black already registered with postscriptFonts().
Noto Serif Kannada already registered with postscriptFonts().
Noto Serif Kannada Light already registered with postscriptFonts().
Noto Serif Kannada Medium already registered with postscriptFonts().
Noto Serif Kannada Thin already registered with postscriptFonts().
Noto Serif Khmer Black already registered with postscriptFonts().
Noto Serif Khmer already registered with postscriptFonts().
Noto Serif Khmer Light already registered with postscriptFonts().
Noto Serif Khmer Medium already registered with postscriptFonts().
Noto Serif Khmer Thin already registered with postscriptFonts().
Noto Serif Lao Blk already registered with postscriptFonts().
Noto Serif Lao already registered with postscriptFonts().
Noto Serif Lao Light already registered with postscriptFonts().
Noto Serif Lao Med already registered with postscriptFonts().
Noto Serif Lao Thin already registered with postscriptFonts().
Noto Serif Malayalam Black already registered with postscriptFonts().
Noto Serif Malayalam already registered with postscriptFonts().
Noto Serif Malayalam Light already registered with postscriptFonts().
Noto Serif Malayalam Medium already registered with postscriptFonts().
Noto Serif Malayalam Thin already registered with postscriptFonts().
Noto Serif Myanmar Blk already registered with postscriptFonts().
Noto Serif Myanmar already registered with postscriptFonts().
Noto Serif Myanmar Light already registered with postscriptFonts().
Noto Serif Myanmar Med already registered with postscriptFonts().
Noto Serif Myanmar Thin already registered with postscriptFonts().
Noto Serif Sinhala Black already registered with postscriptFonts().
Noto Serif Sinhala already registered with postscriptFonts().
Noto Serif Sinhala Light already registered with postscriptFonts().
Noto Serif Sinhala Medium already registered with postscriptFonts().
Noto Serif Sinhala Thin already registered with postscriptFonts().
Noto Serif Tamil Blk already registered with postscriptFonts().
Noto Serif Tamil already registered with postscriptFonts().
Noto Serif Tamil Light already registered with postscriptFonts().
Noto Serif Tamil Med already registered with postscriptFonts().
Noto Serif Tamil Thin already registered with postscriptFonts().
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Black. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Light. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Medium. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Thin. Skipping setup for this font.
Noto Serif Tangut already registered with postscriptFonts().
Noto Serif Telugu Black already registered with postscriptFonts().
Noto Serif Telugu already registered with postscriptFonts().
Noto Serif Telugu Light already registered with postscriptFonts().
Noto Serif Telugu Medium already registered with postscriptFonts().
Noto Serif Telugu Thin already registered with postscriptFonts().
Noto Serif Thai Blk already registered with postscriptFonts().
Noto Serif Thai already registered with postscriptFonts().
Noto Serif Thai Light already registered with postscriptFonts().
Noto Serif Thai Med already registered with postscriptFonts().
Noto Serif Thai Thin already registered with postscriptFonts().
Noto Serif Tibetan Black already registered with postscriptFonts().
Noto Serif Tibetan already registered with postscriptFonts().
Noto Serif Tibetan Light already registered with postscriptFonts().
Noto Serif Tibetan Medium already registered with postscriptFonts().
Noto Serif Tibetan Thin already registered with postscriptFonts().
NovaMono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Nunito. Skipping setup for this font.
orya already registered with postscriptFonts().
More than one version of regular/bold/italic found for padmaa. Skipping setup for this font.
Pothana2000 already registered with postscriptFonts().
ProFont for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Roboto. Skipping setup for this font.
More than one version of regular/bold/italic found for Roboto Condensed. Skipping setup for this font.
Roboto Mono for Powerline already registered with postscriptFonts().
Roboto Mono Light for Powerline already registered with postscriptFonts().
Roboto Mono Medium for Powerline already registered with postscriptFonts().
Roboto Mono Thin for Powerline already registered with postscriptFonts().
Sagar already registered with postscriptFonts().
Space Mono already registered with postscriptFonts().
Space Mono for Powerline already registered with postscriptFonts().
Symbol Neu for Powerline already registered with postscriptFonts().
TAMu_Kadambri already registered with postscriptFonts().
TAMu_Kalyani already registered with postscriptFonts().
TAMu_Maduram already registered with postscriptFonts().
Tinos for Powerline already registered with postscriptFonts().
TSCu_Comic already registered with postscriptFonts().
TSCu_Paranar already registered with postscriptFonts().
TSCu_Times already registered with postscriptFonts().
Ubuntu already registered with postscriptFonts().
Ubuntu Light already registered with postscriptFonts().
Ubuntu Condensed already registered with postscriptFonts().
Ubuntu Mono already registered with postscriptFonts().
Ubuntu Mono derivative Powerline already registered with postscriptFonts().
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_search_spaces)
#+end_SRC

#+RESULTS:
: 'data.frame':	69 obs. of  8 variables:
:  $ name                   : chr  "atax" "dgemv3" "fdtd4d2d" "gemver" ...
:  $ year                   : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...
:  $ dimension              : int  19 49 30 24 11 15 14 12 20 25 ...
:  $ search_space_size      : num  1.65e+14 2.73e+30 7.06e+24 7.26e+17 1.56e+08 ...
:  $ log10_search_space_size: int  14 30 24 17 8 8 12 8 16 19 ...
:  $ domain                 : chr  "Linear Algebra" "Linear Algebra" "Linear Algebra" "Linear Algebra" ...
:  $ author                 : chr  "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" ...
:  $ gscholar_citation      : chr  "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" ...
***** Generate Caption
#+begin_SRC R :results output :session *R* :eval no-export :exports results
citations <- unique(df_search_spaces$gscholar_citation)
citations <- paste(citations[citations != ""], collapse = ",")
cat(paste("\\nbsp{}\\cite{", citations, "}", sep = ""))
#+end_SRC

#+RESULTS:
:
: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,abdelfattah2016performance,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}

***** Plots
- Geom_label around thesis work
- Mark Seymour et al. example
#+begin_SRC R :results graphics output :session *R* :file "./img/search_spaces.pdf" :width 18 :height 8.7 :eval no-export
library(ggplot2)
library(dplyr)
library(scales)
library(RColorBrewer)
library(ggrepel)
library(patchwork)

point_alpha = 1.0
point_size = 3
label_size = 6

shapes = c(15, 16, 17, 18, 6, 7, 9, 0, 3, 5, 12, 14, 13, 11)

legend_rows = length(unique(df_search_spaces$domain)) / 2
legend_position = c(0.66, 0.12)

base_size = 25
font_family = "Liberation Sans"

x_lims <- c(0, 60)
y_lims <- c(1, 50)

color_palette = colorRampPalette(brewer.pal(9,
                                            "Set1"))(
                                                length(
                                                    unique(
                                                        df_search_spaces$domain)))

x_text = element_text(size = 26)
y_text = element_text(size = 26)

x_label = element_text(size = 28)
y_label = element_text(size = 28)

scientific_10 <- function(x) {
    print(x)
    result <- parse(text = gsub("(.*)",
                                "10^\\1",
                                format(x)))
    print(result)
    return(result)
}

p1 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_rect(aes(xmin = x_lims[1],
                  xmax = x_lims[2],
                  ymin = y_lims[1],
                  ymax = y_lims[2]),
              show.legend = FALSE,
              fill = NA,
              color = "gray35",
              linetype = 2) +
    geom_text(data = data.frame(x = x_lims[2],
                                y = y_lims[1],
                                label = "Detailed"),
              aes(x = x,
                  y = y,
                  label = label,
                  shape = NA),
              color = "gray35",
              vjust = 1.3,
              hjust = 0,
              angle = 90,
              size = label_size,
              show.legend = FALSE) +
    geom_point(alpha = point_alpha,
               size = point_size,
               show.legend = FALSE) +
    geom_text_repel(data = filter(df_search_spaces,
                                  thesis == FALSE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    xlim = c(130, 200),
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  name != "resnet50_weights" &
                                  name != "gemv" &
                                  dimension > 40 &
                                  log10_search_space_size >= 30),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    xlim = c(130, NA),
                    ylim = c(NA, NA),
                    nudge_y = 1,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30) %>%
                     filter(name == "resnet50_weights"),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(60, NA),
                    xlim = c(130, NA),
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30) %>%
                     filter(name == "gemv"),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(50, NA),
                    xlim = c(130, NA),
                    size = label_size,
                    show.legend = FALSE) +
    xlab("Dimension") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(text = element_text(family = font_family),
          axis.text.x = x_text,
          axis.text.y = y_text,
          axis.title.y = element_blank())

p2 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_point(alpha = point_alpha,
               size = point_size) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 40 &
                               dimension < 60 &
                               thesis == FALSE &
                               log10_search_space_size >= 30,
                               log10_search_space_size < 50),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(30, 50),
                    xlim = c(NA, 45),
                    nudge_x = -3,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension > 40 &
                               dimension < 60 &
                               thesis == TRUE &
                               log10_search_space_size >= 30,
                               log10_search_space_size < 50),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(30, 50),
                    xlim = c(NA, 45),
                    nudge_x = -3,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension < 20 &
                               thesis == FALSE &
                               log10_search_space_size > 19),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(20, 50),
                    xlim = c(0, 25),
                    nudge_y = 1.6,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension < 20 &
                               thesis == TRUE &
                               log10_search_space_size > 19),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(20, 50),
                    xlim = c(0, 25),
                    nudge_y = 1.6,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 30 &
                               thesis == FALSE &
                               log10_search_space_size < 25),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(15, 50),
                    xlim = c(30, NA),
                    nudge_y = 2,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension > 30 &
                               thesis == TRUE &
                               log10_search_space_size < 25),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(15, 50),
                    xlim = c(30, NA),
                    nudge_y = 2,
                    size = label_size,
                    show.legend = FALSE) +
    xlim(x_lims[1], x_lims[2]) +
    xlab("Dimension") +
    ylab("Search Space Size") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(limits = y_lims, label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(axis.text.x = x_text,
          axis.text.y = y_text,
          legend.position = legend_position,
          legend.direction = "horizontal",
          legend.spacing.x = unit(0.0, 'cm'),
          legend.spacing.y = unit(0.0, 'cm'),
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 15),
          text = element_text(family = font_family)) +
    guides(color = guide_legend(nrow = legend_rows,
                                override.aes = list(alpha = 1.0,
                                                    size = 3)))

p2 * p1
#+end_SRC

#+RESULTS:
[[file:./img/search_spaces.pdf]]
**** Loop Blocking and Unrolling
***** First try
:PROPERTIES:
:EXPORT_FILE_NAME: blocking_unrolling.pdf
:END:

#+begin_src latex :fit true
% Export this heading with C-c C-e C-s C-b l p
\documentclass{standalone}
\usepackage{tikz}
\begin{document}
\begin{tikzpicture}
\draw[red] (0,0) circle (2cm);
\end{tikzpicture}
\end{document}
#+end_src try with export
***** Arnaud's tip with src blocks
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
\\usepackage[usenames]{color}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: \usepackage[usenames]{color}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage{tikz}")
#+HEADER: :exports results :results raw :file ./img/test.pdf
#+begin_src latex :eval no-export
\begin{tikzpicture}
\draw[blue] (0,0) circle (2cm);
\end{tikzpicture}
#+end_src

#+RESULTS:
[[file:./img/test.pdf]]

***** ggplot
****** Setup
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(extrafont)
loadfonts(device = "postscript")
#+end_SRC

#+RESULTS:
#+begin_example

Akaash already registered with postscriptFonts().
AkrutiMal1 already registered with postscriptFonts().
AkrutiMal2 already registered with postscriptFonts().
AkrutiTml1 already registered with postscriptFonts().
AkrutiTml2 already registered with postscriptFonts().
Anonymice Powerline already registered with postscriptFonts().
Arimo for Powerline already registered with postscriptFonts().
Bitstream Vera Sans already registered with postscriptFonts().
Bitstream Vera Sans Mono already registered with postscriptFonts().
Bitstream Vera Serif already registered with postscriptFonts().
Cousine for Powerline already registered with postscriptFonts().
IBM 3270 already registered with postscriptFonts().
IBM 3270 Narrow already registered with postscriptFonts().
IBM 3270 Semi-Narrow already registered with postscriptFonts().
DejaVu Math TeX Gyre already registered with postscriptFonts().
DejaVu Sans already registered with postscriptFonts().
DejaVu Sans Light already registered with postscriptFonts().
DejaVu Sans Condensed already registered with postscriptFonts().
DejaVu Sans Mono already registered with postscriptFonts().
DejaVu Sans Mono for Powerline already registered with postscriptFonts().
DejaVu Serif already registered with postscriptFonts().
DejaVu Serif Condensed already registered with postscriptFonts().
Droid Arabic Kufi already registered with postscriptFonts().
Droid Arabic Naskh already registered with postscriptFonts().
Droid Naskh Shift Alt already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans. Skipping setup for this font.
Droid Sans Arabic already registered with postscriptFonts().
Droid Sans Armenian already registered with postscriptFonts().
Droid Sans Devanagari already registered with postscriptFonts().
Droid Sans Ethiopic already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Fallback. Skipping setup for this font.
Droid Sans Georgian already registered with postscriptFonts().
Droid Sans Hebrew already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Mono. Skipping setup for this font.
Droid Sans Mono Dotted for Powerline already registered with postscriptFonts().
Droid Sans Mono Slashed for Powerline already registered with postscriptFonts().
Droid Sans Tamil already registered with postscriptFonts().
Droid Sans Thai already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Serif. Skipping setup for this font.
Font Awesome 5 Brands Regular already registered with postscriptFonts().
Font Awesome 5 Free Regular already registered with postscriptFonts().
Font Awesome 5 Free Solid already registered with postscriptFonts().
Gargi-1.2b already registered with postscriptFonts().
Goha-Tibeb Zemen already registered with postscriptFonts().
Go Mono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for GurbaniBoliLite. Skipping setup for this font.
Hack already registered with postscriptFonts().
Inconsolata Black already registered with postscriptFonts().
Inconsolata already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Inconsolata for Powerline. Skipping setup for this font.
Inconsolata Condensed already registered with postscriptFonts().
Inconsolata Condensed Black already registered with postscriptFonts().
Inconsolata Condensed Bold already registered with postscriptFonts().
Inconsolata Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Condensed Light already registered with postscriptFonts().
Inconsolata Condensed Medium already registered with postscriptFonts().
Inconsolata Condensed SemiBold already registered with postscriptFonts().
Inconsolata Expanded already registered with postscriptFonts().
Inconsolata Expanded Black already registered with postscriptFonts().
Inconsolata Expanded Bold already registered with postscriptFonts().
Inconsolata Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Expanded Light already registered with postscriptFonts().
Inconsolata Expanded Medium already registered with postscriptFonts().
Inconsolata Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed already registered with postscriptFonts().
Inconsolata Extra Condensed Black already registered with postscriptFonts().
Inconsolata Extra Condensed Bold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Extra Condensed Light already registered with postscriptFonts().
Inconsolata Extra Condensed Medium already registered with postscriptFonts().
Inconsolata Extra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Extra Expanded already registered with postscriptFonts().
Inconsolata Extra Expanded Black already registered with postscriptFonts().
Inconsolata Extra Expanded Bold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Extra Expanded Light already registered with postscriptFonts().
Inconsolata Extra Expanded Medium already registered with postscriptFonts().
Inconsolata Extra Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraLight already registered with postscriptFonts().
Inconsolata Light already registered with postscriptFonts().
Inconsolata Medium already registered with postscriptFonts().
Inconsolata SemiBold already registered with postscriptFonts().
Inconsolata Semi Condensed already registered with postscriptFonts().
Inconsolata Semi Condensed Black already registered with postscriptFonts().
Inconsolata Semi Condensed Bold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Semi Condensed Light already registered with postscriptFonts().
Inconsolata Semi Condensed Medium already registered with postscriptFonts().
Inconsolata Semi Condensed SemiBold already registered with postscriptFonts().
Inconsolata Semi Expanded already registered with postscriptFonts().
Inconsolata Semi Expanded Black already registered with postscriptFonts().
Inconsolata Semi Expanded Bold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Semi Expanded Light already registered with postscriptFonts().
Inconsolata Semi Expanded Medium already registered with postscriptFonts().
Inconsolata Semi Expanded SemiBold already registered with postscriptFonts().
Inconsolata Ultra Condensed already registered with postscriptFonts().
Inconsolata Ultra Condensed Black already registered with postscriptFonts().
Inconsolata Ultra Condensed Bold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Condensed Light already registered with postscriptFonts().
Inconsolata Ultra Condensed Medium already registered with postscriptFonts().
Inconsolata Ultra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Ultra Expanded already registered with postscriptFonts().
Inconsolata Ultra Expanded Black already registered with postscriptFonts().
Inconsolata Ultra Expanded Bold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Expanded Light already registered with postscriptFonts().
Inconsolata Ultra Expanded Medium already registered with postscriptFonts().
Inconsolata Ultra Expanded SemiBold already registered with postscriptFonts().
Liberation Mono already registered with postscriptFonts().
Liberation Sans already registered with postscriptFonts().
Liberation Serif already registered with postscriptFonts().
Ligconsolata already registered with postscriptFonts().
Likhan already registered with postscriptFonts().
Literation Mono Powerline already registered with postscriptFonts().
malayalam already registered with postscriptFonts().
MalOtf already registered with postscriptFonts().
Meslo LG L DZ for Powerline already registered with postscriptFonts().
Meslo LG L for Powerline already registered with postscriptFonts().
Meslo LG M DZ for Powerline already registered with postscriptFonts().
Meslo LG M for Powerline already registered with postscriptFonts().
Meslo LG S DZ for Powerline already registered with postscriptFonts().
Meslo LG S for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for monofur for Powerline. Skipping setup for this font.
More than one version of regular/bold/italic found for Mukti Narrow. Skipping setup for this font.
Noto Kufi Arabic already registered with postscriptFonts().
Noto Kufi Arabic Medium already registered with postscriptFonts().
Noto Kufi Arabic Semi bold already registered with postscriptFonts().
Noto Mono for Powerline already registered with postscriptFonts().
Noto Music already registered with postscriptFonts().
Noto Naskh Arabic already registered with postscriptFonts().
Noto Naskh Arabic UI already registered with postscriptFonts().
Noto Nastaliq Urdu already registered with postscriptFonts().
Noto Sans Black already registered with postscriptFonts().
Noto Sans already registered with postscriptFonts().
Noto Sans Light already registered with postscriptFonts().
Noto Sans Medium already registered with postscriptFonts().
Noto Sans Thin already registered with postscriptFonts().
Noto Sans Adlam already registered with postscriptFonts().
Noto Sans Adlam Unjoined already registered with postscriptFonts().
Noto Sans AnatoHiero already registered with postscriptFonts().
Noto Sans Arabic Blk already registered with postscriptFonts().
Noto Sans Arabic already registered with postscriptFonts().
Noto Sans Arabic Light already registered with postscriptFonts().
Noto Sans Arabic Med already registered with postscriptFonts().
Noto Sans Arabic Thin already registered with postscriptFonts().
Noto Sans Arabic UI Bk already registered with postscriptFonts().
Noto Sans Arabic UI already registered with postscriptFonts().
Noto Sans Arabic UI Lt already registered with postscriptFonts().
Noto Sans Arabic UI Md already registered with postscriptFonts().
Noto Sans Arabic UI Th already registered with postscriptFonts().
Noto Sans Armenian Blk already registered with postscriptFonts().
Noto Sans Armenian already registered with postscriptFonts().
Noto Sans Armenian Light already registered with postscriptFonts().
Noto Sans Armenian Med already registered with postscriptFonts().
Noto Sans Armenian Thin already registered with postscriptFonts().
Noto Sans Avestan already registered with postscriptFonts().
Noto Sans Bamum already registered with postscriptFonts().
Noto Sans Bassa Vah already registered with postscriptFonts().
Noto Sans Batak already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Blk. Skipping setup for this font.
Noto Sans Bengali already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Thin. Skipping setup for this font.
Noto Sans Bengali UI already registered with postscriptFonts().
Noto Sans Bhaiksuki already registered with postscriptFonts().
Noto Sans Brahmi already registered with postscriptFonts().
Noto Sans Buginese already registered with postscriptFonts().
Noto Sans Buhid already registered with postscriptFonts().
Noto Sans CanAborig Bk already registered with postscriptFonts().
Noto Sans CanAborig already registered with postscriptFonts().
Noto Sans CanAborig Lt already registered with postscriptFonts().
Noto Sans CanAborig Md already registered with postscriptFonts().
Noto Sans CanAborig Th already registered with postscriptFonts().
Noto Sans Carian already registered with postscriptFonts().
Noto Sans CaucAlban already registered with postscriptFonts().
Noto Sans Chakma already registered with postscriptFonts().
Noto Sans Cham Blk already registered with postscriptFonts().
Noto Sans Cham already registered with postscriptFonts().
Noto Sans Cham Light already registered with postscriptFonts().
Noto Sans Cham Med already registered with postscriptFonts().
Noto Sans Cham Thin already registered with postscriptFonts().
Noto Sans Cherokee Blk already registered with postscriptFonts().
Noto Sans Cherokee already registered with postscriptFonts().
Noto Sans Cherokee Light already registered with postscriptFonts().
Noto Sans Cherokee Med already registered with postscriptFonts().
Noto Sans Cherokee Thin already registered with postscriptFonts().
Noto Sans Coptic already registered with postscriptFonts().
Noto Sans Cuneiform already registered with postscriptFonts().
Noto Sans Cypriot already registered with postscriptFonts().
Noto Sans Deseret already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Bk. Skipping setup for this font.
Noto Sans Devanagari already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Lt. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Md. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Th. Skipping setup for this font.
Noto Sans Devanagari UI already registered with postscriptFonts().
Noto Sans Display Black already registered with postscriptFonts().
Noto Sans Display already registered with postscriptFonts().
Noto Sans Display Light already registered with postscriptFonts().
Noto Sans Display Medium already registered with postscriptFonts().
Noto Sans Display Thin already registered with postscriptFonts().
Noto Sans Duployan already registered with postscriptFonts().
Noto Sans EgyptHiero already registered with postscriptFonts().
Noto Sans Elbasan already registered with postscriptFonts().
Noto Sans Ethiopic Blk already registered with postscriptFonts().
Noto Sans Ethiopic already registered with postscriptFonts().
Noto Sans Ethiopic Light already registered with postscriptFonts().
Noto Sans Ethiopic Med already registered with postscriptFonts().
Noto Sans Ethiopic Thin already registered with postscriptFonts().
Noto Sans Georgian Blk already registered with postscriptFonts().
Noto Sans Georgian already registered with postscriptFonts().
Noto Sans Georgian Light already registered with postscriptFonts().
Noto Sans Georgian Med already registered with postscriptFonts().
Noto Sans Georgian Thin already registered with postscriptFonts().
Noto Sans Glagolitic already registered with postscriptFonts().
Noto Sans Gothic already registered with postscriptFonts().
Noto Sans Grantha already registered with postscriptFonts().
Noto Sans Gujarati already registered with postscriptFonts().
Noto Sans Gujarati UI already registered with postscriptFonts().
Noto Sans Gurmukhi Black already registered with postscriptFonts().
Noto Sans Gurmukhi already registered with postscriptFonts().
Noto Sans Gurmukhi Light already registered with postscriptFonts().
Noto Sans Gurmukhi Medium already registered with postscriptFonts().
Noto Sans Gurmukhi Thin already registered with postscriptFonts().
Noto Sans Gurmukhi UI Black already registered with postscriptFonts().
Noto Sans Gurmukhi UI already registered with postscriptFonts().
Noto Sans Gurmukhi UI Light already registered with postscriptFonts().
Noto Sans Gurmukhi UI Medium already registered with postscriptFonts().
Noto Sans Gurmukhi UI Thin already registered with postscriptFonts().
Noto Sans HanifiRohg already registered with postscriptFonts().
Noto Sans Hanunoo already registered with postscriptFonts().
Noto Sans Hatran already registered with postscriptFonts().
Noto Sans Hebrew Blk already registered with postscriptFonts().
Noto Sans Hebrew already registered with postscriptFonts().
Noto Sans Hebrew Light already registered with postscriptFonts().
Noto Sans Hebrew Med already registered with postscriptFonts().
Noto Sans Hebrew Thin already registered with postscriptFonts().
Noto Sans ImpAramaic already registered with postscriptFonts().
Noto Sans Indic Siyaq Numbers already registered with postscriptFonts().
Noto Sans InsPahlavi already registered with postscriptFonts().
Noto Sans InsParthi already registered with postscriptFonts().
Noto Sans Javanese already registered with postscriptFonts().
Noto Sans Kaithi already registered with postscriptFonts().
Noto Sans Kannada Black already registered with postscriptFonts().
Noto Sans Kannada already registered with postscriptFonts().
Noto Sans Kannada Light already registered with postscriptFonts().
Noto Sans Kannada Medium already registered with postscriptFonts().
Noto Sans Kannada Thin already registered with postscriptFonts().
Noto Sans Kannada UI Black already registered with postscriptFonts().
Noto Sans Kannada UI already registered with postscriptFonts().
Noto Sans Kannada UI Light already registered with postscriptFonts().
Noto Sans Kannada UI Medium already registered with postscriptFonts().
Noto Sans Kannada UI Thin already registered with postscriptFonts().
Noto Sans Kayah Li already registered with postscriptFonts().
Noto Sans Kharoshthi already registered with postscriptFonts().
Noto Sans Khmer Black already registered with postscriptFonts().
Noto Sans Khmer already registered with postscriptFonts().
Noto Sans Khmer Light already registered with postscriptFonts().
Noto Sans Khmer Medium already registered with postscriptFonts().
Noto Sans Khmer Thin already registered with postscriptFonts().
Noto Sans Khmer UI Black already registered with postscriptFonts().
Noto Sans Khmer UI already registered with postscriptFonts().
Noto Sans Khmer UI Light already registered with postscriptFonts().
Noto Sans Khmer UI Medium already registered with postscriptFonts().
Noto Sans Khmer UI Thin already registered with postscriptFonts().
Noto Sans Khojki already registered with postscriptFonts().
Noto Sans Khudawadi already registered with postscriptFonts().
Noto Sans Lao Blk already registered with postscriptFonts().
Noto Sans Lao already registered with postscriptFonts().
Noto Sans Lao Light already registered with postscriptFonts().
Noto Sans Lao Med already registered with postscriptFonts().
Noto Sans Lao Thin already registered with postscriptFonts().
Noto Sans Lao UI Blk already registered with postscriptFonts().
Noto Sans Lao UI already registered with postscriptFonts().
Noto Sans Lao UI Light already registered with postscriptFonts().
Noto Sans Lao UI Med already registered with postscriptFonts().
Noto Sans Lao UI Thin already registered with postscriptFonts().
Noto Sans Lepcha already registered with postscriptFonts().
Noto Sans Limbu already registered with postscriptFonts().
Noto Sans Linear A already registered with postscriptFonts().
Noto Sans Linear B already registered with postscriptFonts().
Noto Sans Lisu already registered with postscriptFonts().
Noto Sans Lycian already registered with postscriptFonts().
Noto Sans Lydian already registered with postscriptFonts().
Noto Sans Mahajani already registered with postscriptFonts().
Noto Sans Malayalam Black already registered with postscriptFonts().
Noto Sans Malayalam already registered with postscriptFonts().
Noto Sans Malayalam Light already registered with postscriptFonts().
Noto Sans Malayalam Medium already registered with postscriptFonts().
Noto Sans Malayalam Thin already registered with postscriptFonts().
Noto Sans Malayalam UI Black already registered with postscriptFonts().
Noto Sans Malayalam UI already registered with postscriptFonts().
Noto Sans Malayalam UI Light already registered with postscriptFonts().
Noto Sans Malayalam UI Medium already registered with postscriptFonts().
Noto Sans Malayalam UI Thin already registered with postscriptFonts().
Noto Sans Mandaic already registered with postscriptFonts().
Noto Sans Manichaean already registered with postscriptFonts().
Noto Sans Marchen already registered with postscriptFonts().
Noto Sans Math already registered with postscriptFonts().
Noto Sans Mayan Numerals already registered with postscriptFonts().
Noto Sans MeeteiMayek already registered with postscriptFonts().
Noto Sans Mende Kikakui already registered with postscriptFonts().
Noto Sans Meroitic already registered with postscriptFonts().
Noto Sans Miao already registered with postscriptFonts().
Noto Sans Modi already registered with postscriptFonts().
Noto Sans Mongolian already registered with postscriptFonts().
Noto Sans Mono Black already registered with postscriptFonts().
Noto Sans Mono already registered with postscriptFonts().
Noto Sans Mono Light already registered with postscriptFonts().
Noto Sans Mono Medium already registered with postscriptFonts().
Noto Sans Mono Thin already registered with postscriptFonts().
Noto Sans Mro already registered with postscriptFonts().
Noto Sans Multani already registered with postscriptFonts().
Noto Sans Myanmar Blk already registered with postscriptFonts().
Noto Sans Myanmar already registered with postscriptFonts().
Noto Sans Myanmar Light already registered with postscriptFonts().
Noto Sans Myanmar Med already registered with postscriptFonts().
Noto Sans Myanmar Thin already registered with postscriptFonts().
Noto Sans Myanmar UI Black already registered with postscriptFonts().
Noto Sans Myanmar UI already registered with postscriptFonts().
Noto Sans Myanmar UI Light already registered with postscriptFonts().
Noto Sans Myanmar UI Medium already registered with postscriptFonts().
Noto Sans Myanmar UI Thin already registered with postscriptFonts().
Noto Sans Nabataean already registered with postscriptFonts().
Noto Sans Newa already registered with postscriptFonts().
Noto Sans NewTaiLue already registered with postscriptFonts().
Noto Sans N'Ko already registered with postscriptFonts().
Noto Sans Ogham already registered with postscriptFonts().
Noto Sans Ol Chiki already registered with postscriptFonts().
Noto Sans OldHung already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Noto Sans Old Italic. Skipping setup for this font.
Noto Sans OldNorArab already registered with postscriptFonts().
Noto Sans Old Permic already registered with postscriptFonts().
Noto Sans OldPersian already registered with postscriptFonts().
Noto Sans OldSogdian already registered with postscriptFonts().
Noto Sans OldSouArab already registered with postscriptFonts().
Noto Sans Old Turkic already registered with postscriptFonts().
Noto Sans Oriya already registered with postscriptFonts().
Noto Sans Oriya UI already registered with postscriptFonts().
Noto Sans Osage already registered with postscriptFonts().
Noto Sans Osmanya already registered with postscriptFonts().
Noto Sans Pahawh Hmong already registered with postscriptFonts().
Noto Sans Palmyrene already registered with postscriptFonts().
Noto Sans PauCinHau already registered with postscriptFonts().
Noto Sans PhagsPa already registered with postscriptFonts().
Noto Sans Phoenician already registered with postscriptFonts().
Noto Sans PsaPahlavi already registered with postscriptFonts().
Noto Sans Rejang already registered with postscriptFonts().
Noto Sans Runic already registered with postscriptFonts().
Noto Sans Samaritan already registered with postscriptFonts().
Noto Sans Saurashtra already registered with postscriptFonts().
Noto Sans Sharada already registered with postscriptFonts().
Noto Sans Shavian already registered with postscriptFonts().
Noto Sans Siddham already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Blk. Skipping setup for this font.
Noto Sans Sinhala already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Thin. Skipping setup for this font.
Noto Sans Sinhala UI already registered with postscriptFonts().
Noto Sans SoraSomp already registered with postscriptFonts().
Noto Sans Sundanese already registered with postscriptFonts().
Noto Sans Syloti Nagri already registered with postscriptFonts().
Noto Sans Symbols Blk already registered with postscriptFonts().
Noto Sans Symbols already registered with postscriptFonts().
Noto Sans Symbols Light already registered with postscriptFonts().
Noto Sans Symbols Med already registered with postscriptFonts().
Noto Sans Symbols Thin already registered with postscriptFonts().
Noto Sans Symbols2 already registered with postscriptFonts().
Noto Sans Syriac Black already registered with postscriptFonts().
Noto Sans Syriac already registered with postscriptFonts().
Noto Sans Syriac Thin already registered with postscriptFonts().
Noto Sans Tagalog already registered with postscriptFonts().
Noto Sans Tagbanwa already registered with postscriptFonts().
Noto Sans Tai Le already registered with postscriptFonts().
Noto Sans Tai Tham already registered with postscriptFonts().
Noto Sans Tai Viet already registered with postscriptFonts().
Noto Sans Takri already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Blk. Skipping setup for this font.
Noto Sans Tamil already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Thin. Skipping setup for this font.
Noto Sans Tamil Supplement already registered with postscriptFonts().
Noto Sans Tamil UI already registered with postscriptFonts().
Noto Sans Telugu Black already registered with postscriptFonts().
Noto Sans Telugu already registered with postscriptFonts().
Noto Sans Telugu Light already registered with postscriptFonts().
Noto Sans Telugu Medium already registered with postscriptFonts().
Noto Sans Telugu Thin already registered with postscriptFonts().
Noto Sans Telugu UI Black already registered with postscriptFonts().
Noto Sans Telugu UI already registered with postscriptFonts().
Noto Sans Telugu UI Light already registered with postscriptFonts().
Noto Sans Telugu UI Medium already registered with postscriptFonts().
Noto Sans Telugu UI Thin already registered with postscriptFonts().
Noto Sans Thaana Black already registered with postscriptFonts().
Noto Sans Thaana already registered with postscriptFonts().
Noto Sans Thaana Light already registered with postscriptFonts().
Noto Sans Thaana Medium already registered with postscriptFonts().
Noto Sans Thaana Thin already registered with postscriptFonts().
Noto Sans Thai Blk already registered with postscriptFonts().
Noto Sans Thai already registered with postscriptFonts().
Noto Sans Thai Light already registered with postscriptFonts().
Noto Sans Thai Med already registered with postscriptFonts().
Noto Sans Thai Thin already registered with postscriptFonts().
Noto Sans Thai UI Blk already registered with postscriptFonts().
Noto Sans Thai UI already registered with postscriptFonts().
Noto Sans Thai UI Light already registered with postscriptFonts().
Noto Sans Thai UI Med already registered with postscriptFonts().
Noto Sans Thai UI Thin already registered with postscriptFonts().
Noto Sans Tibetan already registered with postscriptFonts().
Noto Sans Tifinagh already registered with postscriptFonts().
Noto Sans Tirhuta already registered with postscriptFonts().
Noto Sans Ugaritic already registered with postscriptFonts().
Noto Sans Vai already registered with postscriptFonts().
Noto Sans WarangCiti already registered with postscriptFonts().
Noto Sans Yi already registered with postscriptFonts().
Noto Serif Black already registered with postscriptFonts().
Noto Serif already registered with postscriptFonts().
Noto Serif Light already registered with postscriptFonts().
Noto Serif Medium already registered with postscriptFonts().
Noto Serif Thin already registered with postscriptFonts().
Noto Serif Ahom already registered with postscriptFonts().
Noto Serif Armenian Bk already registered with postscriptFonts().
Noto Serif Armenian already registered with postscriptFonts().
Noto Serif Armenian Lt already registered with postscriptFonts().
Noto Serif Armenian Md already registered with postscriptFonts().
Noto Serif Armenian Th already registered with postscriptFonts().
Noto Serif Balinese already registered with postscriptFonts().
Noto Serif Bengali Black already registered with postscriptFonts().
Noto Serif Bengali already registered with postscriptFonts().
Noto Serif Bengali Light already registered with postscriptFonts().
Noto Serif Bengali Medium already registered with postscriptFonts().
Noto Serif Bengali Thin already registered with postscriptFonts().
Noto Serif Devanagari Black already registered with postscriptFonts().
Noto Serif Devanagari already registered with postscriptFonts().
Noto Serif Devanagari Light already registered with postscriptFonts().
Noto Serif Devanagari Medium already registered with postscriptFonts().
Noto Serif Devanagari Thin already registered with postscriptFonts().
Noto Serif Display Black already registered with postscriptFonts().
Noto Serif Display already registered with postscriptFonts().
Noto Serif Display Light already registered with postscriptFonts().
Noto Serif Display Medium already registered with postscriptFonts().
Noto Serif Display Thin already registered with postscriptFonts().
Noto Serif Dogra already registered with postscriptFonts().
Noto Serif Ethiopic Bk already registered with postscriptFonts().
Noto Serif Ethiopic already registered with postscriptFonts().
Noto Serif Ethiopic Lt already registered with postscriptFonts().
Noto Serif Ethiopic Md already registered with postscriptFonts().
Noto Serif Ethiopic Th already registered with postscriptFonts().
Noto Serif Georgian Bk already registered with postscriptFonts().
Noto Serif Georgian already registered with postscriptFonts().
Noto Serif Georgian Lt already registered with postscriptFonts().
Noto Serif Georgian Md already registered with postscriptFonts().
Noto Serif Georgian Th already registered with postscriptFonts().
Noto Serif Gujarati Black already registered with postscriptFonts().
Noto Serif Gujarati already registered with postscriptFonts().
Noto Serif Gujarati Light already registered with postscriptFonts().
Noto Serif Gujarati Medium already registered with postscriptFonts().
Noto Serif Gujarati Thin already registered with postscriptFonts().
Noto Serif Gurmukhi Black already registered with postscriptFonts().
Noto Serif Gurmukhi already registered with postscriptFonts().
Noto Serif Gurmukhi Light already registered with postscriptFonts().
Noto Serif Gurmukhi Medium already registered with postscriptFonts().
Noto Serif Gurmukhi Thin already registered with postscriptFonts().
Noto Serif Hebrew Blk already registered with postscriptFonts().
Noto Serif Hebrew already registered with postscriptFonts().
Noto Serif Hebrew Light already registered with postscriptFonts().
Noto Serif Hebrew Med already registered with postscriptFonts().
Noto Serif Hebrew Thin already registered with postscriptFonts().
Noto Serif Kannada Black already registered with postscriptFonts().
Noto Serif Kannada already registered with postscriptFonts().
Noto Serif Kannada Light already registered with postscriptFonts().
Noto Serif Kannada Medium already registered with postscriptFonts().
Noto Serif Kannada Thin already registered with postscriptFonts().
Noto Serif Khmer Black already registered with postscriptFonts().
Noto Serif Khmer already registered with postscriptFonts().
Noto Serif Khmer Light already registered with postscriptFonts().
Noto Serif Khmer Medium already registered with postscriptFonts().
Noto Serif Khmer Thin already registered with postscriptFonts().
Noto Serif Lao Blk already registered with postscriptFonts().
Noto Serif Lao already registered with postscriptFonts().
Noto Serif Lao Light already registered with postscriptFonts().
Noto Serif Lao Med already registered with postscriptFonts().
Noto Serif Lao Thin already registered with postscriptFonts().
Noto Serif Malayalam Black already registered with postscriptFonts().
Noto Serif Malayalam already registered with postscriptFonts().
Noto Serif Malayalam Light already registered with postscriptFonts().
Noto Serif Malayalam Medium already registered with postscriptFonts().
Noto Serif Malayalam Thin already registered with postscriptFonts().
Noto Serif Myanmar Blk already registered with postscriptFonts().
Noto Serif Myanmar already registered with postscriptFonts().
Noto Serif Myanmar Light already registered with postscriptFonts().
Noto Serif Myanmar Med already registered with postscriptFonts().
Noto Serif Myanmar Thin already registered with postscriptFonts().
Noto Serif Sinhala Black already registered with postscriptFonts().
Noto Serif Sinhala already registered with postscriptFonts().
Noto Serif Sinhala Light already registered with postscriptFonts().
Noto Serif Sinhala Medium already registered with postscriptFonts().
Noto Serif Sinhala Thin already registered with postscriptFonts().
Noto Serif Tamil Blk already registered with postscriptFonts().
Noto Serif Tamil already registered with postscriptFonts().
Noto Serif Tamil Light already registered with postscriptFonts().
Noto Serif Tamil Med already registered with postscriptFonts().
Noto Serif Tamil Thin already registered with postscriptFonts().
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Black. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Light. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Medium. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Thin. Skipping setup for this font.
Noto Serif Tangut already registered with postscriptFonts().
Noto Serif Telugu Black already registered with postscriptFonts().
Noto Serif Telugu already registered with postscriptFonts().
Noto Serif Telugu Light already registered with postscriptFonts().
Noto Serif Telugu Medium already registered with postscriptFonts().
Noto Serif Telugu Thin already registered with postscriptFonts().
Noto Serif Thai Blk already registered with postscriptFonts().
Noto Serif Thai already registered with postscriptFonts().
Noto Serif Thai Light already registered with postscriptFonts().
Noto Serif Thai Med already registered with postscriptFonts().
Noto Serif Thai Thin already registered with postscriptFonts().
Noto Serif Tibetan Black already registered with postscriptFonts().
Noto Serif Tibetan already registered with postscriptFonts().
Noto Serif Tibetan Light already registered with postscriptFonts().
Noto Serif Tibetan Medium already registered with postscriptFonts().
Noto Serif Tibetan Thin already registered with postscriptFonts().
NovaMono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Nunito. Skipping setup for this font.
orya already registered with postscriptFonts().
More than one version of regular/bold/italic found for padmaa. Skipping setup for this font.
Pothana2000 already registered with postscriptFonts().
ProFont for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Roboto. Skipping setup for this font.
More than one version of regular/bold/italic found for Roboto Condensed. Skipping setup for this font.
Roboto Mono for Powerline already registered with postscriptFonts().
Roboto Mono Light for Powerline already registered with postscriptFonts().
Roboto Mono Medium for Powerline already registered with postscriptFonts().
Roboto Mono Thin for Powerline already registered with postscriptFonts().
Sagar already registered with postscriptFonts().
Space Mono already registered with postscriptFonts().
Space Mono for Powerline already registered with postscriptFonts().
Symbol Neu for Powerline already registered with postscriptFonts().
TAMu_Kadambri already registered with postscriptFonts().
TAMu_Kalyani already registered with postscriptFonts().
TAMu_Maduram already registered with postscriptFonts().
Tinos for Powerline already registered with postscriptFonts().
TSCu_Comic already registered with postscriptFonts().
TSCu_Paranar already registered with postscriptFonts().
TSCu_Times already registered with postscriptFonts().
Ubuntu already registered with postscriptFonts().
Ubuntu Light already registered with postscriptFonts().
Ubuntu Condensed already registered with postscriptFonts().
Ubuntu Mono already registered with postscriptFonts().
Ubuntu Mono derivative Powerline already registered with postscriptFonts().
#+end_example

****** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/blocking_unrolling.pdf" :width 17.4 :height 6 :eval no-export
library(ggplot2)
library(dplyr)
library(tidyr)
library(paletteer)
library(patchwork)

font_family = "Liberation Sans"
base_size = 20

x_lab = "Columns"
y_lab = "Rows"

x_text = element_text(size = 20)
y_text = element_text(size = 20)

regular <- function(block_size) {
    matrix_size <- block_size * 4
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size)){
        for(j in seq(1, matrix_size)){
            df_matrix[i, j] <- iteration
            iteration <- iteration + 1
        }
    }

    return(df_matrix)
}

tiled <- function(block_size) {
    matrix_size <- block_size * 4
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size, by = block_size)){
        for(j in seq(1, matrix_size, by = block_size)){
            for(x in seq(i, min(i + block_size, matrix_size + 1) - 1)){
                for(y in seq(j, min(j + block_size, matrix_size + 1) - 1)){
                    df_matrix[x, y] <- iteration
                    iteration <- iteration + 1
                }
            }
        }
    }

    return(df_matrix)
}

unrolled <- function(block_size) {
    matrix_size <- block_size * 4
    unrolling_factor <- block_size
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size, by = block_size)){
        for(j in seq(1, matrix_size, by = block_size)){
            df_matrix[seq(i, min(i + block_size, matrix_size + 1) - 1),
                      seq(j, min(j + block_size, matrix_size + 1) - 1)] <- iteration
            iteration <- iteration + 1
        }
    }

    return(df_matrix)
}

to_tile <- function(df) {
    df %>%
        data.frame() %>%
        mutate(row = row_number()) %>%
        gather(key = "col", value = "order", -row) %>%
        mutate(col = unlist(
                   lapply(col,
                          function(x) {
                              as.integer(strsplit(x, "X")[[1]][2])
                          })))
}

regular_access <- to_tile(regular(4))
tiled_access <- to_tile(tiled(4))
unrolled_access <- to_tile(unrolled(4))

t_regular_access <- to_tile(t(regular(4)))
t_tiled_access <- to_tile(t(tiled(4)))
t_unrolled_access <- to_tile(t(unrolled(4)))

cf_palette <- "pals::jet"

p1 <- ggplot() +
    geom_tile(data = regular_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order),
              show.legend = FALSE) +
    scale_fill_paletteer_c(cf_palette) +
    scale_color_paletteer_c(cf_palette) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(a) Regular Access Pattern") +
    theme_bw(base_size = base_size) +
    theme(axis.title.x = x_text,
          axis.title.y = y_text,
          text = element_text(family = font_family),
          legend.position = "bottom",
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 16),
          legend.title = element_text(size = 23, margin = margin(r = 10)),
          legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

p2 <- ggplot() +
    geom_tile(data = tiled_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order),
              show.legend = FALSE) +
    scale_fill_paletteer_c(cf_palette) +
    scale_color_paletteer_c(cf_palette) +
    #scale_color_paletteer_c(name = "Access Order",
    #                        cf_palette,
    #                        guide = guide_colorbar(title.vjust = 1.1,
    #                                               title.hjust = 1,
    #                                               label.position = "bottom"),
    #                        breaks = c(1, 256),
    #                        labels = c("First", "Last")) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(b) After Tiling") +
    theme_bw(base_size = base_size) +
    theme(axis.title.x = x_text,
          axis.title.y = y_text,
          text = element_text(family = font_family),
          legend.position = "bottom",
          legend.direction = "horizontal",
          # legend.background = element_rect(fill = "transparent", colour = NA),
          # legend.text = element_text(size = 16),
          legend.title = element_text(margin = margin(r = 40)),
          # legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

p3 <- ggplot() +
    geom_tile(data = tiled_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order)) +
    scale_color_paletteer_c(cf_palette) +
    scale_fill_paletteer_c(name = "Order",
                           cf_palette,
                           guide = guide_colorbar(barwidth = 0.7,
                                                  barheight = 20,
                                                  direction = "vertical",
                                                  label.hjust = 1,
                                                  ticks = FALSE,
                                                  reverse = FALSE),
                           limits = c(1, 256),
                           breaks = c(1, 256),
                           labels = c("Last", "First")) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(c) After Tiling and Unrolling") +
    theme_bw(base_size = base_size) +
    theme(text = element_text(family = font_family),
          axis.title.x = x_text,
          axis.title.y = y_text,
          legend.position = "right",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 20),
          legend.title = element_text(size = 23, margin = margin(b = 15)),
          # legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    guides(color = FALSE)

p1 + p2 + p3
#+end_SRC

#+RESULTS:
[[file:./img/blocking_unrolling.pdf]]



** Optimization Methods (Part II)
*** Function Minimization Methods
**** Introduction
***** (Old) Complete Classification Tree for Function Minimization Methods
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" )
#+HEADER: :exports results :results raw :file ./img/old_tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    l sep+=1em
  },
  [{Minimize $f: \mathcal{X} \to \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
      draw,
      color = NavyBlue
      [{Search Heuristics},
        draw,
        color = BurntOrange,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{\textbf{Random} \textbf{Sampling}}, draw]
        [{Reachable Optima},
          draw,
          color = BurntOrange
          [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
            draw,
            color = BurntOrange
            [{Strong $corr(f(X),d(X,X_{*}))$?},
              draw,
              color = NavyBlue
              [{More Global},
                draw,
                color = BurntOrange,
                edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                [, phantom]]
              [{More Local},
                draw,
                color = BurntOrange,
                edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                [, phantom]
                [{High local optima density?},
                  draw,
                  color = NavyBlue
                  [{Exploit Steepest Descent},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                    [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                  [{Allows\\exploration},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                    [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
            [,phantom]]
          [,phantom]]]
      [{Statistical Learning},
        draw,
        color = BurntOrange,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Parametric Learning},
          draw,
          color = BurntOrange
          [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
            draw,
            color = BurntOrange
            [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
            [, phantom]]
          [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
            draw,
            color = BurntOrange
            [, phantom]
            [{Check for model adequacy?},
              draw,
              alias = adequacy,
              color = NavyBlue
              [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                draw,
                alias = interactions,
                color = NavyBlue,
                edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                  edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                  draw
                  [, phantom]
                  [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                    edge = {-stealth, ForestGreen, semithick},
                    edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                    draw,
                    alias = estimate,
                    color = ForestGreen]]
                [{\textbf{Optimal} \textbf{Design}},
                  draw,
                  alias = optimal,
                  edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [{\textbf{Space-filling} \textbf{Designs}},
                draw,
                edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                [, phantom]
                [{Model selection},
                  edge = {-stealth, ForestGreen, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                  draw,
                  alias = selection,
                  color = ForestGreen]]]]]
        [{Nonparametric Learning},
          draw,
          color = BurntOrange
          [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
            draw
            [, phantom]
            [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
              edge = {-stealth, ForestGreen, semithick},
              draw,
              alias = uncertainty,
              color = ForestGreen
              [{Minimize $uncertainty(\hat{f}(X))$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                draw,
                color = ForestGreen]
              [{Minimize $\hat{f}(X)$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                draw,
                color = ForestGreen]
              [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                draw,
                color = ForestGreen]]]
          [{\textbf{Gaussian} \textbf{Process Regression}},
            alias = gaussian,
            draw]
          [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
  \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
  \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
  \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
  \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/old_tree.pdf]]

***** Complete Classification Tree for Function Minimization Methods
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" "\\usepackage{amsmath}" "\\DeclareMathOperator*{\\argmin}{arg\\,min}" )
#+HEADER: :exports results :results raw :file ./img/tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    l sep+=1.4em
  },
  [{Minimize $f: \mathcal{X} \to \mathbb{R}$,\\$Y = f(X = [x_1,\dots,x_k]^{\top} \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Is $f(X)$ known,\\or cheap to measure?},
      draw
      [{\textit{Search Heuristics}\\Leverage measurements\\to minimize $f(X)$},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Can $\nabla{}f$ and $\mathbf{H}f$ be computed?},
          draw
          [{Stochastic\\Descent},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{Population\\based\\\textbf{Genetic}\\\textbf{Algorithms}\\$\dots$}, draw]
            [{Direct\\\textbf{Nelder-Mead}\\$\dots$}, draw]
            [{Single-state\\\textbf{Simulated}\\\textbf{Annealing}\\$\dots$}, draw]]
          [{\textbf{Gradient}\\\textbf{Descent}\\$\dots$},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]]
      [{\textit{Statistical Learning}\\Construct a surrogate\\$\hat{f}(\cdot, \theta(X))$, estimate $\theta(X)$},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{Is $|\theta(X)|$ finite?},
          draw
          [{Parametric Models},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
              draw
              [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]]
            [{\textbf{Design of Experiments}\\using linear models\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
              draw
              [{Find \textit{significant} $x_i \in X$,\\or check \textit{lack of fit} of $\hat{f}$?},
                draw,
                alias = adequacy
                [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                  draw,
                  alias = interactions,
                  edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                  [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                    draw
                    [, phantom]
                    [{Pick $\hat{X}_{*} = \argmin_{X \in \mathcal{X}}\hat{f}$,\\reduce dimension of $\mathcal{X}$},
                      edge = {-stealth, Black, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      alias = estimate,
                      color = Black]]
                  [{\textbf{Optimal}\\\textbf{Design}},
                    draw,
                    alias = optimal,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                [, phantom]
                [, phantom]
                [, phantom]
                [, phantom]
                [, phantom]
                [, phantom]
                [{\textbf{Space-filling}\\\textbf{Designs}},
                  draw,
                  edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                  [{Model selection},
                    edge = {-stealth, Black, semithick},
                    edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                    draw,
                    alias = selection,
                    color = Black]]]]]
          [{Nonparametric Models},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
              draw
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [{Estimate $\hat{f}(\cdot)$ and\\$uncertainty(\hat{f}(\cdot))$},
                edge = {-stealth, Black, semithick},
                draw,
                alias = uncertainty,
                color = Black
                [{Minimize\\$uncertainty(\hat{f}(X))$},
                  edge = {Black, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                  draw,
                  color = Black]
                [{Minimize $\hat{f}(X)$},
                  edge = {Black, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                  draw,
                  color = Black]
                [{Minimize \textit{EI}, or\\$\hat{f}(X) - uncertainty(\hat{f}(X))$},
                  edge = {Black, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                  draw,
                  color = Black]]]
            [{\textbf{Gaussian}\\\textbf{Process Regression}},
              alias = gaussian,
              draw]]]]]]
  \draw [-stealth, semithick, Black](selection) to [bend left=22] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
  \draw [-stealth, semithick, Black](estimate.east) to [bend right=32] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
  \draw [-stealth, semithick, Black](gaussian) to (uncertainty);
  \draw [-stealth, semithick, Black](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/tree.pdf]]

***** Simplest Version
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" )
#+HEADER: :exports results :results raw :file ./img/simplest_tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    if n children=0{tier=terminal}{},
    l sep+=1em
  },
  [{Minimize $f: \mathcal{X} \to \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$?},
      draw,
      [{Search\\Heuristics},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{\textbf{Random}\\\textbf{Sampling}}, draw]
        [{Reachable\\Optima},
          draw,
          [{Introduce a\\\textit{population} of $X$\\[0.5em]\textbf{Genetic}\\\textbf{Algorithms}}, draw]
          [{Estimate $f^{\prime}(X)$\\[0.5em]\textbf{Gradient}\\\textbf{Descent}}, draw]
          [{Allow worse $f(X)$\\[0.5em]\textbf{Simulated}\\\textbf{Annealing}}, draw]]]
      [{Statistical\\Learning},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Parametric\\Learning},
          draw,
          [{\textbf{Independent}\\\textbf{Bandits}}, draw]
          [{Choose best $X$ for\\
              a Linear Model\\
              $\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$\\[0.5em]
              \textbf{Experimental}\\\textbf{Design}},
              draw]
          [{Unsupervised\\[0.5em]\textbf{Clustering}}, draw]]
        [{Nonparametric\\Learning},
          draw,
          [{\textbf{Decision}\\\textbf{Trees}},
            draw]
          [{\textbf{Gaussian}\\\textbf{Process}\\\textbf{Regression}},
            draw]
          [{\textbf{Neural}\\\textbf{Networks}}, draw]]]]]
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/simplest_tree.pdf]]

***** Representing Sampling Strategies
****** Generate Fake Data with Algorithms
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
fake_gradient_df_seed <- data.frame(x1 = c(1, 1, 99, 99),
                                    x2 = c(1, 99, 1, 99),
                                    run = c(1, 2, 3, 4),
                                    sign1 = c(1, 1, -1, -1),
                                    sign2 = c(1, -1, 1, -1))

fake_gradient_df <- NULL

for(run_id in c(1, 2, 3, 4)) {
    if (is.null(fake_gradient_df)) {
        fake_gradient_df <- fake_gradient_df_seed[run_id, ]
    } else {
        fake_gradient_df <- rbind(fake_gradient_df, fake_gradient_df_seed[run_id, ])
    }

    for(i in 1:10) {
        row <- nrow(fake_gradient_df)
        fake_descent <- data.frame(x1 = ceiling(fake_gradient_df[row, "x1"] + (fake_gradient_df[row, "sign1"] * runif(1, min = 1, max = 5))),
                                   x2 = ceiling(fake_gradient_df[row, "x2"] + (fake_gradient_df[row, "sign2"] * runif(1, min = 1, max = 5))),
                                   run = fake_gradient_df[row, "run"],
                                   sign1 = fake_gradient_df[row, "sign1"],
                                   sign2 = fake_gradient_df[row, "sign2"])
        fake_gradient_df <- rbind(fake_gradient_df, fake_descent)
    }
}

fake_gradient_df$name <- rep("Gradient Descent", nrow(fake_gradient_df))
df <- bind_rows(df, fake_gradient_df)

fake_sima_df_seed <- data.frame(x1 = c(30, 30, 70, 70),
                                x2 = c(30, 70, 30, 70),
                                run = c(1, 2, 3, 4),
                                sign1 = c(1, 1, -1, -1),
                                sign2 = c(1, -1, 1, -1))

fake_sima_df <- NULL

for(run_id in c(1, 2, 3, 4)) {
    if (is.null(fake_sima_df)) {
        fake_sima_df <- fake_sima_df_seed[run_id, ]
    } else {
        fake_sima_df <- rbind(fake_sima_df, fake_sima_df_seed[run_id, ])
    }

    for(i in 1:10) {
        row <- nrow(fake_sima_df)
        fake_descent <- data.frame(x1 = ceiling(fake_sima_df[row, "x1"] + (fake_sima_df[row, "sign1"] * runif(1, min = -5, max = 5))),
                                   x2 = ceiling(fake_sima_df[row, "x2"] + (fake_sima_df[row, "sign2"] * runif(1, min = -5, max = 5))),
                                   run = fake_sima_df[row, "run"],
                                   sign1 = fake_sima_df[row, "sign1"],
                                   sign2 = fake_sima_df[row, "sign2"])
        fake_sima_df <- rbind(fake_sima_df, fake_descent)
    }
}

fake_sima_df$name <- rep("Simulated Annealing", nrow(fake_sima_df))
df <- bind_rows(df, fake_sima_df)
#+END_SRC

#+RESULTS:

****** Generate Data
#+HEADER: :results output :session *R* :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)
library(RColorBrewer)

sample_size <- 50
pre_sample_size <- 30 * sample_size
search_space_size <- 100

center_x1 <- (search_space_size / 2) - 30
center_x2 <- (search_space_size / 2) - 30

get_cost <- function(df) {
    return(((df$x1 - center_x1) ^ 2) + ((df$x2 - center_x2) ^ 2) + ((abs((df$x1 - center_x1) * (df$x2 - center_x2)))**.7 * sin((df$x1 - center_x1) * (df$x2 - center_x2))))
}

objective_df <- expand.grid(seq(0, search_space_size, 1),
                            seq(0, search_space_size, 1))
names(objective_df) <- c("x1", "x2")

objective_df$Y <- get_cost(objective_df)

sima_samples <- 15

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_sima_df <- as.data.frame(locator(n = sima_samples, type = "l"))
names(fake_sima_df) <- c("x1", "x2")
dev.off()

fake_sima_df$run <- c(rep(1, nrow(fake_sima_df)))
fake_sima_df$name <- rep("Simulated Annealing", nrow(fake_sima_df))

fake_sima_df$cost <- get_cost(fake_sima_df)
fake_sima_df$min <- fake_sima_df$cost == min(fake_sima_df$cost)

df <- fake_sima_df

descent_samples <- 20

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_descent_df <- as.data.frame(locator(n = descent_samples, type = "l"))
names(fake_descent_df) <- c("x1", "x2")
dev.off()

paths <- 5
fake_runs <- rep(1, descent_samples / paths)
for(i in 2:paths){
    fake_runs <- c(fake_runs, rep(i, descent_samples / paths))
}

fake_descent_df$run <- fake_runs
fake_descent_df$name <- rep("Gradient Descent", nrow(fake_descent_df))

fake_descent_df$cost <- get_cost(fake_descent_df)
fake_descent_df$min <- fake_descent_df$cost == min(fake_descent_df$cost)

df <- bind_rows(df, fake_descent_df)

rs_df <- data.frame(x1 = sample(0:search_space_size, sample_size, replace = T),
                    x2 = sample(0:search_space_size, sample_size, replace = T))
rs_df$name <- rep("Random Sampling", nrow(rs_df))

rs_df$cost <- get_cost(rs_df)
rs_df$min <- rs_df$cost == min(rs_df$cost)

df <- bind_rows(df, rs_df)

lhs_df <- lhs.design(nruns = sample_size, nfactors = 2, digits = 0, type = "maximin",
                     factor.names = list(x1 = c(0, search_space_size), x2 = c(0, search_space_size)))
lhs_df$name <- rep("Latin Hypercube Sampling", nrow(lhs_df))

lhs_df$cost <- get_cost(lhs_df)
lhs_df$min <- lhs_df$cost == min(lhs_df$cost)

df <- bind_rows(df, lhs_df)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2, full_factorial, nTrials = sample_size)
dopt_df <- output$design

dopt_df$name <- rep("DOpt. Linear Model", nrow(dopt_df))
dopt_df$cost <- get_cost(dopt_df)
dopt_df$min <- rep(FALSE, nrow(dopt_df))

regression <- lm(cost ~ x1 + x2, data = dopt_df)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Linear Model"
best$min <- TRUE

dopt_df <- bind_rows(dopt_df, best)
df <- bind_rows(df, dopt_df)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
doptq_df <- output$design

doptq_df$name <- rep("DOpt. Quadratic Model", nrow(doptq_df))
doptq_df$cost <- get_cost(doptq_df)
doptq_df$min <- rep(FALSE, nrow(doptq_df))

regression <- lm(cost ~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), data = doptq_df)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Quadratic Model"
best$min <- TRUE

doptq_df <- bind_rows(doptq_df, best)
df <- bind_rows(df, doptq_df)
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: FrF2
Loading required package: DoE.base
Loading required package: grid
Loading required package: conf.design
Registered S3 method overwritten by 'DoE.base':
  method           from
  factorize.factor conf.design

Attaching package: ‘DoE.base’

The following objects are masked from ‘package:stats’:

    aov, lm

The following object is masked from ‘package:graphics’:

    plot.design

The following object is masked from ‘package:base’:

    lengths

Loading required package: rsm
null device
          1
null device
          1
#+end_example

****** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file ./img/sampling_comparison.pdf :exports none :width 15 :height 11.5 :eval no-export
#+BEGIN_SRC R
library(extrafont)
df$facet <- factor(df$name, levels = c("Random Sampling",
                                           "Latin Hypercube Sampling",
                                           "Gradient Descent",
                                           "Simulated Annealing",
                                           "DOpt. Linear Model",
                                           "DOpt. Quadratic Model"))
ggplot(df, aes(x = x1, y = x2)) +
    scale_x_continuous(limits = c(-1, 101),
                       expand = c(0, 0)) +
    scale_y_continuous(limits = c(-1, 101),
                       expand = c(0, 0)) +
    xlab("x") +
    ylab("y") +
    facet_wrap(facet ~ .,
               ncol = 3) +
    #geom_raster(data = objective_df, aes(fill = Y), show.legend = FALSE) +
    #geom_contour(data = objective_df, aes(z = Y), colour = "white", linetype = 8) + #, breaks = 1 * (2 ^ (2:20))) +
    geom_contour(data = objective_df,
                 aes(z = Y),
                 linetype = 1,
                 colour = "black",
                 alpha = 0.6,
                 show.legend = FALSE,
                 breaks = 1 * (2 ^ (4:20))) +
    geom_path(data = subset(df,
                            name %in% c("Gradient Descent", "Simulated Annealing")),
              aes(group = run),
              color = "black",
              alpha = 0.55,
              size = 1) +
    geom_point(shape = 19,
               size = 3,
               colour = "black",
               alpha = 0.55) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.55,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.55,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.45,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.45,
                width = 8,
                height = 8) +
    scale_fill_distiller(palette = "Greys",
                         direction = -1,
                         limits = c(min(objective_df$Y) - 1000,
                                    max(objective_df$Y))) +
    geom_point(data = subset(df,
                             min == TRUE),
               color = "red",
               shape = 3,
               size = 12,
               alpha = 1,
               stroke = 2) +
    theme_bw(base_size = 35) +
    theme(panel.grid = element_blank(),
          text = element_text(family="serif"),
          strip.background = element_rect(fill = "white"),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/sampling_comparison.pdf]]

***** Search Space Introduction
****** Simple Search Spaces
******* Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
a = 0.6
c = -30

l = 3
k = -6
#+end_SRC

#+RESULTS:

******* Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

resolution = 100

new_grid = expand.grid(seq(from = -10.0, to = 10.0, length.out = resolution),
                       seq(from = -10.0, to = 10.0, length.out = resolution))

df = data.frame(x = new_grid$Var1,
                y = new_grid$Var2)

amp_x = 1
amp_y = 1.5

#noise_sd = 0.033
noise_sd = 20

# f <- function(x, y){
#     return((x ^ 2) * y)
# }

# f <- function(x, y){
#     return(((x ^ 2) * amp_x) + ((y ^ 2) * amp_y))
# }

f <- function(y, x){
    return(((x + (2 * y) - 7) ^ 2) + (((2 * x) + y - 5)^ 2))
}


df$z = f(df$x, df$y) +
    rnorm(resolution ^ 2,
          mean = 0.0,
          sd = noise_sd)

df$type = "noise"

plot_df = df

df$z = f(df$x, df$y)

df$type = "no_noise"

plot_df = bind_rows(plot_df, df)

df$z = f(df$x, df$y) +
    rnorm(resolution ^ 2,
          mean = 0.0,
          sd = noise_sd)

df$type = "constrained"

constraint = data.frame(x_min = 0.1,
                        x_max = 0.4,
                        y_min = -0.3,
                        y_max = 0.4)

quad_constraint <- function(x1, x2, a, c){
    return(x2 < ((a * (x1 ^ 2)) + c))
}

lin_constraint <- function(x1, x2, l, k){
    return(x1 > l | x1 < k)
}

#df[quad_constraint(df$y, df$x, a, c), "z"] = NA
df[lin_constraint(df$y, df$x, l, k), "z"] = NA

plot_df = bind_rows(plot_df, df)

write.csv(plot_df,
          "data/search_spaces/simple_search_space.csv",
          row.names = FALSE)

#+end_SRC

#+RESULTS:

******* Search Spaces Overview
j#+begin_SRC R :results graphics output :session *R* :file "./img/simple_search_space.pdf" :width 30 :height 21 :eval no-export
library(dplyr)
library(ggplot2)
library(paletteer)
library(patchwork)
library(latex2exp)

plot_df = read.csv("data/search_spaces/simple_search_space.csv",
                   header = TRUE)

color_palette = "viridis::viridis"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 10
optimum_stroke = 3
optima_shape = 4

noise_sd = 0.033

constraint_label = "Not feasible"

constraint = data.frame(x_min = 0.1,
                        x_max = 0.4,
                        y_min = -0.3,
                        y_max = 0.4)

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

theme_config_line = theme(axis.title.y = element_text(angle = 0,
                                                      margin = margin(t = 0,
                                                                      b = 0,
                                                                      r = 0.7,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      vjust = 0.5,
                                                      size = title_size),
                          axis.title.x = element_text(margin = margin(t = 0.4,
                                                                      b = -0.7,
                                                                      r = 0,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      size = title_size),
                          axis.ticks = element_blank(),
                          axis.text.x = element_blank(),
                          axis.text.y = element_blank())

p_no_noise = ggplot() +
    scale_x_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    scale_y_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    geom_contour(data = filter(plot_df,
                               type == "no_noise"),
                 aes(x = x,
                     y = y,
                     z = z,
                     color = ..level..),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "no_noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    scale_y_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    geom_contour(data = filter(plot_df, type == "noise"),
                 aes(x = x,
                     y = y,
                     z = z,
                     color = ..level..),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    scale_y_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    geom_contour(data = filter(plot_df,
                               type == "constrained"),
                 aes(x = x,
                     y = y,
                     z = z,
                     color = ..level..),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(type == "constrained") %>%
                    filter(is.na(z)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(type == "constrained") %>%
                    filter(is.na(z)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(type == "constrained") %>%
                   filter(!is.na(z)) %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    # geom_text(aes(x = 0.6,
    #               y = 0.83),
    #           size = 12,
    #           color = "gray50",
    #           angle = 0,
    #           label = constraint_label) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "no_noise") %>%
                  filter(y == plot_df %>%
                         filter(type == "no_noise") %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = line_thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "no_noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(d)}")) +
    ylab(TeX("$y$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_noise_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "noise") %>%
                  filter(y == plot_df %>%
                         filter(type == "noise") %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = line_thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(e)}")) +
    ylab(TeX("$y$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_constrained_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "constrained") %>%
                  filter(y == plot_df %>%
                         filter(type == "constrained") %>%
                         filter(!is.na(z)) %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = line_thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "constrained") %>%
                   filter(!is.na(z)) %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    # geom_vline(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #            aes(xintercept = max(x)),
    #            linetype = 2,
    #            color = constraint_border,
    #            size = 1.3) +
    # geom_ribbon(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #             aes(y = z,
    #                 xmax = max(x),
    #                 xmin = 10),
    #             alpha = 0.1) +
    # geom_text(aes(x = (constraint$x_max - constraint$x_min),
    #               y = 0.455),
    #           size = 12,
    #           hjust = -0.64,
    #           vjust = 1.1,
    #           color = "gray58",
    #           angle = -90,
    #           label = constraint_label) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(f)}")) +
    ylab(TeX("$y$")) +
    theme_bw(base_size = base_size) +
    theme_config_line

(p_no_noise | p_noise | p_constrained) /
    (p_no_noise_line | p_noise_line | p_constrained_line)
#+end_SRC

#+RESULTS:
[[file:./img/simple_search_space.pdf]]

****** Local, global optimum, convexity
******* Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/optima_convexity.svg" :width 20 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(paletteer)
library(patchwork)
library(latex2exp)

plot_df = read.csv("data/search_spaces/simple_search_space.csv",
                   header = TRUE)

color_palette = "viridis::viridis"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.5
base_size = 32
title_size = 39
optimum_size = 10
optimum_stroke = 3

optima_shape = 4
constraint_label = "Not feasible"

constraint = data.frame(x_min = 0.1,
                        x_max = 0.4,
                        y_min = -0.3,
                        y_max = 0.4)

convex_line = plot_df %>%
    filter(y == plot_df %>%
           filter(type == "no_noise") %>%
           filter(z == min(z)) %>%
           select(y) %>% first()) %>%
    filter(type == "no_noise") %>%
    mutate(n = row_number()) %>%
    filter(n == round(0.08 * max(n)) |
           n == round(0.88 * max(n)))

convex_line_model = lm(z ~ x, convex_line)

convex_line_prediction = plot_df %>%
    filter(y == plot_df %>%
           filter(type == "no_noise") %>%
           filter(z == min(z)) %>%
           select(y) %>%
           first()) %>%
    filter(type == "no_noise")

convex_line_prediction$pred = predict(convex_line_model, convex_line_prediction)

theme_config_line = theme(axis.title.y = element_text(angle = 0,
                                                      margin = margin(t = 0,
                                                                      b = 0,
                                                                      r = 0.7,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      vjust = 0.5,
                                                      size = title_size),
                          axis.title.x = element_text(margin = margin(t = 0.4,
                                                                      b = -0.7,
                                                                      r = 0,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      size = title_size),
                          axis.ticks = element_blank(),
                          axis.text.x = element_blank(),
                          axis.text.y = element_blank())

constrained_local_optima = data.frame(x = c(-0.31, -0.47, -0.542),
                                      y = c(0.14, 0.24, 0.34),
                                      x_b = c(-0.25, -0.25, -0.25),
                                      y_b = c(0.5, 0.5, 0.5))

local_optima_curvature = 0.5

p_no_noise_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "no_noise") %>%
                  filter(y == plot_df %>%
                         filter(type == "no_noise") %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "no_noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = optima_shape) +
    geom_line(data = convex_line_prediction,
              aes(x = x,
                  y = pred),
              color = constraint_border,
              linetype = 3,
              size = 1.9,
              show.legend = FALSE) +
    geom_line(data = convex_line,
              aes(x = x,
                  y = z),
              color = constraint_border,
              size = 2.3,
              show.legend = FALSE) +
    geom_point(data = convex_line,
               aes(x = x,
                   y = z,
                   color = z),
               size = 6,
               show.legend = FALSE) +
    # geom_curve(aes(x = 0.0,
    #                y = 0.1,
    #                xend = -0.01,
    #                yend = 0.01),
    #            angle = 90,
    #            curvature = 0.05,
    #            lineend = "round",
    #            linejoin = "round",
    #            size = 1.4,
    #            color = constraint_border,
    #            arrow = arrow(length = unit(0.3, "inches"))) +
    # geom_rect(aes(xmin = -0.25,
    #               xmax = 0.25,
    #               ymin = 0.1,
    #               ymax = 0.26),
    #           show.legend = FALSE,
    #           fill = "gray94",
    #           color = constraint_border,
    #           size = 0.4) +
    # geom_text(aes(x = 0., y = 0.18),
    #           size = 12,
    #           color = constraint_border,
    #           label = TeX("\\overset{Global}{Optimum}")) +
    # geom_rect(aes(xmin = -0.3,
    #               xmax = 0.3,
    #               ymin = 0.69,
    #               ymax = 0.85),
    #           show.legend = FALSE,
    #           fill = "gray94",
    #           color = constraint_border,
    #           size = 0.4) +
    # geom_text(aes(x = 0., y = 0.77),
    #           size = 12,
    #           color = constraint_border,
    #           label = TeX("\\overset{$y = f(\\cdot,\\, x_2)$}{is convex}")) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$y$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_constrained_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "constrained") %>%
                  filter(y == plot_df %>%
                         filter(type == "constrained") %>%
                         filter(!is.na(z)) %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    # geom_curve(data = constrained_local_optima,
    #            aes(x = x_b,
    #                y = y_b,
    #                xend = x,
    #                yend = y),
    #            angle = 90,
    #            curvature = 0.3,
    #            lineend = "round",
    #            linejoin = "round",
    #            size = 1.4,
    #            color = constraint_border,
    #            arrow = arrow(length = unit(0.3, "inches"))) +
    # geom_vline(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #            aes(xintercept = max(x)),
    #            linetype = 2,
    #            color = constraint_border,
    #            size = 1.3) +
    # geom_ribbon(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #             aes(y = z,
    #                 xmax = max(x),
    #                 xmin = 1),
    #             alpha = 0.1) +
    # geom_rect(aes(xmin = -0.22,
    #               xmax = 0.55,
    #               ymin = 0.45,
    #               ymax = 0.55),
    #           show.legend = FALSE,
    #           fill = "gray94",
    #           color = constraint_border,
    #           size = 0.4) +
    # geom_text(aes(x = 0.16, y = 0.5),
    #           size = 12,
    #           color = constraint_border,
    #           label = "Local Optima") +
    geom_point(data = plot_df %>%
                   filter(type == "constrained") %>%
                   filter(!is.na(z)) %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = optima_shape) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$y$")) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_no_noise_line | p_constrained_line
#+end_SRC

#+RESULTS:
[[file:./img/optima_convexity.svg]]
**** Descent Template for Booth's Function (With gradient)
***** Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
l = 3
k = -6
#+end_SRC

#+RESULTS:
***** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_descent_gradient_template.pdf" :width 30 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(pracma)
library(paletteer)
library(patchwork)
library(latex2exp)

color_palette = "viridis::viridis"
fill_palette = "ggthemes::Gray"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 8
optimum_stroke = 3
optima_shape = 4
contour_color = "gray65"
grad_alpha = 0.45

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

resolution = 100

noise_sd = 20

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)

grad_resolution = 6
grad_length = 0.83
picked = unique(plot_df$x)[seq(0, resolution, by = grad_resolution)]

scale_limits = c(-10.1, 10.1)

p_no_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    color = sqrt((x_grad ^ 2) + (y_grad ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_noisy),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                alpha = grad_alpha,
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked) %>%
                    filter(!is.na(z_constrained)),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_constrained),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(!is.na(z_constrained)) %>%
                   filter(z_constrained == min(z_constrained)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise | p_noise | p_constrained
#+end_SRC

#+RESULTS:
[[file:./img/booth_descent_gradient_template.pdf]]

**** Methods Based on Derivatives
***** Gradient of Booth's Function
****** Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
l = 3
k = -6
#+end_SRC

#+RESULTS:
****** Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

resolution = 100

new_grid = expand.grid(seq(from = -10.0, to = 10.0, length.out = resolution),
                       seq(from = -10.0, to = 10.0, length.out = resolution))

df = data.frame(x = new_grid$Var1,
                y = new_grid$Var2)

noise_sd = 20
grad_noise_sd = 30

f <- function(y, x){
    return(((x + (2 * y) - 7) ^ 2) + (((2 * x) + y - 5)^ 2))
}

f_grad <- function(x, y){
    return(data.frame(grad_x = (10 * x) + (8 * y) - 34,
                      grad_y = (8 * x) + (10 * y) - 38))
}

lin_constraint <- function(x1, x2, l, k){
    return(x1 > l | x1 < k)
}

df$z = f(df$x, df$y)

noise = rnorm(resolution ^ 2,
              mean = 0.0,
              sd = noise_sd)

grad_noise_x = rnorm(resolution ^ 2,
                     mean = 0.0,
                     sd = grad_noise_sd)
grad_noise_y = rnorm(resolution ^ 2,
                     mean = 0.0,
                     sd = grad_noise_sd)

df$z_noisy = f(df$x, df$y) + noise

df$z_constrained = df$z_noisy
df[lin_constraint(df$y, df$x, l, k), "z_constrained"] = NA

grad_f = f_grad(df$x, df$y)
grad_f_noisy = f_grad(df$x, df$y)

grad_f_noisy$grad_x = grad_f_noisy$grad_x + grad_noise_x
grad_f_noisy$grad_y = grad_f_noisy$grad_y + grad_noise_y

plot_df = df %>%
    mutate(x_grad = grad_f$grad_x,
           y_grad = grad_f$grad_y,
           x_grad_noisy = grad_f_noisy$grad_x,
           y_grad_noisy = grad_f_noisy$grad_y)

write.csv(plot_df,
          "data/search_spaces/gradient.csv",
          row.names = FALSE)
#+end_SRC

#+RESULTS:

****** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_gradient.pdf" :width 30 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(pracma)
library(paletteer)
library(patchwork)
library(latex2exp)

color_palette = "viridis::viridis"
fill_palette = "ggthemes::Green"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 8
optimum_stroke = 3
optima_shape = 4
contour_color = "gray65"

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

resolution = 100

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)

grad_resolution = 6
grad_length = 0.83
picked = unique(plot_df$x)[seq(0, resolution, by = grad_resolution)]

scale_limits = c(-10.1, 10.1)

p_no_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    color = sqrt((x_grad ^ 2) + (y_grad ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_noisy),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked) %>%
                    filter(!is.na(z_constrained)),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                show.legend = FALSE) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_constrained),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(!is.na(z_constrained)) %>%
                   filter(z_constrained == min(z_constrained)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise | p_noise | p_constrained
#+end_SRC

#+RESULTS:
[[file:./img/booth_gradient.pdf]]

***** Descent Template for Booth's Function (With gradient)
****** Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
l = 3
k = -6
#+end_SRC

#+RESULTS:
****** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_descent_gradient_template.pdf" :width 30 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(pracma)
library(paletteer)
library(patchwork)
library(latex2exp)

color_palette = "viridis::viridis"
fill_palette = "ggthemes::Gray"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 8
optimum_stroke = 3
optima_shape = 4
contour_color = "gray65"
grad_alpha = 0.45

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

resolution = 100

noise_sd = 20

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)

grad_resolution = 6
grad_length = 0.83
picked = unique(plot_df$x)[seq(0, resolution, by = grad_resolution)]

scale_limits = c(-10.1, 10.1)

p_no_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    color = sqrt((x_grad ^ 2) + (y_grad ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_noisy),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                alpha = grad_alpha,
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked) %>%
                    filter(!is.na(z_constrained)),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_constrained),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(!is.na(z_constrained)) %>%
                   filter(z_constrained == min(z_constrained)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise | p_noise | p_constrained
#+end_SRC

#+RESULTS:
[[file:./img/booth_descent_gradient_template.pdf]]
*** Learning
**** Linear Regression
***** 3d Booth's Function Example
****** Loading Data
Using data from [[Gradient of Booth's Function]]
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)
sampled = sample_n(plot_df, size = 10)
plot_df$linear_prediction = predict(lm(z_noisy ~ x + y, data = sampled), plot_df)
plot_df$quadratic_prediction = predict(lm(z_noisy ~ x + y +
                                              I(x ^ 2) +
                                              I(y ^ 2), data = sampled), plot_df)
plot_df$interaction_prediction = predict(lm(z_noisy ~ x * y +
                                                I(x ^ 2) +
                                                I(y ^ 2), data = sampled), plot_df)
#+end_SRC

#+RESULTS:
****** Computing Model Quality Assessment Metrics
#+begin_SRC R :results output :session *R* :eval no-export :exports results
model_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)
model_sampled = sample_n(model_df, size = 10)

mean_model = lm(z_noisy ~ 1, data = model_sampled)
lin_model = lm(z_noisy ~ x + y, data = model_sampled)
quad_model = lm(z_noisy ~ x + y +
                    I(x ^ 2) +
                    I(y ^ 2), data = model_sampled)
inter_model = lm(z_noisy ~ x * y +
                     I(x ^ 2) +
                     I(y ^ 2), data = model_sampled)

model_df$mean_prediction = predict(mean_model, model_df)
model_df$linear_prediction = predict(lin_model, model_df)
model_df$quadratic_prediction = predict(quad_model, model_df)
model_df$interaction_prediction = predict(inter_model, model_df)

print("Training MSE")
print(mean(summary(mean_model)$residuals ^ 2))
print(mean(summary(lin_model)$residuals ^ 2))
print(mean(summary(quad_model)$residuals ^ 2))
print(mean(summary(inter_model)$residuals ^ 2))

print("Test MSE")
print(mean((model_df$mean_prediction - model_df$z_noisy) ^ 2))
print(mean((model_df$linear_prediction - model_df$z_noisy) ^ 2))
print(mean((model_df$quadratic_prediction - model_df$z_noisy) ^ 2))
print(mean((model_df$interaction_prediction - model_df$z_noisy) ^ 2))

print("True MSE")
print(mean((model_df$mean_prediction - model_df$z) ^ 2))
print(mean((model_df$linear_prediction - model_df$z) ^ 2))
print(mean((model_df$quadratic_prediction - model_df$z) ^ 2))
print(mean((model_df$interaction_prediction - model_df$z) ^ 2))

print("Adj R2")
print(summary(mean_model)$adj.r.squared)
print(summary(lin_model)$adj.r.squared)
print(summary(quad_model)$adj.r.squared)
print(summary(inter_model)$adj.r.squared)
#+end_SRC

#+RESULTS:
#+begin_example
[1] "Training MSE"
[1] 195976.4
[1] 35683.63
[1] 16529.28
[1] 126.1449
[1] "Test MSE"
[1] 210773.1
[1] 203607.6
[1] 159856.1
[1] 867.6703
[1] "True MSE"
[1] 210393.6
[1] 203146.2
[1] 159442.2
[1] 457.4678
[1] "Adj R2"
[1] 0
[1] 0.7658955
[1] 0.8481822
[1] 0.9985517
#+end_example

****** 3D Noisy
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)
    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(z_noisy ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy.pdf]]

****** 3D Noisy: Linear Prediction
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy_linear.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    # xlim = c(-10, 10)
    # ylim = c(-10, 10)
    # zlim = c(-57.03081, 2578.93100)
    # xlim.scaled = c(-0.5, 0.5)
    # ylim.scaled = c(-0.5, 0.5)
    # zlim.scaled = c(-0.5, 0.5)

    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)

    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(linear_prediction ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy_linear.pdf]]

****** 3D Noisy: Quadratic Prediction
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy_quadratic.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    # xlim = c(-10, 10)
    # ylim = c(-10, 10)
    # zlim = c(-57.03081, 2578.93100)
    # xlim.scaled = c(-0.5, 0.5)
    # ylim.scaled = c(-0.5, 0.5)
    # zlim.scaled = c(-0.5, 0.5)

    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)

    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(quadratic_prediction ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy_quadratic.pdf]]

****** 3D Noisy: Quadratic + Interactions Prediction
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy_quadratic_interactions.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    # xlim = c(-10, 10)
    # ylim = c(-10, 10)
    # zlim = c(-57.03081, 2578.93100)
    # xlim.scaled = c(-0.5, 0.5)
    # ylim.scaled = c(-0.5, 0.5)
    # zlim.scaled = c(-0.5, 0.5)

    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)

    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(interaction_prediction ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy_quadratic_interactions.pdf]]

***** 2d ad-hoc example
****** Generating Data
#+begin_SRC R :results graphics output :session *R* :file "./img/regression_example.pdf" :width 10 :height 10 :eval no-export
library(latex2exp)
library(ggplot2)
library(dplyr)

f <- function(X) {
  return((0.2 * X) + (4 / X) + (0.5 * runif(length(X))))
}

sample_f <- function(X, id) {
  return(data.frame(X = X, Y = f(X), repetition = id))
}

X <- seq(1, 10)

df <- bind_rows(sample_f(X, 1),
                sample_f(X, 2),
                sample_f(X, 3),
                sample_f(X, 4))

fake_experiment <- ggplot(df, aes(x = as.factor(X),
                                  y = Y)) +
  #scale_color_brewer(palette = "Dark2") +
  scale_x_discrete() +
  xlab(TeX("$\\mathbf{x}$")) +
  ylab(TeX("$\\mathbf{y} = f(\\mathbf{x}) + \\mathbf{\\epsilon}$")) +
  ylim(1.6, 4.8) +
  #labs(color = "Repetition") +
  theme_bw(base_size = 32) +
  theme(legend.position = c(0.68, 0.96),
        legend.direction = "horizontal",
        legend.background = element_rect(fill = "transparent"))

p1 = fake_experiment +
    #geom_point(size = 2) + #, aes(color = as.factor(repetition))) +
    geom_point(size = 3, shape = 1, stroke = 1.2) +
    annotate("text",
             size = 10.2,
             x = 5.76,
             y = 4.7,
             label = TeX(paste("$f(\\mathbf{x})",
                               " = \\frac{1}{5}\\,x_1",
                               " + 4\\,\\left(\\frac{1}{x_1}\\right)$",
                               sep = ""))) +
    theme(legend.position = c(0.68, 0.96),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent"))

p1
#+end_SRC

#+RESULTS:
[[file:./img/regression_example.pdf]]
****** Full Model
#+begin_SRC R :results graphics output :session *R* :file "./img/regression_example_full_model.pdf" :width 10 :height 10 :eval no-export
library(latex2exp)
library(ggplot2)
library(dplyr)

p2 = fake_experiment +
    geom_smooth(aes(x = as.numeric(X), y = Y),
                method = "lm",
                formula = "y ~ x + I(1 / x)",
                alpha = 0.3,
                color = "blue") +
    geom_point(size = 3, shape = 1, stroke = 1.2) +
    annotate("text",
             size = 10.2,
             x = 5.76,
             y = 4.7,
             label = TeX(paste("$\\hat{f}_{\\theta}(\\mathbf{x}) =",
                               " M(\\mathbf{x})\\theta(\\mathbf{X},\\mathbf{y}) + \\epsilon",
                               " = \\theta_0\\cdot 1 + \\theta_{1}x_1",
                               " + \\theta_2\\left(\\frac{1}{x_1}\\right)",
                               " + \\epsilon$",
                               sep = ""))) +
    theme(axis.title.y = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank())

p2
#+end_SRC

#+RESULTS:
[[file:./img/regression_example_full_model.pdf]]
****** Full Figure
#+begin_SRC R :results graphics output :session *R* :file "./img/regression_example_full.pdf" :width 20 :height 10 :eval no-export
library(patchwork)

p1 | p2
#+end_SRC

#+RESULTS:
[[file:./img/regression_example_full.pdf]]
**** Gaussian Process Regression
***** Sampling from Multivariate Normal Distributions
****** 2-Dimensional Example
******* Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(MASS)

n <- 2
sigma_0 <- data.frame(d1 = c(1, 0), d2 = c(0, 1))
sigma_1 <- data.frame(d1 = c(1, 0.85), d2 = c(0.85, 1))
sigma_2 <- data.frame(d1 = c(2.4, 0.85), d2 = c(0.85, 0.6))
means <- c(0, 0)

sample_size = 1000

mv_sample_0 <- mvrnorm(n = sample_size, means, as.matrix(sigma_0))
mv_sample_0 <- as.data.frame(mv_sample_0)

mv_sample_1 <- mvrnorm(n = sample_size, means, as.matrix(sigma_1))
mv_sample_1 <- as.data.frame(mv_sample_1)

mv_sample_2 <- mvrnorm(n = sample_size, means, as.matrix(sigma_2))
mv_sample_2 <- as.data.frame(mv_sample_2)
#+end_SRC

#+RESULTS:

******* 2D-uncorrelated plot
#+begin_SRC R :results graphics output :session *R* :file "./img/2d_gaussian_uncorrelated.pdf" :width 60 :height 20 :eval no-export
library(ggplot2)
library(patchwork)
library(latex2exp)
library(paletteer)

color_palette = "viridis::viridis"
fill_palette = "ggthemes::Gray"

darkest_viridis = paletteer_c(palette = "viridis::viridis",
                              n = 2)[[1]]

light_gray = "gray65"
title_size = 80

density_thickness = 3
point_size = 7

lims = c(-3.5, 3.5)

plot_margin = margin(t = -0.45,
                     r = -0.45,
                     b = -0.45,
                     l = -0.45,
                     unit = "cm")

theme_config = theme_bw(base_size = 30) +
    theme(axis.title.y = element_text(angle = 0,
                                      margin = margin(t = 0,
                                                      b = 0,
                                                      r = 1,
                                                      l = 0,
                                                      unit = "cm"),
                                      vjust = 0.5,
                                      size = title_size),
          axis.title.x = element_text(margin = margin(t = 1,
                                                      b = 0,
                                                      r = 0,
                                                      l = 0,
                                                      unit = "cm"),
                                      size = title_size),
          axis.ticks = element_blank(),
          axis.text.x = element_blank(),
          plot.margin = margin(t = 10,
                               r = 10,
                               b = 0.0,
                               l = 40),
          axis.text.y = element_blank())

p1 = ggplot() +
    geom_density(data = mv_sample_0,
                 aes(x = V1),
                 size = density_thickness,
                 alpha = 0.7,
                 color = darkest_viridis) +
    xlim(lims) +
    theme_minimal() +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          plot.margin = plot_margin)

p2 = ggplot() +
    geom_density(data = mv_sample_0,
                 aes(x = V2),
                 size = density_thickness,
                 alpha = 0.8,
                 color = darkest_viridis) +
    xlim(lims) +
    coord_flip() +
    theme_minimal() +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          plot.margin = plot_margin)

p3 = ggplot() +
    geom_point(data = mv_sample_0,
               aes(x = V1,
                   y = V2),
               size = point_size,
               #shape = 1,
               alpha = 0.6,
               #stroke = 1.5,
               color = darkest_viridis) +
    xlim(lims) +
    ylim(lims) +
    ylab(TeX("$x_2")) +
    xlab(TeX("$x_1")) +
    theme_config

p4 = ggplot(data = data.frame(x = c(0.0),
                              y = c(0.0)),
            aes(x = x,
                y = x)) +
    theme_minimal() +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          plot.margin = plot_margin)

p1_1 = ggplot() +
    geom_density(data = mv_sample_1,
                 aes(x = V1),
                 size = density_thickness,
                 alpha = 0.7,
                 color = darkest_viridis) +
    xlim(lims) +
    theme_minimal() +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          plot.margin = plot_margin)

p2_1 = ggplot() +
    geom_density(data = mv_sample_1,
                 aes(x = V2),
                 size = density_thickness,
                 alpha = 0.8,
                 color = darkest_viridis) +
    xlim(lims) +
    coord_flip() +
    theme_minimal() +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          plot.margin = plot_margin)

p3_1 = ggplot() +
    geom_point(data = mv_sample_1,
               aes(x = V1,
                   y = V2),
               size = point_size,
               alpha = 0.6,
               color = darkest_viridis) +
    xlim(lims) +
    ylim(lims) +
    ylab(TeX("$x_2")) +
    xlab(TeX("$x_1")) +
    theme_config

p1_2 = ggplot() +
    geom_density(data = mv_sample_2,
                 aes(x = V1),
                 size = density_thickness,
                 alpha = 0.7,
                 color = darkest_viridis) +
    xlim(lims) +
    theme_minimal() +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          plot.margin = plot_margin)

p2_2 = ggplot() +
    geom_density(data = mv_sample_2,
                 aes(x = V2),
                 size = density_thickness,
                 alpha = 0.8,
                 color = darkest_viridis) +
    xlim(lims) +
    coord_flip() +
    theme_minimal() +
    theme(axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          plot.margin = plot_margin)

p3_2 = ggplot() +
    geom_point(data = mv_sample_2,
               aes(x = V1,
                   y = V2),
               size = point_size,
               alpha = 0.6,
               color = darkest_viridis) +
    xlim(lims) +
    ylim(lims) +
    ylab(TeX("$x_2")) +
    xlab(TeX("$x_1")) +
    theme_config

# (((p1 | p4) /
#   (p3 | p2)) +
#  plot_layout(heights = c(1, 2))) |
#     ((p1_1 | p4) / (p3_1 | p2_1)) |
#     ((p1_2 | p4) / (p3_2 | p2_2))

panel_a = (p1 | p4 | p3 | p2) +
    plot_layout(ncol = 2,
                nrow = 2,
                widths = c(2, 1),
                heights = c(1, 2))

panel_b = (p1_1 | p4 | p3_1 | p2_1) +
    plot_layout(ncol = 2,
                nrow = 2,
                widths = c(2, 1),
                heights = c(1, 2))

panel_c = (p1_2 | p4 | p3_2 | p2_2) +
    plot_layout(ncol = 2,
                nrow = 2,
                widths = c(2, 1),
                heights = c(1, 2))

panel_a | panel_b | panel_c
#+end_SRC

#+RESULTS:
[[file:./img/2d_gaussian_uncorrelated.pdf]]

****** Covariance Kernels
******* Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(kergp)
library(latex2exp)

n = 300
x0 = 0
x = seq(from = 0, to = 3, length.out = n)

df = data.frame(x = x,
                y = as.vector(k1FunMatern5_2(x0,
                                             x,
                                             par = c(range = 1,
                                                     var = 2))),
                label = c("Matérn 5/2"))

df = bind_rows(df,
               data.frame(x = x,
                          y = as.vector(k1FunMatern3_2(x0,
                                                       x,
                                                       par = c(range = 1,
                                                               var = 2))),
                          label = c("Matérn 3/2")))

df = bind_rows(df,
               data.frame(x = x,
                          y = as.vector(k1FunGauss(x0,
                                                   x,
                                                   par = c(range = 1,
                                                           var = 2))),
                          label = c("Squared Exponential")))

df = bind_rows(df,
               data.frame(x = x,
                          y = as.vector(k1FunExp(x0,
                                                 x,
                                                 par = c(range = 1,
                                                         var = 2))),
                          label = c("Exponential")))
#+end_SRC

#+RESULTS:
******* Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/radial_basis_kernels.pdf" :height 10 :width 13 :eval no-export
library(ggplot2)
library(latex2exp)

ggplot() +
    geom_line(data = df,
              aes(x = x,
                  y = y,
                  color = label),
              alpha = 0.8,
              size = 5) +
    scale_color_brewer(palette = "Set1") +
    ylab(TeX("$Cov(\\mathbf{x},\\,\\mathbf{x}\\prime)$")) +
    xlab(TeX("$||\\,\\mathbf{x} -\\,\\mathbf{x}\\prime\\,||$")) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0.01, 0.01)) +
    theme_bw(base_size = 40) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank(),
          legend.position = c(0.74, 0.87),
          legend.title = element_blank(),
          legend.text = element_text(size = 36),
          legend.background = element_blank())
#+end_SRC

#+RESULTS:
[[file:./img/radial_basis_kernels.pdf]]
****** Larger Dimensional Example: Change of Perspective
******* Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(kergp)
library(dplyr)
library(tidyr)
library(latex2exp)
library(MASS)

n = 20
x = seq(from = 0, to = 3, length.out = n)
means = rep(0, n)

sigma_0 = k1FunMatern5_2(x, x, par = c(range = 1, var = 1))
sample_size = 100

ys = data.frame(t(mvrnorm(n = sample_size, means, sigma_0)))

mv_sample_3 = ys %>%
    mutate(x = x) %>%
    pivot_longer(-x, names_to = "sample", values_to = "value")
#+end_SRC

#+RESULTS:
******* Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/matern52_50d_mv_samples.pdf" :width 20 :height 10 :eval no-export
library(ggplot2)
library(paletteer)
library(latex2exp)
library(patchwork)

p_limits = c(-4, 4)
p_palette = "khroma::stratigraphy"
s_size = 6
point_size = 5
leg_size = 34
point_alpha = 0.45

colors = paletteer_d(palette = p_palette, n = 175)
point_color = colors[length(colors) / 2]

plot_df = mv_sample_3 %>%
    group_by(x) %>%
    mutate(y_mean = mean(value),
           y_95ci = (1.96 * sd(value)) / sqrt(n())) %>%
    ungroup()

sample_select = "X1"

p1 = ggplot() +
    geom_jitter(data = mv_sample_3,
               aes(x = x,
                   y = value),
               height = 0.0,
               width = 0.03,
               color = "gray55",
               alpha = point_alpha,
               size = point_size) +
    geom_point(data = plot_df %>% filter(sample == sample_select),
              aes(x = x,
                  y = value,
                  color = sample),
              size = s_size) +
    # geom_point(data = plot_df,
    #            aes(x = x,
    #                y = y_mean),
    #            color = "black",
    #            stat = "unique",
    #            show.legend = FALSE,
    #            size = 4) +
    # geom_errorbar(data = plot_df,
    #               aes(x = x,
    #                   ymin = y_mean - y_95ci,
    #                   ymax = y_mean + y_95ci),
    #               color = "black",
    #               stat = "unique",
    #               show.legend = FALSE) +
    scale_color_manual(labels = c(unname(TeX(paste("Single sample of ",
                                                   "$N(\\mu,\\Sigma)$",
                                                   sep = "")))),
                       values = c(point_color)) +
    scale_x_continuous(expand = c(0.025, 0.025)) +
    #scale_y_continuous(limits = p_limits, expand = c(0.01, 0.01)) +
    theme_bw(base_size = 35) +
    ylab(TeX("Sampled Values")) +
    xlab(TeX("Dimensions $d_{1},\\ldots,d_{20}$")) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          legend.title = element_blank(),
          legend.text = element_text(size = leg_size),
          legend.background = element_blank(),
          legend.position = c(0.35, 0.051),
          panel.grid = element_blank())

p2 = ggplot() +
    geom_jitter(data = mv_sample_3,
               aes(x = x,
                   y = value),
               height = 0.0,
               width = 0.03,
               color = "gray55",
               alpha = point_alpha,
               show.legend = FALSE,
               size = point_size) +
    geom_point(data = plot_df %>%
                   filter(sample == sample_select),
              aes(x = x,
                  y = value,
                  color = sample),
              size = s_size) +
    geom_line(data = plot_df %>%
                  filter(sample == sample_select),
              aes(x = x,
                  y = value),
              color = point_color,
              show.legend = FALSE,
              size = 1.7) +
    scale_color_manual(labels = unname(TeX(paste("Single sample of ",
                                                   "$\\hat{f}_{\\theta}(",
                                                   "\\mathbf{x}_1,\\ldots,",
                                                   "\\mathbf{x}_{20})$",
                                                   sep = ""))),
                       values = c(point_color)) +
    scale_x_continuous(expand = c(0.025, 0.025)) +
    theme_bw(base_size = 35) +
    ylab(TeX("$\\hat{f}_{\\theta}$")) +
    xlab(TeX(paste("$d_1,\\ldots,d_{20}$ ",
                   "interpreted as ",
                   "$\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{20}$",
                   sep = ""))) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          axis.title.y = element_text(angle = 0,
                                      vjust = 0.5),
          legend.title = element_blank(),
          legend.background = element_blank(),
          legend.key = element_blank(),
          legend.position = c(0.42, 0.051),
          legend.text = element_text(size = leg_size),
          panel.grid = element_blank()) +
    guides(color = guide_legend(label.vjust = 1.7))

p1 | p2
#+end_SRC

#+RESULTS:
[[file:./img/matern52_50d_mv_samples.pdf]]

****** Fitting Noiseless Data
******* Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(kergp)
library(dplyr)
library(tidyr)
library(latex2exp)
library(MASS)

n = 50
x = seq(from = 0, to = 3, length.out = n)
means = rep(0, n)


sigma_0 = k1FunMatern5_2(x, x, par = c(range = 1, var = 1))
sample_size = 300

obs_1 = data.frame(x_obs = c(1.5), y_obs = c(-1.0))
noise = 0.00

sigma_1_2 = k1FunMatern5_2(x, obs_1$x_obs, par = c(range = 1, var = 1))
sigma_2_1 = t(sigma_1_2)
sigma_2_2 = k1FunMatern5_2(obs_1$x_obs,
                           obs_1$x_obs,
                           par = c(range = 1,
                                   var = 1)) + noise

ys = data.frame(t(mvrnorm(n = sample_size, means, sigma_0)))

means_1 = as.vector(sigma_1_2 %*% solve(sigma_2_2) * obs_1$y_obs)
sigma_1 = sigma_0 - (sigma_1_2 %*% solve(sigma_2_2) %*% sigma_2_1)

obs_2 = data.frame(x_obs = c(0.3, 1.5, 2),
                   y_obs = c(-0.45, -1.0, -0.4))
noise = noise * diag(3)

sigma_1_2 = k1FunMatern5_2(x, obs_2$x_obs, par = c(range = 1, var = 1))
sigma_2_1 = t(sigma_1_2)
sigma_2_2 = k1FunMatern5_2(obs_2$x_obs,
                           obs_2$x_obs,
                           par = c(range = 1,
                                   var = 1)) + noise

ys = data.frame(t(mvrnorm(n = sample_size, means, sigma_0)))

means_2 = as.vector(sigma_1_2 %*% solve(sigma_2_2) %*% obs_2$y_obs)
sigma_2 = sigma_0 - (sigma_1_2 %*% solve(sigma_2_2) %*% sigma_2_1)

mv_sample_0 = ys %>%
    mutate(x = x) %>%
    pivot_longer(-x, names_to = "sample", values_to = "value")

mv_sample_1 = data.frame(t(mvrnorm(n = sample_size, means_1, sigma_1))) %>%
    mutate(x = x) %>%
    pivot_longer(-x, names_to = "sample", values_to = "value")

mv_sample_2 = data.frame(t(mvrnorm(n = sample_size, means_2, sigma_2))) %>%
    mutate(x = x) %>%
    pivot_longer(-x, names_to = "sample", values_to = "value")
#+end_SRC

#+RESULTS:
******* Fitting Data
#+begin_SRC R :results graphics output :session *R* :file "./img/matern52_50d_fitting.pdf" :width 30 :height 10 :eval no-export
library(ggplot2)
library(paletteer)
library(latex2exp)
library(patchwork)

p_limits = c(-3.4, 3.4)
p_palette = "khroma::stratigraphy"
s_size = 5

p1 = ggplot() +
    geom_line(data = mv_sample_0,
              aes(x = x,
                  y = value,
                  color = sample),
              show.legend = FALSE,
              size = 1.7) +
    scale_color_paletteer_d(palette = p_palette) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = p_limits, expand = c(0.01, 0.01)) +
    theme_bw(base_size = 35) +
    ylab(TeX("$\\hat{f}_{\\theta}(\\mathbf{x})$")) +
    xlab(TeX("$\\mathbf{x}$")) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank())

p2 = ggplot() +
    geom_line(data = mv_sample_1,
              aes(x = x,
                  y = value,
                  color = sample),
              show.legend = FALSE,
              size = 1.7) +
    geom_point(data = obs_1,
                aes(x = x_obs,
                    y = y_obs),
                color = "black",
                size = s_size) +
    scale_color_paletteer_d(palette = p_palette) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = p_limits, expand = c(0.01, 0.01)) +
    theme_bw(base_size = 35) +
    ylab(TeX("$\\hat{f}_{\\theta}(\\mathbf{x})$ $|$ $(\\mathbf{x}_1, y_1)$")) +
    xlab(TeX("$\\mathbf{x}$")) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank())

p3 = ggplot() +
    geom_line(data = mv_sample_2,
              aes(x = x,
                  y = value,
                  color = sample),
              show.legend = FALSE,
              size = 1.7) +
    geom_point(data = obs_2,
                aes(x = x_obs,
                    y = y_obs),
                color = "black",
                size = s_size) +
    scale_color_paletteer_d(palette = p_palette) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = p_limits, expand = c(0.01, 0.01)) +
    theme_bw(base_size = 35) +
    ylab(TeX("$\\hat{f}_{\\theta}(\\mathbf{x})$ $|$ $(\\mathbf{X}, \\mathbf{y})$")) +
    xlab(TeX("$\\mathbf{x}$")) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank())

p1 | p2 | p3
#+end_SRC

#+RESULTS:
[[file:./img/matern52_50d_fitting.pdf]]
****** Fitting Noisy Data
******* Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(kergp)
library(dplyr)
library(tidyr)
library(latex2exp)
library(MASS)

n = 50
x = seq(from = 0, to = 3, length.out = n)
means = rep(0, n)


sigma_0 = k1FunMatern5_2(x, x, par = c(range = 1, var = 1))
sample_size = 300

obs_1 = data.frame(x_obs = c(1.5), y_obs = c(-1.0))
noise = 0.008

sigma_1_2 = k1FunMatern5_2(x, obs_1$x_obs, par = c(range = 1, var = 1))
sigma_2_1 = t(sigma_1_2)
sigma_2_2 = k1FunMatern5_2(obs_1$x_obs,
                           obs_1$x_obs,
                           par = c(range = 1,
                                   var = 1)) + noise

ys = data.frame(t(mvrnorm(n = sample_size, means, sigma_0)))

means_1 = as.vector(sigma_1_2 %*% solve(sigma_2_2) * obs_1$y_obs)
sigma_1 = sigma_0 - (sigma_1_2 %*% solve(sigma_2_2) %*% sigma_2_1)

obs_2 = data.frame(x_obs = c(0.3, 1.5, 2),
                   y_obs = c(-0.45, -1.0, -0.4))
noise = noise * diag(3)

sigma_1_2 = k1FunMatern5_2(x, obs_2$x_obs, par = c(range = 1, var = 1))
sigma_2_1 = t(sigma_1_2)
sigma_2_2 = k1FunMatern5_2(obs_2$x_obs,
                           obs_2$x_obs,
                           par = c(range = 1,
                                   var = 1)) + noise

ys = data.frame(t(mvrnorm(n = sample_size, means, sigma_0)))

means_2 = as.vector(sigma_1_2 %*% solve(sigma_2_2) %*% obs_2$y_obs)
sigma_2 = sigma_0 - (sigma_1_2 %*% solve(sigma_2_2) %*% sigma_2_1)

mv_sample_0 = ys %>%
    mutate(x = x) %>%
    pivot_longer(-x, names_to = "sample", values_to = "value")

mv_sample_1 = data.frame(t(mvrnorm(n = sample_size, means_1, sigma_1))) %>%
    mutate(x = x) %>%
    pivot_longer(-x, names_to = "sample", values_to = "value")

mv_sample_2 = data.frame(t(mvrnorm(n = sample_size, means_2, sigma_2))) %>%
    mutate(x = x) %>%
    pivot_longer(-x, names_to = "sample", values_to = "value")
#+end_SRC

#+RESULTS:
******* Fitting Data
#+begin_SRC R :results graphics output :session *R* :file "./img/matern52_50d_fitting_noisy.pdf" :width 30 :height 10 :eval no-export
library(ggplot2)
library(paletteer)
library(latex2exp)
library(patchwork)

p_limits = c(-3.4, 3.4)
p_palette = "khroma::stratigraphy"
s_size = 5

p1 = ggplot() +
    geom_line(data = mv_sample_0,
              aes(x = x,
                  y = value,
                  color = sample),
              show.legend = FALSE,
              size = 1.7) +
    scale_color_paletteer_d(palette = p_palette) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = p_limits, expand = c(0.01, 0.01)) +
    theme_bw(base_size = 35) +
    ylab(TeX("$\\hat{f}_{\\theta}(\\mathbf{x})$")) +
    xlab(TeX("$\\mathbf{x}$")) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank())

p2 = ggplot() +
    geom_line(data = mv_sample_1,
              aes(x = x,
                  y = value,
                  color = sample),
              show.legend = FALSE,
              size = 1.7) +
    geom_point(data = obs_1,
                aes(x = x_obs,
                    y = y_obs),
                color = "black",
                size = s_size) +
    scale_color_paletteer_d(palette = p_palette) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = p_limits, expand = c(0.01, 0.01)) +
    theme_bw(base_size = 35) +
    ylab(TeX("$\\hat{f}_{\\theta}(\\mathbf{x})$ $|$ $(\\mathbf{x}_1, y_1)$")) +
    xlab(TeX("$\\mathbf{x}$")) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank())

p3 = ggplot() +
    geom_line(data = mv_sample_2,
              aes(x = x,
                  y = value,
                  color = sample),
              show.legend = FALSE,
              size = 1.7) +
    geom_point(data = obs_2,
                aes(x = x_obs,
                    y = y_obs),
                color = "black",
                size = s_size) +
    scale_color_paletteer_d(palette = p_palette) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(limits = p_limits, expand = c(0.01, 0.01)) +
    theme_bw(base_size = 35) +
    ylab(TeX("$\\hat{f}_{\\theta}(\\mathbf{x})$ $|$ $(\\mathbf{X}, \\mathbf{y})$")) +
    xlab(TeX("$\\mathbf{x}$")) +
    theme(axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank())

p1 | p2 | p3
#+end_SRC

#+RESULTS:
[[file:./img/matern52_50d_fitting_noisy.pdf]]
****** Old

#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(latex2exp)
library(MASS)

n <- 10
sigma <- data.frame(diag(10))
names(sigma) <- paste("d", seq(1, n), sep = "")

means <- rep(0, n)

mv_sample <- mvrnorm(n = 300, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("d", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720 :eval no-export
library(ggplot2)
library(GGally)

ggpairs(mv_sample) +
  theme_bw(base_size = 22)
#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figurec4TROr.png]]

#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(latex2exp)
library(MASS)

n <- 10
sigma <- data.frame(diag(10))
names(sigma) <- paste("d", seq(1, n), sep = "")

means <- rep(0, n)

mv_sample <- mvrnorm(n = 1000, means, as.matrix(sigma))
mv_sample <- as.data.frame(mv_sample)

names(mv_sample) <- paste("d", seq(1, n), sep = "")
#+end_SRC

#+RESULTS:

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720 :eval no-export
library(dplyr)
library(tidyr)
library(ggplot2)
library(latex2exp)

plot_data <- mv_sample
sampled_function <- sample_n(plot_data, 1)

plot_data <- plot_data %>%
  gather("x", "f_x") %>%
  mutate(x = ordered(x, levels = names(mv_sample)))

sampled_function <- sampled_function %>%
  gather("x", "f_x") %>%
  mutate(x = ordered(x, levels = names(mv_sample)))

ggplot(plot_data, aes(x = x, y = f_x)) +
  geom_jitter(color = "gray48", size = 3, width = 0.25, alpha = 0.2) +
  geom_point(data = sampled_function,
             aes(color = "Sample of Multivariate Normal"),
             size = 4) +
  geom_line(data = sampled_function,
            color = "red",
            size = 1,
            alpha = 0.3) +
  ylab(TeX("Sampled Values")) +
  xlab(TeX("Dimensions")) +
  scale_fill_manual("", values = "gray48") +
  scale_color_brewer(palette = "Set1") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.24, 0.06))

#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figureNntbaA.png]]

#+begin_SRC R :results graphics output :session *R* :file (org-babel-temp-file "figure" ".png") :width 800 :height 720 :eval no-export
library(dplyr)
library(tidyr)
library(ggplot2)
library(latex2exp)

n <- 10

plot_data <- mv_sample
names(plot_data) <- seq(1, n)

sampled_function <- sample_n(plot_data, 1)

plot_data <- plot_data %>%
  gather("x", "f_x") %>%
  mutate(x = as.numeric(x))

sampled_function <- sampled_function %>%
  gather("x", "f_x") %>%
  mutate(x = as.numeric(x))

ggplot(plot_data, aes(x = x, y = f_x)) +
  geom_jitter(color = "gray48", size = 3, width = 0.25, alpha = 0.2) +
  geom_point(data = sampled_function,
             aes(color = "Sampled Function"),
             size = 4) +
  geom_line(data = sampled_function,
            color = "red",
            size = 1,
            alpha = 0.3) +
  ylab(TeX("Samples of $(d_1,\\ldots,d_{10})$ interpreted as $f(x \\in \\lbrack 1,10 \\rbrack)$")) +
  xlab(TeX("$(d_1,\\ldots,d_{10})$ interpreted as discrete $x \\in \\lbrack 1,10 \\rbrack$")) +
  scale_x_discrete(limits = seq(1, 10)) +
  scale_fill_manual("", values = "gray48") +
  scale_color_brewer(palette = "Set1") +
  theme_bw(base_size = 26) +
  theme(legend.title = element_blank(),
        legend.background = element_rect(fill = "transparent"),
        legend.position = c(0.2, 0.06))

#+end_SRC

#+RESULTS:
[[file:/tmp/babel-oICNIQ/figure128cP4.png]]
***** Trends
****** Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)
library(DiceKriging)

df = data.frame(x = c(-0.8, 0.1, 0.3, 0.5, 0.7, 0.9),
                y = c(0.4, 0.5, 0.4, 0.3, 0.25, 0.4))

target_x = data.frame(x = seq(from = -2.0,
                              to = 2.0,
                              length.out = 100))

const_trend_gp = km(design = df %>% select(x),
                    response = df %>% select(y),
                    multistart = 10,
                    estim.method = "LOO",
                    covtype = "gauss")

linear_trend_gp = km(formula = y ~ x,
                     design = df %>% select(x),
                     response = df %>% select(y),
                     multistart = 10,
                     estim.method = "LOO",
                     covtype = "gauss")

quad_trend_gp = km(formula = y ~ x + I(x ^ 2),
                     design = df %>% select(x),
                     response = df %>% select(y),
                     multistart = 10,
                     estim.method = "LOO",
                     covtype = "gauss")

target_x$const_prediction = predict(const_trend_gp,
                                    target_x %>% select(x),
                                    "UK")$mean

target_x$linear_prediction = predict(linear_trend_gp,
                                     target_x %>% select(x),
                                     "UK")$mean

target_x$quadratic_prediction = predict(quad_trend_gp,
                                        target_x %>% select(x),
                                        "UK")$mean
#+end_SRC

#+RESULTS:
#+begin_example

optimisation start
------------------
,* estimation method   : LOO
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~1
,* covariance model :
  - type :  gauss
  - nugget : NO
  - parameters lower bounds :  1e-10
  - parameters upper bounds :  3.4
  - best initial criterion value(s) :  0.538556 0.7236078 1.265375 1.394417 1.531472 1.736053 1.934439 2.076638 2.278273 2.499264

N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      0.53856  |proj g|=      0.72654
At iterate     1  f =       0.0093  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      0.72361  |proj g|=      0.79668
At iterate     1  f =       0.0093  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.2654  |proj g|=       1.0143
At iterate     1  f =       0.0093  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.3944  |proj g|=       1.0744
At iterate     1  f =       0.0093  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.5315  |proj g|=       1.1445
At iterate     1  f =       0.0093  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.7361  |proj g|=       1.2649
At iterate     1  f =       0.0093  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.9344  |proj g|=       1.2362
At iterate     1  f =    0.0049177  |proj g|=     0.0013682
At iterate     2  f =    0.0049165  |proj g|=    0.00036938
At iterate     3  f =    0.0049164  |proj g|=    1.2369e-05
At iterate     4  f =    0.0049164  |proj g|=    1.0501e-07

iterations 4
function evaluations 5
segments explored during Cauchy searches 4
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 1.0501e-07
final function value 0.00491641

F = 0.00491641
final  value 0.004916
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       2.0766  |proj g|=       1.0167
At iterate     1  f =      0.10857  |proj g|=       0.51797
ys=-2.380e-01  -gs= 1.034e+00, BFGS update SKIPPED
At iterate     2  f =       0.0093  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       2.2783  |proj g|=      0.71868
At iterate     1  f =       1.3458  |proj g|=        1.0512
ys=-1.010e+00  -gs= 5.165e-01, BFGS update SKIPPED
At iterate     2  f =       0.0093  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       2.4993  |proj g|=      0.42264
At iterate     1  f =       2.2608  |proj g|=       0.74368
ys=-1.357e-01  -gs= 1.786e-01, BFGS update SKIPPED
At iterate     2  f =       0.0093  |proj g|=             0

iterations 2
function evaluations 4
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0
final function value 0.0093

F = 0.0093
final  value 0.009300
converged

,* The 10 best values (multistart) obtained are:
 0.0093 0.0093 0.0093 0.0093 0.0093 0.0093 0.004916414 0.0093 0.0093 0.0093
,* The model corresponding to the best one (0.004916414) is stored.

optimisation start
------------------
,* estimation method   : LOO
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~x
,* covariance model :
  - type :  gauss
  - nugget : NO
  - parameters lower bounds :  1e-10
  - parameters upper bounds :  3.4
  - best initial criterion value(s) :  0.007912638 0.01124534 0.01184891 0.01500916 0.1992156 0.2393992 1.857197 1.96498 1.969283 2.105195

N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=    0.0079126  |proj g|=     0.020771
At iterate     1  f =    0.0075231  |proj g|=      0.016539
At iterate     2  f =    0.0071191  |proj g|=     0.0069969
At iterate     3  f =    0.0070768  |proj g|=     0.0045511
At iterate     4  f =    0.0070595  |proj g|=     0.0007455
At iterate     5  f =    0.0070589  |proj g|=     6.867e-05
At iterate     6  f =    0.0070589  |proj g|=    1.1583e-06
At iterate     7  f =    0.0070589  |proj g|=    1.7668e-09

iterations 7
function evaluations 9
segments explored during Cauchy searches 7
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 1.76676e-09
final function value 0.00705891

F = 0.00705891
final  value 0.007059
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=     0.011245  |proj g|=     0.038275
At iterate     1  f =     0.010333  |proj g|=      0.015619
At iterate     2  f =     0.007065  |proj g|=     0.0025819
At iterate     3  f =    0.0070589  |proj g|=    4.7106e-05
At iterate     4  f =    0.0070589  |proj g|=    2.5366e-06

iterations 4
function evaluations 10
segments explored during Cauchy searches 4
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 2.53656e-06
final function value 0.00705891

F = 0.00705891
final  value 0.007059
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=     0.011849  |proj g|=     0.052394
At iterate     1  f =      0.01032  |proj g|=      0.015533
At iterate     2  f =    0.0071997  |proj g|=       0.01014
At iterate     3  f =    0.0070811  |proj g|=     0.0044346
At iterate     4  f =    0.0070609  |proj g|=     0.0014409
At iterate     5  f =    0.0070589  |proj g|=    0.00015242
At iterate     6  f =    0.0070589  |proj g|=    4.7196e-06
At iterate     7  f =    0.0070589  |proj g|=    1.6014e-08

iterations 7
function evaluations 12
segments explored during Cauchy searches 7
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 1.60142e-08
final function value 0.00705891

F = 0.00705891
final  value 0.007059
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=     0.015009  |proj g|=     0.045573
At iterate     1  f =     0.012178  |proj g|=      0.058198
ys=-5.754e-04  -gs= 2.077e-03, BFGS update SKIPPED
At iterate     2  f =     0.010323  |proj g|=      0.015548
At iterate     3  f =    0.0071872  |proj g|=     0.0097433
At iterate     4  f =    0.0070643  |proj g|=     0.0022521
At iterate     5  f =    0.0070593  |proj g|=    0.00065554
At iterate     6  f =    0.0070589  |proj g|=     3.405e-05
At iterate     7  f =    0.0070589  |proj g|=    4.8752e-07

iterations 7
function evaluations 12
segments explored during Cauchy searches 7
BFGS updates skipped 1
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 4.87518e-07
final function value 0.00705891

F = 0.00705891
final  value 0.007059
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      0.19922  |proj g|=      0.70148
At iterate     1  f =     0.015661  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0156606

F = 0.0156606
final  value 0.015661
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       0.2394  |proj g|=      0.72664
At iterate     1  f =     0.015661  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0156606

F = 0.0156606
final  value 0.015661
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.8572  |proj g|=       1.0692
At iterate     1  f =     0.071637  |proj g|=       0.59544
At iterate     2  f =     0.015661  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0156606

F = 0.0156606
final  value 0.015661
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=        1.965  |proj g|=      0.93656
At iterate     1  f =      0.44383  |proj g|=       0.83567
ys=-1.031e+00  -gs= 8.771e-01, BFGS update SKIPPED
At iterate     2  f =     0.015661  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0156606

F = 0.0156606
final  value 0.015661
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.9693  |proj g|=       0.9313
At iterate     1  f =      0.46404  |proj g|=       0.84554
ys=-1.048e+00  -gs= 8.673e-01, BFGS update SKIPPED
At iterate     2  f =     0.015661  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0156606

F = 0.0156606
final  value 0.015661
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       2.1052  |proj g|=      0.76714
At iterate     1  f =       1.1358  |proj g|=        1.1703
ys=-8.633e-01  -gs= 5.885e-01, BFGS update SKIPPED
At iterate     2  f =     0.015661  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.0156606

F = 0.0156606
final  value 0.015661
converged

,* The 10 best values (multistart) obtained are:
 0.00705891 0.00705891 0.00705891 0.00705891 0.01566064 0.01566064 0.01566064 0.01566064 0.01566064 0.01566064
,* The model corresponding to the best one (0.00705891) is stored.
+ +
optimisation start
------------------
,* estimation method   : LOO
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~x + I(x^2)
,* covariance model :
  - type :  gauss
  - nugget : NO
  - parameters lower bounds :  1e-10
  - parameters upper bounds :  3.4
  - best initial criterion value(s) :  0.5485256 0.8030316 0.8301607 1.320869 2.06434 2.22403 2.231048 2.381351 2.51904 2.688355

N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      0.54853  |proj g|=      0.14021
At iterate     1  f =      0.54801  |proj g|=      0.095903
At iterate     2  f =       0.5471  |proj g|=      0.030753
At iterate     3  f =      0.54704  |proj g|=      0.014972
At iterate     4  f =      0.54703  |proj g|=    0.00087944
At iterate     5  f =      0.54703  |proj g|=     2.117e-05
At iterate     6  f =      0.54703  |proj g|=    3.2282e-08

iterations 6
function evaluations 8
segments explored during Cauchy searches 6
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 3.2282e-08
final function value 0.547027

F = 0.547027
final  value 0.547027
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      0.80303  |proj g|=      0.37505
At iterate     1  f =      0.55164  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      0.83016  |proj g|=      0.38961
At iterate     1  f =      0.55164  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       1.3209  |proj g|=      0.61229
At iterate     1  f =      0.55164  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       2.0643  |proj g|=       1.0219
At iterate     1  f =      0.55164  |proj g|=             0

iterations 1
function evaluations 2
segments explored during Cauchy searches 1
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=        2.224  |proj g|=       1.0204
At iterate     1  f =      0.54847  |proj g|=       0.13986
At iterate     2  f =        0.548  |proj g|=        0.0957
Nonpositive definiteness in Cholesky factorization in formk;
   refresh the lbfgs memory and restart the iteration.
At iterate     3  f =      0.54711  |proj g|=      0.034297
At iterate     4  f =      0.54705  |proj g|=      0.017114
At iterate     5  f =      0.54703  |proj g|=     0.0011351
At iterate     6  f =      0.54703  |proj g|=    3.0953e-05
At iterate     7  f =      0.54703  |proj g|=    6.1007e-08

iterations 7
function evaluations 10
segments explored during Cauchy searches 8
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 6.10069e-08
final function value 0.547027

F = 0.547027
final  value 0.547027
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=        2.231  |proj g|=       1.0081
At iterate     1  f =      0.55337  |proj g|=       0.15903
At iterate     2  f =      0.55164  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       2.3814  |proj g|=      0.74986
At iterate     1  f =       1.2691  |proj g|=       0.58968
ys=-1.162e+00  -gs= 5.623e-01, BFGS update SKIPPED
At iterate     2  f =      0.55164  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=        2.519  |proj g|=      0.52576
At iterate     1  f =       2.0779  |proj g|=        1.0324
ys=-3.945e-01  -gs= 2.764e-01, BFGS update SKIPPED
At iterate     2  f =      0.55164  |proj g|=             0

iterations 2
function evaluations 3
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged
N = 1, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       2.6884  |proj g|=      0.27928
At iterate     1  f =       2.5932  |proj g|=       0.41288
ys=-3.731e-02  -gs= 7.800e-02, BFGS update SKIPPED
At iterate     2  f =      0.55164  |proj g|=             0

iterations 2
function evaluations 4
segments explored during Cauchy searches 2
BFGS updates skipped 1
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0
final function value 0.551638

F = 0.551638
final  value 0.551638
converged

,* The 10 best values (multistart) obtained are:
 0.5470274 0.551638 0.551638 0.551638 0.551638 0.5470274 0.551638 0.551638 0.551638 0.551638
,* The model corresponding to the best one (0.5470274) is stored.
#+end_example
****** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/gp_trends.pdf" :width 13 :height 10 :eval no-export
library(ggplot2)
library(dplyr)
library(tidyr)
library(latex2exp)

plot_df = target_x %>%
    pivot_longer(cols = c(const_prediction,
                          linear_prediction,
                          quadratic_prediction),
                 names_to = "Trends",
                 values_to = "y") %>%
    mutate(Trends = factor(Trends,
                           levels = c("const_prediction",
                                      "linear_prediction",
                                      "quadratic_prediction"),
                           labels = c("Constant",
                                      "Linear",
                                      "Quadratic")))

ggplot() +
    geom_line(data = plot_df,
              size = 3.4,
              alpha = 0.6,
              aes(x = x,
                  y = y,
                  color = Trends)) +
    geom_point(data = df,
               size = 4.5,
               shape = 1,
               stroke = 3,
               aes(x = x,
                   y = y),
               color = "black") +
    ylim(0, 1.0) +
    ylab(TeX("$\\hat{f}_{\\theta}(x)$")) +
    xlab(TeX("$x$")) +
    scale_color_brewer(name = "Surrogate Trend", palette = "Set1") +
    scale_x_continuous(expand = c(0.01, 0.01)) +
    scale_y_continuous(expand = c(0.01, 0.01)) +
    theme_bw(base_size = 39) +
    theme(axis.title.y = element_text(angle = 90,
                                      vjust = 0.5),
          axis.ticks = element_blank(),
          axis.text = element_blank(),
          panel.grid = element_blank(),
          legend.position = c(0.22, 0.84),
          legend.direction = "vertical",
          legend.background = element_rect(fill = "transparent"))
#+end_SRC

#+RESULTS:
[[file:./img/gp_trends.pdf]]

***** 3d Booth Example
****** Loading Data
Using data from [[Gradient of Booth's Function]]
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)
sampled = sample_n(plot_df, size = 10)
#+end_SRC

#+RESULTS:
****** Computing GP Fits
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(DiceKriging)

const_trend_gp = km(design = sampled %>% select(x, y),
                    response = sampled %>% select(z_noisy),
                    covtype = "matern5_2",
                    nugget = 1e-6)

linear_trend_gp = km(formula = z_noisy ~ x + y,
                     design = sampled %>% select(x, y),
                     response = sampled %>% select(z_noisy),
                     covtype = "matern5_2",
                     nugget = 1e-6)

quad_trend_gp = km(formula = z_noisy ~ x + y +
                         I(x ^ 2) + I(y ^ 2),
                     design = sampled %>% select(x, y),
                     response = sampled %>% select(z_noisy),
                     covtype = "matern5_2",
                     nugget = 1e-6)

inter_trend_gp = km(formula = z_noisy ~ x * y +
                        I(x ^ 2) + I(y ^ 2),
                    design = sampled %>% select(x, y),
                    response = sampled %>% select(z_noisy),
                    covtype = "matern5_2",
                    nugget = 1e-6)

plot_df$const_prediction = predict(const_trend_gp,
                                   plot_df %>% select(x, y),
                                   "UK")$mean

plot_df$linear_prediction = predict(linear_trend_gp,
                                    plot_df %>% select(x, y),
                                    "UK")$mean

plot_df$quadratic_prediction = predict(quad_trend_gp,
                                    plot_df %>% select(x, y),
                                    "UK")$mean

plot_df$interaction_prediction = predict(inter_trend_gp,
                                    plot_df %>% select(x, y),
                                    "UK")$mean
#+end_SRC

#+RESULTS:
#+begin_example

optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~1
,* covariance model :
  - type :  matern5_2
  - nugget : 1e-06
  - parameters lower bounds :  1e-10 1e-10
  - parameters upper bounds :  38.38384 33.53535
  - variance bounds :  10588.31 1198707
  - best initial criterion value(s) :  -69.15038

N = 3, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=        69.15  |proj g|=       1.0591
At iterate     1  f =       68.245  |proj g|=       0.51666
At iterate     2  f =       67.684  |proj g|=       0.22174
At iterate     3  f =       67.361  |proj g|=      0.062222
At iterate     4  f =       67.344  |proj g|=      0.017177
At iterate     5  f =       67.343  |proj g|=     0.0059212
At iterate     6  f =       67.343  |proj g|=     0.0016697
At iterate     7  f =       67.343  |proj g|=    0.00020049
At iterate     8  f =       67.343  |proj g|=    1.1044e-05
At iterate     9  f =       67.343  |proj g|=    5.8017e-05

iterations 9
function evaluations 13
segments explored during Cauchy searches 9
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 5.80172e-05
final function value 67.343

F = 67.343
final  value 67.342994
converged

optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~x + y
,* covariance model :
  - type :  matern5_2
  - nugget : 1e-06
  - parameters lower bounds :  1e-10 1e-10
  - parameters upper bounds :  38.38384 33.53535
  - variance bounds :  5884.415 608500.8
  - best initial criterion value(s) :  -65.11035

N = 3, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=        65.11  |proj g|=      0.36238
At iterate     1  f =       64.959  |proj g|=       0.28619
At iterate     2  f =       64.724  |proj g|=      0.035067
At iterate     3  f =       64.722  |proj g|=     0.0066773
At iterate     4  f =       64.721  |proj g|=     0.0029722
At iterate     5  f =       64.721  |proj g|=    1.8046e-05
At iterate     6  f =       64.721  |proj g|=    1.0031e-05

iterations 6
function evaluations 7
segments explored during Cauchy searches 6
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 1.00307e-05
final function value 64.7214

F = 64.7214
final  value 64.721351
converged

optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~x + y + I(x^2) + I(y^2)
,* covariance model :
  - type :  matern5_2
  - nugget : 1e-06
  - parameters lower bounds :  1e-10 1e-10
  - parameters upper bounds :  38.38384 33.53535
  - variance bounds :  4739.433 516844.1
  - best initial criterion value(s) :  -67.35271

N = 3, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       67.353  |proj g|=       1.2091
At iterate     1  f =        66.15  |proj g|=       0.71586
At iterate     2  f =       65.413  |proj g|=       0.17312
At iterate     3  f =       65.343  |proj g|=      0.042875
At iterate     4  f =       65.339  |proj g|=     0.0045831
At iterate     5  f =       65.339  |proj g|=    9.0744e-05
At iterate     6  f =       65.339  |proj g|=    9.0756e-05

iterations 6
function evaluations 7
segments explored during Cauchy searches 6
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 9.07564e-05
final function value 65.3391

F = 65.3391
final  value 65.339062
converged

optimisation start
------------------
,* estimation method   : MLE
,* optimisation method : BFGS
,* analytical gradient : used
,* trend model : ~x + y + I(x^2) + I(y^2) + x:y
,* covariance model :
  - type :  matern5_2
  - nugget : 1e-06
  - parameters lower bounds :  1e-10 1e-10
  - parameters upper bounds :  38.38384 33.53535
  - variance bounds :  3.737058 568.1909
  - best initial criterion value(s) :  -45.0685

N = 3, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       45.069  |proj g|=      0.75398
At iterate     1  f =       42.139  |proj g|=       0.15773
At iterate     2  f =       34.359  |proj g|=      0.023091
At iterate     3  f =       34.039  |proj g|=      0.017983
At iterate     4  f =       33.862  |proj g|=    0.00015446
At iterate     5  f =       33.862  |proj g|=    0.00011581
At iterate     6  f =       33.862  |proj g|=    3.6625e-07
At iterate     7  f =       33.862  |proj g|=    8.6504e-10

iterations 7
function evaluations 16
segments explored during Cauchy searches 8
BFGS updates skipped 0
active bounds at final generalized Cauchy point 2
norm of the final projected gradient 8.65044e-10
final function value 33.8619

F = 33.8619
final  value 33.861945
converged
#+end_example

****** Computing Quality Assessment Metrics
#+begin_SRC R :results output :session *R* :eval no-export :exports results
print("const trend, LOO train MSE")
print(log10(mean((sampled$z_noisy -
                  leaveOneOut.km(const_trend_gp, "UK", FALSE)$mean) ^ 2)))
print("linear trend, LOO train MSE")
print(log10(mean((sampled$z_noisy -
                  leaveOneOut.km(linear_trend_gp, "UK", FALSE)$mean) ^ 2)))
print("quad trend, LOO train MSE")
print(log10(mean((sampled$z_noisy -
           leaveOneOut.km(quad_trend_gp, "UK", FALSE)$mean) ^ 2)))
print("interaction trend, LOO train MSE")
print(log10(mean((sampled$z_noisy -
           leaveOneOut.km(inter_trend_gp, "UK", FALSE)$mean) ^ 2)))

print("const trend, test MSE")
print(log10(mean((plot_df$z_noisy -
            predict(const_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))
print("linear trend, test MSE")
print(log10(mean((plot_df$z_noisy -
            predict(linear_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))
print("quad trend, test MSE")
print(log10(mean((plot_df$z_noisy -
            predict(quad_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))
print("interaction trend, test MSE")
print(log10(mean((plot_df$z_noisy -
            predict(inter_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))

print("const trend, true MSE")
print(log10(mean((plot_df$z -
            predict(linear_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))
print("linear trend, true MSE")
print(log10(mean((plot_df$z -
            predict(linear_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))
print("quad trend, true MSE")
print(log10(mean((plot_df$z -
            predict(quad_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))
print("interaction trend, true MSE")
print(log10(mean((plot_df$z -
            predict(inter_trend_gp, newdata = plot_df %>% select(x, y),
                    "UK", FALSE)$mean) ^ 2)))
#+end_SRC

#+RESULTS:
#+begin_example
[1] "const trend, LOO train MSE"
[1] 4.248826
[1] "linear trend, LOO train MSE"
[1] 4.480169
[1] "quad trend, LOO train MSE"
[1] 4.51261
[1] "interaction trend, LOO train MSE"
[1] 1.708737
[1] "const trend, test MSE"
[1] 4.421653
[1] "linear trend, test MSE"
[1] 4.7298
[1] "quad trend, test MSE"
[1] 4.403773
[1] "interaction trend, test MSE"
[1] 2.71315
[1] "const trend, true MSE"
[1] 4.726364
[1] "linear trend, true MSE"
[1] 4.726364
[1] "quad trend, true MSE"
[1] 4.396952
[1] "interaction trend, true MSE"
[1] 2.046654
#+end_example

****** 3D Noisy: Constant Trend Prediction
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy_gp_const.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    # xlim = c(-10, 10)
    # ylim = c(-10, 10)
    # zlim = c(-57.03081, 2578.93100)
    # xlim.scaled = c(-0.5, 0.5)
    # ylim.scaled = c(-0.5, 0.5)
    # zlim.scaled = c(-0.5, 0.5)

    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)

    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(const_prediction ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy_gp_const.pdf]]

****** 3D Noisy: Linear Prediction
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy_gp_linear.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    # xlim = c(-10, 10)
    # ylim = c(-10, 10)
    # zlim = c(-57.03081, 2578.93100)
    # xlim.scaled = c(-0.5, 0.5)
    # ylim.scaled = c(-0.5, 0.5)
    # zlim.scaled = c(-0.5, 0.5)

    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)

    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(linear_prediction ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy_gp_linear.pdf]]

****** 3D Noisy: Quadratic Prediction
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy_quadratic_gp.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    # xlim = c(-10, 10)
    # ylim = c(-10, 10)
    # zlim = c(-57.03081, 2578.93100)
    # xlim.scaled = c(-0.5, 0.5)
    # ylim.scaled = c(-0.5, 0.5)
    # zlim.scaled = c(-0.5, 0.5)

    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)

    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(quadratic_prediction ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy_quadratic_gp.pdf]]

****** 3D Noisy: Quadratic + Interactions Prediction
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_3d_noisy_quadratic_interactions_gp.pdf" :width 10 :height 10 :eval no-export
library(lattice)
library(akima)
library(RColorBrewer)
library(latex2exp)
library(paletteer)
library(dplyr)

new3d_frame <- function(x, y, z,
                        xlim, ylim, zlim,
                        xlim.scaled, ylim.scaled, zlim.scaled,
                        pts,
                        ...) {
    # xlim = c(-10, 10)
    # ylim = c(-10, 10)
    # zlim = c(-57.03081, 2578.93100)
    # xlim.scaled = c(-0.5, 0.5)
    # ylim.scaled = c(-0.5, 0.5)
    # zlim.scaled = c(-0.5, 0.5)

    panel.3dwire(x = x, y = y, z = z,
                 xlim = xlim,
                 ylim = ylim,
                 zlim = zlim,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 zlim.scaled = zlim.scaled,
                 col = NA,
                 ...)
    xx <- xlim.scaled[1] + diff(xlim.scaled) *
        (sampled$x - xlim[1]) / diff(xlim)
    yy <- ylim.scaled[1] + diff(ylim.scaled) *
        (sampled$y - ylim[1]) / diff(ylim)
    zz <- zlim.scaled[1] + diff(zlim.scaled) *
        (sampled$z_noisy - zlim[1]) / diff(zlim)

    panel.3dscatter(x = xx,
                    y = yy,
                    z = zz,
                    xlim = xlim,
                    ylim = ylim,
                    zlim = zlim,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    zlim.scaled = zlim.scaled,
                    col = colors[length(colors)],
                    type = "p",
                    ...)
}

colors = paletteer_c(palette = "viridis::viridis", n = 200, direction = -1)

wireframe(interaction_prediction ~ x * y,
          data = plot_df,
          xlab = list(TeX("$x_1$"), cex = 2.5),
          ylab = list(TeX("$x_2$"), cex = 2.5),
          zlab = list(TeX("$\\hat{f}_{\\theta}(x_1, x_2)$"),
                      rot = "90", cex = 2.5),
          colorkey = FALSE,
          col.regions = colors,
          drape = TRUE,
          perspective = TRUE,
          lattice.options = lattice.options(list(border = FALSE)),
          scales = list(arrows = FALSE, draw = FALSE),
          screen = list(z = 155, x = -70, y = 0),
          pts = sampled,
          panel.3d.wireframe = new3d_frame,
          par.box = list(col = colors[length(colors)]))
#+end_SRC

#+RESULTS:
[[file:./img/booth_3d_noisy_quadratic_interactions_gp.pdf]]

*** Experimental Design
**** Estimating Linear Effects
***** Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

compute_y <- function(xs,
                      beta = 1.2,
                      noise_sd = 0.2,
                      scedasticity = 0) {
    return((beta * xs) +
           rnorm(length(xs),
                 0,
                 noise_sd +
                 ((xs ^ 2) * scedasticity)))
}

samples = 5

resolution = 50

xs = seq(from = 0, to = 1, length.out = resolution)

sampled_xs = rep(xs, samples)

small_noise = 0.01
large_noise = 0.2
scedasticity = 1

min_close = (resolution / 2) - 1
max_close = (resolution / 2) + 1

min_far = 2
max_far = resolution - 1

generate_experiment <- function(noise_sd,
                                scedasticity,
                                id,
                                model_id,
                                x1,
                                x2) {
    df = data.frame(x = sampled_xs,
                    y = compute_y(sampled_xs,
                                  noise_sd = noise_sd,
                                  scedasticity = scedasticity),
                    id = id,
                    model_id = model_id)

    design_min = df %>%
        filter(x == xs[x1]) %>%
        filter(y == max(y)) %>%
        distinct(x, y, .keep_all = TRUE) %>%
        mutate(model_id = model_id)

    design_max = df %>%
        filter(x == xs[x2]) %>%
        filter(y == min(y)) %>%
        distinct(x, y, .keep_all = TRUE) %>%
        mutate(model_id = model_id)

    design = bind_rows(design_min,
                       design_max)

    predictions = data.frame(y = predict(lm(y ~ x,
                                            design),
                                         newdata = data.frame(x = xs)),
                             x = xs) %>%
        mutate(id = id,
               model_id = model_id)

    return(list(df, design, predictions))
}

experiment = generate_experiment(small_noise, 0,
                                 "small_noise", "close_points",
                                 min_close, max_close)

df = experiment[[1]]
design = experiment[[2]]
predictions = experiment[[3]]

experiment = generate_experiment(small_noise, 0,
                                 "small_noise", "distant_points",
                                 min_far, max_far)

df = df %>% bind_rows(experiment[[1]])
design = design %>% bind_rows(experiment[[2]])
predictions = predictions %>% bind_rows(experiment[[3]])

experiment = generate_experiment(large_noise, 0,
                                 "large_noise", "distant_points",
                                 min_far, max_far)

df = df %>% bind_rows(experiment[[1]])
design = design %>% bind_rows(experiment[[2]])
predictions = predictions %>% bind_rows(experiment[[3]])

experiment = generate_experiment(large_noise, 0,
                                 "large_noise", "close_points",
                                 min_close, max_close)

df = df %>% bind_rows(experiment[[1]])
design = design %>% bind_rows(experiment[[2]])
predictions = predictions %>% bind_rows(experiment[[3]])

experiment = generate_experiment(large_noise, scedasticity,
                                 "heteroscedastic", "distant_points",
                                 min_far, max_far)

df = df %>% bind_rows(experiment[[1]])
design = design %>% bind_rows(experiment[[2]])
predictions = predictions %>% bind_rows(experiment[[3]])

experiment = generate_experiment(large_noise, scedasticity,
                                 "heteroscedastic", "close_points",
                                 min_close, max_close)

df = df %>% bind_rows(experiment[[1]])
design = design %>% bind_rows(experiment[[2]])
predictions = predictions %>% bind_rows(experiment[[3]])


write.csv(df, "data/linear_effects/sample.csv", row.names = FALSE)
write.csv(design, "data/linear_effects/design.csv", row.names = FALSE)
write.csv(predictions, "data/linear_effects/predictions.csv", row.names = FALSE)
#+end_SRC

#+RESULTS:

***** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/experimental_design/lin_effects.pdf" :width 20 :height 30 :eval no-export
library(ggplot2)
library(latex2exp)

update_factors <- function(df) {
    return(df %>%
           mutate(model_id = factor(model_id,
                                    levels = c("close_points",
                                               "distant_points"),
                                    labels = c(TeX("Picking Close Points"),
                                               TeX("Picking Distant Points"))),
                  id = factor(id,
                              levels = c("small_noise",
                                         "large_noise",
                                         "heteroscedastic"),
                              labels = c(TeX("Smaller $\\epsilon$"),
                                         TeX("Larger $\\epsilon$"),
                                         TeX("Heteroscedastic")))))
}

df = update_factors(read.csv("data/linear_effects/sample.csv"))
design = update_factors(read.csv("data/linear_effects/design.csv"))
predictions = update_factors(read.csv("data/linear_effects/predictions.csv"))

ggplot() +
    geom_point(data = df,
               aes(x = x,
                   y = y),
               color = "gray48",
               size = 4,
               stroke = 2,
               alpha = 0.6,
               shape = 1) +
    geom_line(data = predictions,
              size = 3,
              alpha = 0.8,
              color = "red",
              aes(x = x,
                  y = y)) +
    geom_vline(data = predictions %>%
                   group_by(model_id, id) %>%
                   filter(y == min(y)),
               aes(xintercept = x),
               size = 2.2,
               alpha = 0.7,
               color = "red",
               linetype = 2) +
    geom_text(data = predictions %>%
                  group_by(model_id, id) %>%
                  filter(y == min(y)) %>%
                  ungroup() %>%
                  filter(as.numeric(id) == 1,
                         as.numeric(model_id) == 1),
              aes(x = x,
                  y = 1.8,
                  label = "Min. Prediction for X"),
              angle = 90,
              size = 10,
              vjust = 1.5,
              fontface = "bold",
              color = "red",
              stat = "unique") +
    geom_point(data = design,
               size = 6,
               color = "blue",
               aes(x = x,
                   y = y)) +
    ylim(min(df$y), max(df$y)) +
    facet_grid(id ~ model_id,
               labeller = label_parsed) +
    ylab(TeX("$f(\\mathbf{X}) + \\epsilon$")) +
    xlab(TeX("$\\mathbf{X}$")) +
    theme_bw(base_size = 40) +
    theme(strip.background = element_rect(fill = "transparent"),
          strip.text = element_text(size = 40),
          strip.text.y = element_text(angle = 90))
#+end_SRC

#+RESULTS:
[[file:./img/experimental_design/lin_effects.pdf]]

** Applications (Part III)
*** HAQ Experiments with NNs
**** ResNet50 Shrunken Structure
#+begin_SRC python :results output :session *Python* :eval no-export :exports results
import torch
import torch.nn as nn
from torchsummary import summary
import torchvision.models as models
import hiddenlayer as hl

network = "resnet50"

model = torch.hub.load('pytorch/vision:v0.6.0',
                       network,
                       pretrained = True)

transforms = [
    # Fold Conv, BN, RELU layers into one
    hl.transforms.Fold("Conv > BatchNorm > Relu", "ConvBnRelu"),
    # Fold Conv, BN layers together
    hl.transforms.Fold("Conv > BatchNorm", "ConvBn"),
    # Fold bottleneck blocks
    hl.transforms.Fold("""
        ((ConvBnRelu > ConvBnRelu > ConvBn) | ConvBn) > Add > Relu
        """, "BottleneckBlock", "Bottleneck Block"),
    # Fold residual blocks
    hl.transforms.Fold("""ConvBnRelu > ConvBnRelu > ConvBn > Add > Relu""",
                       "ResBlock", "Residual Block"),
    # Fold repeated blocks
    hl.transforms.FoldDuplicates(),
]

model_plot =  hl.build_graph(model,
                             torch.zeros([1, 3, 224,  224]),
                             transforms = transforms)

model_plot.save("./img/neural_net_autotuning/resnet50_architecture_generated.svg",
                format = "svg")
#+end_SRC

#+RESULTS:
: Python 3.9.2 (default, Feb 20 2021, 18:40:11)
: [GCC 10.2.0] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: Using cache found in /home/phrb/.cache/torch/hub/pytorch_vision_v0.6.0
: /usr/lib/python3.9/site-packages/torch/onnx/symbolic_helper.py:677: UserWarning: ONNX export mode is set to inference mode, but operator batch_norm is set to training  mode. The model will be exported in inference, as specified by the export mode.
:   warnings.warn("ONNX export mode is set to " + training_mode +
**** ResNet50 Full Structure
#+begin_SRC python :results output :session *Python* :eval no-export :exports results
import torch
import torch.nn as nn
from torchsummary import summary
import torchvision.models as models
import hiddenlayer as hl

network = "resnet50"

model = torch.hub.load('pytorch/vision:v0.6.0',
                       network,
                       pretrained = True)

model_plot =  hl.build_graph(model,
                             torch.zeros([1, 3, 224,  224]))

model_plot.save("./img/neural_net_autotuning/resnet50_full_architecture_generated.svg",
                format = "svg")
#+end_SRC

#+RESULTS:
: Using cache found in /home/phrb/.cache/torch/hub/pytorch_vision_v0.6.0
: /usr/lib/python3.9/site-packages/torch/onnx/symbolic_helper.py:677: UserWarning: ONNX export mode is set to inference mode, but operator batch_norm is set to training  mode. The model will be exported in inference, as specified by the export mode.
:   warnings.warn("ONNX export mode is set to " + training_mode +
* Table of Contents                                                  :ignore:
#+TOC: headlines 5
* Historical Hardware Design Trends and Consequences for Code Optimization
** Introduction
Computer  performance has  sustained exponential  increases over  the last  half
century,  despite   current  physical  limits  on   hardware  design.   Software
optimization  has  performed an  increasingly  substantial  role on  performance
improvement over the last 15 years, requiring the exploration of larger and more
complex search spaces than ever before.

Autotuning  methods  are one  approach  to  tackle performance  optimization  of
complex search  spaces, enabling exploitation of  existing relationships between
program  parameters and  performance.   We can  derive  autotuning methods  from
well-established  statistics,  although   their  usage  is  not   common  in  or
standardized for autotuning domains.

Initial work on this thesis studied  the effectiveness of classical and standard
search heuristics,  such as  Simulated Annealing,  on autotuning  problems.  The
first target autotuning domain was the set  of parameters of a compiler for CUDA
programs.  The search heuristics for this  case study were implemented using the
OpenTuner framework\nbsp\cite{ansel2014opentuner}, and  consisted of an ensemble
of  search  heuristics  coordinated  by a  Multi-Armed  Bandit  algorithm.   The
autotuner  searched  for a  set  of  compilation  parameters that  optimized  17
heterogeneous  GPU  kernels, from  a  set  of approximately  $10^{23}$  possible
combinations of all  parameters.  With 1.5h autotuning runs we  have achieved up
to  $4\times$  speedup  in  comparison   with  the  CUDA  compiler's  high-level
optimizations.   The  compilation  and  execution  times  of  programs  in  this
autotuning domain are relatively fast, and were in the order of a few seconds to
a minute.  Since measurement costs are relatively small, search heuristics could
find  good optimizations  using  as  many measurements  as  needed.  A  detailed
description      of      this      work       is      available      in      our
paper\nbsp\cite{bruel2017autotuning}   published   in   the   /Concurrency   and
Computation: Practice and Experience/ journal.

The  next  case  study  was developed  in  collaboration  with  /Hewlett-Packard
Enterprise/,  and consisted  of  applying the  same heuristics-based  autotuning
approach to the  configuration of parameters involved in the  generation of FPGA
hardware specification  from source  code in  the C  language, a  process called
/High-Level  Synthesis/ (HLS).   The  main  difference from  our  work with  GPU
compiler parameters  was the  time to obtain  the hardware  specification, which
could be in the order of hours for a single kernel.

In this  more complex  scenario, we  achieved up  to $2\times$  improvements for
different hardware metrics using  conventional search algorithms.  These results
were obtained in a simple HLS benchmark, for which compilation times were in the
order of  minutes.  The  search space was  composed of  approximately $10^{123}$
possible  configurations, which  is much  larger than  the search  space in  our
previous work with GPUs. Search space size and the larger measurement cost meant
that  we  did  not  expect  the  heuristics-based  approach  to  have  the  same
effectiveness   as  in   the   GPU   compiler  case   study.    This  work   was
published\nbsp\cite{bruel2017autotuninghls}  at  the  2017  /IEEE  International
Conference on ReConFigurable Computing and FPGAs/.

Approaches using  classical machine  learning and optimization  techniques would
not scale  to industrial-level  HLS, where  each compilation  can take  hours to
complete.  Search space properties also  increase the complexity of the problem,
in  particular  its  structure  composed of  binary,  factorial  and  continuous
variables with potentially complex interactions.   Our results on autotuning HLS
for  FPGAs  corroborate   the  conclusion  that  the   empirical  autotuning  of
expensive-to-evaluate functions, such as those  that appear on the autotuning of
HLS, require a more parsimonious  and transparent approach, that can potentially
be achieved using  the DoE methodology.  The next section  describes our work on
applying the DoE methodology to autotuning.

The  main contribution  of this  thesis is  a strategy  to apply  the Design  of
Experiments methodology to autotuning problems.  The strategy is based on linear
regression  and  its  extensions,  Analysis of  Variance  (ANOVA),  and  Optimal
Design. The strategy  requires the formulation of initial  assumptions about the
target autotuning problem, which are  refined with data collected by efficiently
selected experiments. The main objectives  are to identify relationships between
parameters, suggesting regions for further experimentation in a transparent way,
that is, in a  way that is supported by statistical  tests of significance.  The
effectiveness of the proposed strategy, and its ability to explain optimizations
it  finds,  are  evaluated  on   autotuning  problems  in  the  source-to-source
transformation domain. The initial stages of this work resulted in a publication
on IEEE/ACM CCGrid\nbsp{}\cite{bruel2019autotuning}.

Further efforts in this direction were  dedicated to describe precisely what can
be learned about  search spaces from the application of  the methodology, and to
refine  the  differentiation  of  the approach  for  the  sometimes  conflicting
objectives of model  assessment and prediction. These  discussions are presented
in Chapter\nbsp{}\ref{chap:laplacian}.

Because the  Design of Experiments  methodology requires the specification  of a
class of initial performance models, the methodology can sometimes achieve worse
prediction  capabilities  when  there  is considerable  uncertainty  on  initial
assumptions         about          the         underlying         relationships.
Chapter\nbsp{}\ref{chap:laplacian} and describe the application to autotuning of
Gaussian Process Regression, an approach  that trades some explanatory power for
a much  larger and more flexible  class of underlying models.   We also evaluate
the performance of this approach  on the source-to-source transformation problem
from Chapter\nbsp{}\ref{chap:laplacian}, and on larger autotuning problem on the
mixed-precision quantization of /Convolutional  Neural Network/ (CNN) layers, in
Chapter\nbsp{}\ref{chap:quantization}.

#+begin_export latex
\todo[inline]{Describe thesis structure}
#+end_export

*** Text Structure                                               :noexport:
The remainder of this thesis is organized as follows.  Chapter\nbsp{}[[Efforts for
Reproducible Science]] describes the efforts made over the duration of the work on
this thesis  to establish a  workflow that promotes reproducible  science, using
computational  documents  and  versioning   for  code,  results,  and  analyses.
Chapter\nbsp{}[[Methods  for Function  Minimization]]  presents  background for  the
derived  from  search  heuristics, mathematical  optimization,  and  statistical
learning,  and Chapter\nbsp{}[[Application  of  Function  Minimization Methods  to
Autotuning]]  to autotuning.   Chapter\nbsp{}[[A Design  of Experiments  Methodology]]
describes the DoE  methodology, and its application to  autotuning problems, and
the method  limitations encountered during this  thesis.  Chapter\nbsp{}[[Gaussian
Process  Regression:  A more  Flexible  Method]]  describes the  Gaussian  Process
Regression    method,   and    its   application    to   autotuning    problems.
Chapter\nbsp{}[[Conclusion]]  resumes and  concludes the  high-level discussions  of
this  chapter, in  light of  the  results of  the excursions  performed in  this
thesis.
** The Need for Autotuning
High Performance Computing  has been a cornerstone of  scientific and industrial
progress for at least five decades.  By paying the cost of increased complexity,
software  and  hardware  engineering   advances  continue  to  overcome  several
challenges on the way of  the sustained performance improvements observed during
the  last fifty  years.   A  consequence of  this  mounting  complexity is  that
reaching the theoretical peak hardware  performance for a given program requires
not only expert  knowledge of specific hardware architectures,  but also mastery
of programming models and languages for parallel and distributed computing.

If  we  state  performance  optimization  problems  as  /search/  or  /learning/
problems, by converting implementation and configuration choices to /parameters/
which might affect  performance, we can draw from and  adapt proven methods from
search, mathematical  optimization, and statistical learning.  The effectiveness
of these  adapted methods on  performance optimization problems  varies greatly,
and  hinges on  practical and  mathematical properties  of the  problem and  the
corresponding /search space/. The application  of such methods to the automation
of performance  tuning for specific hardware,  under a set of  /constraints/, is
named /autotuning/.

Improving performance  also relies on gathering  application-specific knowledge,
which entails extensive  experimental costs since, with the  exception of linear
algebra  routines,  theoretical  peak  performance is  not  always  a  reachable
comparison  baseline.   When  adapting  methods  for  autotuning  we  must  face
challenges emerging from practical properties,  such as restricted time and cost
budgets,  constraints  on  feasible  parameter  values,  and  the  need  to  mix
/categorical/,  /continuous/,  and  /discrete/ parameters.   To  achieve  useful
results we must also choose methods that make hypotheses compatible with problem
search  spaces,   such  as  the   existence  of  /discoverable/,  or   at  least
/exploitable/, relationships  between parameters  and performance.   Choosing an
autotuning  method requires  balancing the  exploration of  a problem,  that is,
seeking   to  discover   and  explain   relationships  between   parameters  and
performance, and the exploitation of  known or discovered relationships, seeking
only to find the best possible performance.

Search  algorithms  based  on  machine  learning heuristics  are  not  the  best
candidates for  autotuning domains  where measurements  are lengthy  and costly,
such as compiling industrial-level FPGA  programs, because these algorithms rely
on the  availability of a  large number of  measurements. They also  assume good
optimizations  are  reachable from  a  starting  position, and  that  tendencies
observed locally in the search space are exploitable.  These assumptions are not
usually true in common  autotuning domains, as shown in the  work of Seymour /et
al./\nbsp\cite{seymour2008comparison}.

Autotuning search spaces also usually  have non-linear constraints and undefined
regions, which are  also expected to decrease the effectiveness  of search based
on heuristics and  machine learning.  An additional downside  to heuristics- and
machine learning-based search  is that, usually, optimization  choices cannot be
explained, and knowledge  gained during optimization is not  reusable.  The main
contribution of this  thesis is to study  how to overcome the  reliance on these
assumptions about search spaces, and the lack of explainable optimizations, from
the point of view of a  /Design of Experiments/ (DoE), or /Experimental Design/,
methodology to autotuning.

One of  the first detailed  descriptions and  mathematical treatment of  DoE was
presented  by Ronald  Fisher\nbsp\cite{fisher1937design} in  his 1937  book /The
Design of Experiments/, where he  discussed principles of experimentation, latin
square  sampling and  factorial  designs.  Later  books such  as  the ones  from
Jain\nbsp\cite{bukh1992art}, Montgomery\nbsp\cite{montgomery2017design}  and Box
/et   al./\nbsp\cite{box2005statistics}  present   comprehensive  and   detailed
foundations.   Techniques based  on DoE  are /parsimonious/  because they  allow
decreasing   the  number   of   measurements  required   to  determine   certain
relationships  between parameters  and  metrics, and  are /transparent/  because
parameter  selections and  configurations can  be  justified by  the results  of
statistical tests.

In DoE terminology, a /design/ is a plan for executing a series of measurements,
or /experiments/, whose objective is to identify relationships between /factors/
and /responses/.   While factors and  responses can refer to  different concrete
entities in other domains, in  computer experiments factors can be configuration
parameters for algorithms  and compilers, for example, and responses  can be the
execution time or memory consumption of a program.

Designs  can  serve diverse  purposes,  from  identifying the  most  significant
factors  for  performance, to  fitting  analytical  performance models  for  the
response.  The  field of DoE  encompasses the mathematical formalization  of the
construction of experimental designs.  More practical works in the field present
algorithms to generate designs with different objectives and restrictions.

The contributions of  this thesis are strategies to apply  to program autotuning
the         DoE        methodology,         and        /Gaussian         Process
Regression/\nbsp{}\cite{williams2006gaussian}.  This  thesis presents background
and  a high-level  view  of  the theoretical  foundations  of  each method,  and
detailed  discussions of  the challenges  involved in  specializing the  general
definitions of search  heuristics and statistical learning  methods to different
autotuning problems, as well as what  can be /learned/ about specific autotuning
search spaces,  and how  that acquired  knowledge can  be leveraged  for further
optimization.

This  chapter aims  to substantiate  the claim  that autotuning  methods have  a
fundamental  role to  play on  the future  of program  performance optimization,
arguing  that the  value and  the difficulty  of the  efforts to  carefully tune
software became more apparent ever since advances in hardware stopped leading to
effortless performance improvements, at least from the programmer's perspective.
The following sections discuss the historical  context for the changes in trends
on  computer  architecture,  and  characterize  the  search  spaces  found  when
optimizing performance on different domains.

*** Historical Hardware Design Trends
The physical  constraints imposed  by technological  advances on  circuit design
were evident since  the first vacuum tube computers that  already spanned entire
floors,  such   as  the  ENIAC  in   1945\nbsp{}\cite{ceruzzi2003history}.   The
practical and  economical need to fit  more computing power into  real estate is
one  force for  innovation in  hardware design  that spans  its history,  and is
echoed in modern  supercomputers, such as the /Summit/ from  /Oak Ridge National
Laboratory/\nbsp{}\cite{olcf2020summit}, which spans an entire room.

#+NAME: fig:trends
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: 49 years of microprocessor data, highlighting the sustained exponential
#+CAPTION: increases and reductions on transistor counts and fabrication processes, the
#+CAPTION: stagnation of frequency scaling around 2005, and one solution found for it,
#+CAPTION: the simultaneous exponential increase on logical core count.
#+CAPTION: Data from Wikipedia\nbsp{}\cite{wiki2020transistor,wiki2020chronology}
[[file:img/49_years_processor_data.pdf]]

Figure\nbsp{}[[fig:trends]] highlights the unrelenting and so far successful pursuit
of smaller transistor fabrication processes, and the resulting capability to fit
more computing power on  a fixed chip area.  This trend  was already observed in
integrated      circuits     by      Gordon      Moore      /et     al./      in
1965\nbsp{}\cite{moore1965cramming},  who also  postulated its  continuity.  The
performance improvements produced by the design efforts to make Moore's forecast
a self-fulfilling  prophecy were  boosted until around  2005 by  the performance
gained from increases in circuit frequency.

Robert  Dennard /et  al./ remarked  in 1974\nbsp{}\cite{dennard1974design}  that
smaller  transistors, in  part  because they  generate  shorter circuit  delays,
decrease  the energy  required to  power  a circuit  and enable  an increase  in
operation  frequency without  breaking  power usage  constraints.  This  scaling
effect, named  /Dennard's scaling/,  is hindered  primarily by  leakage current,
caused    by     quantum    tunneling    effects    in     small    transistors.
Figure\nbsp{}[[fig:trends]] shows  a marked  stagnation on frequency  increase after
around 2005,  as transistors crossed  the 10^{2}nm fabrication process.   It was
expected that leakage  due to tunneling would limit  frequency scaling strongly,
even     before      the     transistor     fabrication      process     reached
10nm\nbsp{}\cite{frank2001device}.

Current hardware is now past the  effects of Dennard's scaling.  The increase in
logical cores around  2015 can be interpreted as preparation  for and mitigation
of the  end of frequency  scaling, and ushered in  an age of  multicore scaling.
Still, in order to meet power consumption constraints, up to half of a multicore
processor could have to be powered down, at all times.  This phenomenon is named
/Dark   Silicon/\nbsp{}\cite{esmaeilzadeh2011dark},  and   presents  significant
challenges        to        current         hardware        designers        and
programmers\nbsp{}\cite{venkataramani2015approximate,cheng2015core,henkel2015new}.

The   /Top500/\nbsp{}\cite{top5002020list}   list  gathers   information   about
commercially  available supercomputers,  and ranks  them by  performance on  the
/LINPACK/                            benchmark\nbsp{}\cite{dongarra2003linpack}.
Figure\nbsp{}[[fig:rmax-rpeak]] shows the peak  theoretical performance $RPeak$, and
the maximum performance achieved on the LINPACK benchmark $RMax$, in $Tflops/s$,
for the  top-ranked supercomputers on  TOP500.  Despite the  smaller performance
gains from  hardware design that are  to be expected for  post-Dennard's scaling
processors, the  increase in computer  performance has sustained  an exponential
climb, sustained mostly by software improvements.

Although /hardware  accelerators/ such as  GPUs and  FPGAs, have also  helped to
support exponential performance  increases, their use is not an  escape from the
fundamental  scaling  constraints  imposed   by  current  semiconductor  design.
Figure\nbsp{}[[fig:acc-cores]] shows the increase  in processor and accelerator core
count  on the  top-ranked  supercomputers  on Top500.   Half  of the  top-ranked
supercomputers in the  last decade had accelerator cores and,  of those, all had
around ten times more accelerator than processor cores.  The apparent stagnation
of  core  count in  top-ranked  supercomputers,  even considering  accelerators,
highlights the crucial impact software optimization has on performance.

#+NAME: fig:rmax-rpeak
#+CAPTION: Sustained exponential increase of theoretical \textit{RPeak} and
#+CAPTION: achieved \textit{RMax} performance for
#+CAPTION: the supercomputer ranked 1^{st} on TOP500\nbsp{}\cite{top5002020list}
#+ATTR_LATEX: :width \textwidth :placement [t]
[[file:./img/top500_rmax_rpeak.pdf]]

#+NAME: fig:acc-cores
#+CAPTION: Processor and accelerator core count
#+CAPTION: in supercomputers ranked 1^{st} on
#+CAPTION: TOP500\nbsp{}\cite{top5002020list}.
#+CAPTION: Core  count trends for supercomputers are not
#+CAPTION: necessarily bound to processor trends observed on
#+CAPTION: Figure\nbsp{}\ref{fig:trends}.
#+ATTR_LATEX: :width \textwidth :placement [h!]
[[file:./img/top500_accelerator_cores.pdf]]

Advances in hardware  design are currently not capable  of providing performance
improvements  via frequency  scaling  without dissipating  more  power than  the
processor was  designed to support,  which violates power constraints  and risks
damaging the circuit.  From the programmer's perspective, effortless performance
improvements from hardware  have not been expected for quite  some time, and the
key  to  sustaining  historical  trends  in  performance  scaling  has  lied  in
accelerators, parallel and distributed programming libraries, and fine tuning of
several stages of  the software stack, from instruction selection  to the layout
of neural networks.

The problem of optimizing software  for performance presents its own challenges.
The search  spaces that emerge from  autotuning problems grow quickly  to a size
for which  it would  take a  prohibitive amount  of time  to determine  the best
configuration by exhaustively evaluating  all possibilities. Although this means
we must  seek to decrease  the amount  of possibilities, by  restricting allowed
parameter values, or dropping parameters completely,  it is often unclear how to
decide  which parameters  should be  restricted or  dropped.  The  next sections
introduce a simple  autotuning problem, present an overview of  the magnitude of
the dimension  of autotuning  search spaces, and  briefly introduce  the methods
commonly used to explore search spaces, some of which are discussed in detail in
Chapter\nbsp{}[[Optimization Methods for Autotuning]].

*** Consequences for Code Optimization: Autotuning Loop Nest Optimization
Algorithms for linear  algebra problems are fundamental  to scientific computing
and statistics. Therefore,  decreasing the execution time of  algorithms such as
general  matrix multiplication  (GEMM)\nbsp{}\cite{dongarra1990set}, and  others
from the original BLAS\nbsp{}\cite{lawson1979basic},  is an interesting and well
motivated example, that we will use to introduce the autotuning problem.

One way to improve the performance of such linear algebra programs is to exploit
cache locality by  reordering and organizing loop iterations,  using source code
transformation methods  such as loop  /tiling/, or /blocking/,  and /unrolling/.
We  will now  briefly describe  loop tiling  and unrolling  for a  simple linear
algebra problem. After, we will discuss  an autotuning search space for blocking
and  unrolling  applied  to  GEMM,  and how  these  transformations  generate  a
relatively large and complex search space, which we can explore using autotuning
methods.

Figure\nbsp{}\ref{fig:transpose-c}  shows  three versions  of  code  in the  /C/
language that, given three square matrices $A$,  $B$, and $C$, computes $C = C +
A +  B^{\top}$. The first  optimization we can make  is to preemptively  load to
cache, or /prefetch/, as many as possible of the elements we know will be needed
at       any       given       iteration,       as       is       shown       in
Figure\nbsp{}\ref{fig:transpose-regular-access}. The shaded  elements on the top
row of Figure\nbsp{}[[fig:blocking-unrolling]] represent  the elements that could be
prefetched in iterations of Figure\nbsp{}\ref{fig:transpose-regular-access}.

Since /C/ matrices are stored in /row-major order/, each access of an element of
$B$  forces loading  the next  row elements,  even if  we explicitly  prefetch a
column  of $B$.   Since we  are  accessing $B$  in a  /column-major order/,  the
prefetched row  elements would not  be used  until we reached  the corresponding
column.  Therefore, the  next column  elements will  have to  be loaded  at each
iteration, considerably slowing down the computation.

# #+NAME: fig:blocking-unrolling
# #+ATTR_LATEX: :width \textwidth :placement [t]
# #+CAPTION: Access patterns on GEMM, for the \textit{destination} matrix, using
# #+CAPTION: loop  nest optimizations. Panel \textit{(a)} shows the access order
# #+CAPTION: of a naive implementation, panel \textit{(b)} shows the effect of
# #+CAPTION: loop \textit{tiling}, or \textit{blocking}, and panel \textit{(c)}
# #+CAPTION: shows the compounded effect of loop \textit{unrolling}
# [[file:./img/blocking_unrolling.pdf]]

#+begin_export latex
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      float A[N][N], B[N][N], C[N][N];
      int i, j;
      // Initialize A, B, C
      for(i = 0; i < N; i++){
        // Load line i of A to fast memory
        for(j = 0; j < N; j++){
          // Load C[i][j] to fast memory
          // Load column j of B to fast memory
          C[i][j] += A[i][j] + B[j][i];
          // Write C[i][j] to main memory
        }
      }
    \end{lstlisting}
    \caption{Regular implementation}
    \label{fig:transpose-regular-access}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, x, y;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          // Load block (i, j) of A to fast memory
          // Load block (j, i) of B to fast memory
          for(x = i; x < min(i + B_size, N); x++){
            for(y = j; y < min(j + B_size, N); y++){
              C[x][y] += A[x][y] + B[y][x];
            }
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Blocked, or tiled}
    \label{fig:transpose-blocked-tiled}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          // Load block (i, j) of A to fast memory
          // Load block (j, i) of B to fast memory
          C[i + 0][j + 0] += A[i + 0][j] * B[i][j + 0];
          C[i + 0][j + 1] += A[i + 0][j] * B[i][j + 1];
          // Unroll the remaining 12 iterations
          C[i + Bsize - 1][j + B_size - 2] += A[i + Bsize - 1][j] * B[i][j + B_size - 2];
          C[i + Bsize - 1][j + B_size - 1] += A[i + Bsize - 1][j] * B[i][j + B_size - 1];
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Tiled and unrolled}
    \label{fig:transpose-unrolling}
  \end{subfigure}
  \caption{Loop nest optimizations for $C = C + A + B^{\top}$, in C}
  \label{fig:transpose-c}
\end{figure}
#+end_export

#+NAME: fig:blocking-unrolling
#+ATTR_LATEX: :width .7\textwidth :placement [h!]
#+CAPTION: Access patterns for matrices in $C = C + A + B^{\top}$, with
#+CAPTION: loop  nest optimizations. Panel \textit{(a)} shows the access order
#+CAPTION: of a regular implementation, and panel \textit{(b)} shows the effect of
#+CAPTION: loop \textit{tiling}, or \textit{blocking}
[[file:./img/access_patterns.pdf]]

We  can  solve this  problem  by  reordering  memory  accesses to  request  only
prefetched elements. It  suffices to adequately split loop  indices into blocks,
as shown in Figure\nbsp{}\ref{fig:transpose-blocked-tiled}. Now, memory accesses
are   be   performed   in   /tiles/,   as   shown   on   the   bottom   row   of
Figure\nbsp{}[[fig:blocking-unrolling]].  If  blocks are  correctly sized to  fit in
cache, we  can improve performance  by explicitly prefetching each  tile.  After
blocking, we can still improve performance by /unrolling/ loop iterations, which
forces register  usage and helps  the compiler to  identify regions that  can be
/vectorized/.   A  conceptual  implementation  of loop  unrolling  is  shown  in
Figure\nbsp{}\ref{fig:transpose-unrolling}.

Looking at the  loop nest optimization problem from  the autotuning perspective,
the two /parameters/ that emerge from  the implementations are the /block size/,
which controls the stride, and the /unrolling factor/, which controls the number
of unrolled  iterations.  Larger block sizes  are desirable, because we  want to
avoid extra comparisons,  but blocks should be small enough  to ensure access to
as few as possible out-of-cache elements.  Likewise, the unrolling factor should
be large,  to leverage vectorization and  available registers, but not  so large
that it forces memory to the stack.

The values  of block size  and unrolling  factor that optimize  performance will
depend on the  cache hierarchy, register layout,  and vectorization capabilities
of the  target processor, but  also on the memory  access pattern of  the target
algorithm.   In  addition  to  finding   the  best  values  for  each  parameter
independently, an autotuner  must ideally aim to account  for the /interactions/
between parameters, that is, for the fact that the best value for each parameter
might also depend on the value chosen for the other.

The    next    loop    optimization    example   comes    from    Seymour    /et
al./\nbsp{}\cite{seymour2008comparison},   and   considers  128   blocking   and
unrolling values, in the interval $[0,127]$,  for the GEMM algorithm.  The three
panels of Figure\nbsp{}\ref{fig:gemm-c} show  conceptual implementations of loop
blocking and  unrolling for GEMM in  /C/.  A block  size of zero results  in the
implementation  from  Figure\nbsp{}\ref{fig:regular-access},  and  an  unrolling
factor of zero performs a single iteration per condition check.

#+begin_export latex
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      float A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i++){
        // Load line i of A to fast memory
        for(j = 0; j < N; j++){
          // Load C[i][j] to fast memory
          // Load column j of B to fast memory
          for(k = 0; k < N; k++){
            C[i][j] += A[i][k] * B[k][j];
          }
          // Write C[i][j] to main memory
        }
      }
    \end{lstlisting}
    \caption{Regular implementation}
    \label{fig:regular-access}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k, x, y;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          for(k = 0; k < N; k++){
            // Load block (i, k) of A to fast memory
            // Load block (k, y) of B to fast memory
            for(x = i; x < min(i + B_size, N); x++){
              for(y = j; y < min(j + B_size, N); y++){
                C[x][y] += A[x][k] * B[k][y];
              }
            }
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Blocked, or tiled}
    \label{fig:blocked-tiled}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          for(k = 0; k < N; k++){
            // Load block (i, k) of A to fast memory
            // Load block (k, y) of B to fast memory
            C[i + 0][j + 0] += A[i + 0][k] * B[k][j + 0];
            C[i + 0][j + 1] += A[i + 0][k] * B[k][j + 1];
            // Unroll the remaining 12 iterations
            C[i + Bsize - 1][j + B_size - 2] += A[i + Bsize - 1][k] * B[k][j + B_size - 2];
            C[i + Bsize - 1][j + B_size - 1] += A[i + Bsize - 1][k] * B[k][j + B_size - 1];
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Tiled and unrolled}
    \label{fig:unrolling}
  \end{subfigure}
  \caption{Loop nest optimizations for GEMM, in C}
  \label{fig:gemm-c}
\end{figure}
#+end_export

#+NAME: fig:search-space
#+ATTR_LATEX: :width .6\textwidth :placement [h!]
#+CAPTION: An exhaustively measured search space, defined by loop blocking and unrolling
#+CAPTION: parameters, for a sequential GEMM kernel.
#+CAPTION: Reproduced from Seymour \textit{et al.}\nbsp{}\cite{seymour2008comparison}
[[file:img/seymour2008comparison.pdf]]

It  is straightforward  to change  the block  size of  the implementations  from
Figure\nbsp{}\ref{fig:gemm-c},  but the  unrolling factor  is not  exposed as  a
parameter.  To test different unrolling values  we need to generate new versions
of the  source code with  different numbers of  unrolled iterations.  We  can do
that   with   code   generators  or   with   /source-to-source   transformation/
tools\nbsp{}\cite{videau2017boast,hartono2009annotation,ansel2009petabricks}.
It is  often necessary to  modify the  program we wish  to optimize in  order to
provide a configuration  interface and expose its implicit  parameters.  Once we
are able to control  the block size and the loop  unrolling factor, we determine
the target search space by choosing the values to be explored.

In this  example, the search  space is defined by  the $128^2 =  16384$ possible
combinations  of  blocking  and  unrolling  values.   The  performance  of  each
combination    in    the    search     space,    shown    in    /Mflops/s/    in
Figure\nbsp{}[[fig:search-space]],    was   measured    for   a    sequential   GEMM
implementation,        using        square        matrices        of        size
400\nbsp{}\cite{seymour2008comparison}. We can  represent this autotuning search
space  as a  /3D landscape/,  since we  have two  configurable parameters  and a
single target performance metric.  In this setting, the objective is to find the
/highest/ point, since the objective is to /maximize/ Mflops/s, although usually
the  performance   metric  is   transformed  so  that   the  objective   is  its
/minimization/.

On a first look, there seems to  be no apparent global search space structure in
the landscape on  Figure\nbsp{}[[fig:search-space]], but local features  jump to the
eyes, such as  the ``valley'' across all block sizes  for low unrolling factors,
the ``ramp'' across all unrolling factors for low block sizes, and the series of
jagged  ``plateaus'' across  the middle  regions, with  ridges for  identical or
divisible block sizes  and unrolling factors.  A careful look  reveals also that
there is  a curvature  along the  unrolling factor  axis.  Also  of note  is the
abundance in this  landscape of /local minima/, that is,  points with relatively
good performance, surrounded  by points with worse  performance. By exhaustively
evaluating  all  possibilities, the  original  study  determined that  the  best
performance  on this  program  was achieved  with  a  block size  of  80 and  an
unrolling factor of 2.

In this  conceptual example,  all ${\approx}1.64\times10^4$  configurations were
exhaustively evaluated,  but it is  impossible to do  so in most  settings where
autotuning methods are  useful.  The next section provides a  perspective of the
autotuning  domains  and methods  employed  in  current research,  presenting  a
selection of  search spaces and  discussing the trends  that can be  observed on
search space size, targeted HPC domains, and chosen optimization methods.

*** Autotuning Approaches and Search Spaces
Autotuning  methods have  been used  to improve  performance in  an increasingly
large  variety of  domains,  from  the earlier  applications  to linear  algebra
subprograms,  to the  now ubiquitous  construction and  configuration of  neural
networks,  to the  configuration  of  the increasingly  relevant  tools for  the
re-configurable hardware  of FPGAs.  In this  setting, it is not  far-fetched to
establish a  link between  the continued increases  in performance  and hardware
complexity, that  we discussed previously in  this chapter, to the  increases in
dimension and size of the autotuning problems that we can now tackle.

Figure\nbsp{}[[fig:search-spaces]] presents search space  dimension, measured as the
number of  parameters involved,  and size,  measured as  the number  of possible
parameter  combinations, for  a selection  of search  spaces from  14 autotuning
domains.   Precise  information about  search  space  characterization is  often
missing from works on autotuning  methods and applications. The characterization
of  most of  the search  spaces in  Figure\nbsp{}[[fig:search-spaces]] was  obtained
directly from the text of the corresponding published paper, but for some it was
necessary to extract  characterizations from the available  source code.  Still,
it was impossible  to obtain detailed descriptions of search  spaces for many of
the published works on autotuning methods  and applications, and in that way the
sample shown in this section is  biased, because it contains only information on
works that provided it.

#+NAME: fig:search-spaces
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Dimension and search space size for autotuning problems from 14 domains
#+CAPTION: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,abdelfattah2016performance,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}
#+CAPTION: The left panel shows a zoomed view of the right panel
[[file:img/search_spaces.pdf]]

The left hand  panel of Figure\nbsp{}[[fig:search-spaces]] shows  search spaces with
up to 60 parameters. The over-representation of search spaces for linear algebra
domains in this sample stands out on the  left hand panel, but the domain is not
present on the remaining  portion of the sample, shown on  the right hand panel.
The  largest  search spaces  for  which  we were  able  to  find information  on
published  work are  defined for  the domains  of neural  network configuration,
High-Level  Synthesis  for  FPGAs,   compiler  parameters,  and  domain-specific
languages.

None of the largest search spaces in  this sample, that is, the ones outside the
zoomed area  of the  left hand  panel, come  from works  earlier than  2009. The
sustained  performance improvements  we  discussed previously  have enabled  and
pushed autotuning research toward progressively  larger problems, which has also
been  done to  most  research  areas.  Increased  computing  power  has made  it
feasible,  or at  least tolerable,  to apply  search heuristics  and statistical
learning methods to find program configurations that improve performance.

It is  straightforward to  produce an extremely  large autotuning  search space.
Compilers have  hundreds of binary flags  that can be considered  for selection,
generating  a large  set of  combinations. Despite  that, regarding  performance
improvements, it is likely that most  configuration parameters will have a small
impact, that is,  that only a handful of parameters  are responsible for changes
in performance.  Search  spaces are often much more restrictive  than the one we
discussed in  Section\nbsp{}[[Consequences for Code Optimization:  Autotuning Loop
Nest   Optimization]].   Autotuning   problem   definitions   usually  come   with
/constraints/  on  parameter  values  and limited  /experimental  budgets/,  and
/runtime/ failures  for some  configurations are  often unpredictable.   In this
context,  finding configurations  that improve  performance and  determining the
subset of /significant parameters/ are considerable challenges.

Search  heuristics, such  as methods  based on  genetic algorithms  and gradient
descent, are  a natural way to  tackle these challenges because  they consist of
procedures for exploiting existing  and unknown relationships between parameters
and  performance without  making or  requiring /explicit  hypotheses/ about  the
problem.  Despite that, most commonly used heuristics make /implicit hypotheses/
about search  spaces which are not  always verified, such as  assuming that good
configurations are /reachable/ from a random starting point.

#+begin_SRC R :results output latex :session *R* :eval no-export :exports results
library(knitr)
library(kableExtra)
library(dplyr)

df <- read.csv("data/search_spaces/search_methods.csv")

df <- df %>%
    mutate(citation_name = paste(name,
                                 "~\\cite{",
                                 gscholar_citation,
                                 "}",
                                 sep = "")) %>%
    select(citation_name,
           problem_domain,
           shorthand_method,
           year) %>%
    arrange(year)

names(df) <- c("System", "Domain", "Method", "Year")

caption <- paste("Autotuning methods used by a sample of systems,",
                 "in different domains, ordered by publishing year.",
                 "Methods were classified as either Search Heuristics (SH),",
                 "Machine Learning (ML), or more precisely",
                 "when the originating work provided detailed information.",
                 "Earlier work favored employing Search Heuristics, which",
                 "are less prominent in recent work, which favors methods",
                 "based on Machine Learning.")

kable(df,
      format = "latex",
      escape = FALSE,
      booktabs = TRUE,
      linesep = "",
      label = "search-methods",
      caption = caption) %>%
    kable_styling(latex_options = c("scale_down", "hold_position"))
#+end_SRC

#+RESULTS:
#+begin_export latex

\begin{table}[!h]

\caption{\label{tab:search-methods}Autotuning methods used by a sample of systems, in different domains, ordered by publishing year. Methods were classified as either Search Heuristics (SH), Machine Learning (ML), or more precisely when the originating work provided detailed information. Earlier work favored employing Search Heuristics, which are less prominent in recent work, which favors methods based on Machine Learning.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lllr}
\toprule
System & Domain & Method & Year\\
\midrule
PhiPAC~\cite{bilmes1997optimizing} & Linear Algebra & SH (Exhaustive) & 1997\\
ATLAS~\cite{dongarra1998automatically} & Linear Algebra & SH (Exhaustive) & 1998\\
FFTW~\cite{frigo1998fftw} & Digital Signal Processing & SH (Exhaustive) & 1998\\
Active Harmony~\cite{tapus2002active} & Domain-Specific Language & SH & 2002\\
OSKI~\cite{vuduc2005oski} & Linear Algebra & SH & 2005\\
Seymour, K. \textit{et al.}~\cite{seymour2008comparison} & Linear Algebra & SH & 2008\\
PRO~\cite{tiwari2009scalable} & Linear Algebra & SH & 2009\\
ParamILS~\cite{hutter2009paramils} & Combinatorial Auctions & SH & 2009\\
PetaBricks~\cite{ansel2009petabricks} & Domain-Specific Language & SH (Genetic Algorithm) & 2009\\
MILEPOST GCC~\cite{fursin2011milepost} & Compiler Parameters & ML & 2011\\
Orio~\cite{balaprakash2012spapt} & Linear Algebra & ML (Decision Trees) & 2012\\
pOSKI~\cite{byun2012autotuning} & Linear Algebra & SH & 2012\\
INSIEME~\cite{jordan2012multi} & Compiler Parameters & SH (Genetic Algorithm) & 2012\\
OpenTuner~\cite{ansel2014opentuner} & Compiler Parameters & SH & 2014\\
Lgen~\cite{spampinato2014basic} & Linear Algebra & SH & 2014\\
OPAL~\cite{audet2014optimization} & Parallel Computing & SH & 2014\\
Mametjanov, A. \textit{et al.}~\cite{mametjanov2015autotuning} & High-Level Synthesis & ML (Decision Trees) & 2015\\
CLTune~\cite{nugteren2015cltune} & Parallel Computing & SH & 2015\\
Guerreirro, J. \textit{et al.}~\cite{guerreiro2015multi} & Parallel Computing & SH & 2015\\
Collective Mind~\cite{fursin2015collective} & Compiler Parameters & ML & 2015\\
Abdelfattah, A. \textit{et al.}~\cite{abdelfattah2016performance} & Linear Algebra & SH (Exhaustive) & 2016\\
TANGRAM~\cite{chang2016efficient} & Domain-Specific Language & SH & 2016\\
MASE-BDI~\cite{coelho2016mase} & Environmental Land Change & SH & 2016\\
Xu, C. \textit{et al.}~\cite{xu2017parallel} & High-Level Synthesis & SH & 2017\\
Apollo~\cite{beckingsale2017apollo} & Parallel Computing & ML (Decision Trees) & 2017\\
DeepHyper~\cite{balaprakash2018deephyper} & Neural Networks & ML (Decision Trees) & 2018\\
Tuzov, I. \textit{et al.}~\cite{tuzov2018tuning} & High-Level Synthesis & Design of Experiments & 2018\\
Periscope~\cite{gerndt2018multi} & Compiler Parameters & SH & 2018\\
SynTunSys~\cite{ziegler2019syntunsys} & High-Level Synthesis & SH & 2019\\
Kwon, J. \textit{et al.}~\cite{kwon2019learning} & High-Level Synthesis & ML & 2019\\
FuncyTuner~\cite{wang2019funcytuner} & Compiler Parameters & SH & 2019\\
Ol’ha, J. \textit{et al.}~\cite{olha2019exploiting} & Parallel Computing & Sensitivity Analysis & 2019\\
Petrovic, F. \textit{et al.}~\cite{petrovivc2020benchmark} & Linear Algebra & SH & 2020\\
Chu, Y. \textit{et al.}~\cite{chu2020improving} & Search/MVWCP & SH & 2020\\
\bottomrule
\end{tabular}}
\end{table}
#+end_export

Autotuning methods that make explicit  hypotheses about the target program, such
as methods  based on Design of  Experiments, require some initial  knowledge, or
willingness to make assumptions, about  underlying relationships, and are harder
to  adapt  to   constrained  scenarios,  but  have  the   potential  to  produce
/explainable/ optimizations.  In general, methods based on Machine Learning have
enough  flexibility to  perform  well  in complex  search  spaces  and make  few
assumptions about problems, but usually provide little, if any, that can be used
to explain optimization  choices or derive relationships  between parameters and
performance.

Table\nbsp{}\ref{tab:search-methods} lists some autotuning systems, their target
domains, and  the employed  method, ordered by  publication date.   Some systems
that  did not  provide  detailed  search space  descriptions  and  could not  be
included in Figure\nbsp{}[[fig:search-spaces]], especially some of the earlier work,
provided enough information to categorize their autotuning methods. In contrast,
many  more  recent  works,  especially  those using  methods  based  on  Machine
Learning, did not provide specific  method information. Earlier work often deals
with  search spaces  small enough  to  exhaustively evaluate,  and using  search
heuristics to optimize linear algebra programs is the most prominent category of
earlier work in this sample.  Later  autotuning work target more varied domains,
with  the  most prominent  domains  in  this  sample being  parallel  computing,
compiler parameters, and  High-Level Synthesis.  Systems using  methods based on
Machine Learning become more common on later work than systems using heuristics.

Chapter\nbsp{}[[Methods   for  Function   Minimization]]   provides  more   detailed
definitions and discussions of the applicability, effectiveness, and explanatory
power  of  stochastic  autotuning  methods  based  on  search  heuristics.   The
remainder  of this  chapter details  the contributions  of this  thesis and  the
structure of this document.

#+begin_export latex
\todo[inline]{Third Chapter, or  next Section: Proposal of this  thesis, list of
  contributions}
#+end_export

* Optimization Methods for Autotuning
** Notation and Search Spaces
The  following chapters  discuss the  optimization  methods that  we applied  to
autotuning  problems in  different domains.   Each chapter  presents a  group of
methods,  briefly discussing  each  method  in the  group  and their  underlying
hypotheses.   The  objective  of  each  chapter is  to  provide  high-level  and
straightforward   descriptions  of   optimization   methods,  presenting   clear
definitions tied to the autotuning context.

Each chapter concludes  with a discussion of the applicability  of each group of
methods  to  autotuning  problems.   The methods  we  discuss  have  significant
differences  but  employ  the  same  basic  concepts.   We  will  use  the  same
mathematical notation  to discuss all  methods when possible.  The  remainder of
this chapter  presents common basic concepts  and the associated notation  to be
used     in     subsequent     chapters,     which     are     summarized     in
Table\nbsp{}[[tab:notation]]. Completing  this introduction  to optimization  in the
context of autotuning, the chapter ends with a discussion of common search space
properties.

*** Notation
We will  call /optimization/ the minimization  of a real-valued function  with a
single vector input.  For a function $f: \mathcal{X} \to \mathbb{R}$, we wish to
find  the input  vector  $\mathbf{x}^{\ast} =  [x_1\;\dots\;x_p]^{\top}$ in  the
/parameter    space/,    or    /search   space/    $\mathcal{X}$    for    which
$f(\mathbf{x}^{\ast})$ is  the smallest, compared  to all other  $\mathbf{x} \in
\mathcal{X}$.   The function  $f$  represents, in  the  autotuning context,  the
/performance metric/  we wish to  optimize, such as  the execution time  of some
application,  and  the  parameter  space $\mathcal{X}$  represents  the  set  of
possible /configurations/ we can explore, such as compiler flags.  Therefore, we
define optimization in the autotuning  context as finding the configuration that
minimizes the target  performance metric. For the sake of  simplicity, we assume
we can use the opposite of the performance metrics that should be maximized.

As an example of an autotuning  problem, consider optimizing the choice of flags
for a compiler with $p  = \vert{}\mathbf{x}\vert{}$ flags, where a configuration
$\mathbf{x} = [x_1 \;\dots\; x_p]^{\top}$ consists  of a vector of $p$ /boolean/
values,  denoting whether  each flag  $x_1,  \dots, x_p$  is turned  on for  the
compilation of a specific application.   To find the compiler configuration that
generates the binary with the smallest execution  time, we conduct a set of $n =
\vert{}\mathbf{X}\vert{}$  /experiments/, chosen  according  to some  criterion,
generating  the   $n\times{}p$  /experimental   design  matrix/   $\mathbf{X}  =
[\mathbf{x}_1  \;\dots\;  \mathbf{x}_n]^{\top}$.   Each experiment  consists  of
compiling the target application using the specified compiler configuration, and
measuring the execution time of the resulting binary.

In  this example,  evaluating $f(\mathbf{x})$  involves generating  the compiler
configuration corresponding to the vector  $\mathbf{x}$ of selected flags.  This
involves writing a shell command or a configuration file, running the configured
compiler, checking for  compilation errors, measuring the execution  time of the
resulting binary, and verifying the correctness of its output.

In  practice,   we  may  never   be  able  to   observe  the  /true   value/  of
$f(\mathbf{x})$. In fact,  empirical tests of this nature are  always subject to
unknown or uncontrollable  effects, and to inherent  imprecision in measurement.
In practice,  we settle for observing  $y = f(\mathbf{x}) +  \varepsilon$, where
$\varepsilon$ encapsulates  all unknown and  uncontrollable effects, as  well as
the measurement error.  Returning to the  compiler flag example, suppose that we
could conduct $n = \vert{}\mathcal{X}\vert{} = 2^{p}$ experiments, measuring the
performance of the binaries generated with all possible flag combinations.  With
such experimental design we would obtain the measurements
#+begin_export latex
\begin{align}
  \mathbf{y} =  [y_i = f(\mathbf{x}_i)  +
    \varepsilon_i, \; i =  1,\dots,2^{p}]^{\top}\text{.}
\end{align}
#+end_export
The  measurement  $y_i$  is  an  /estimate/  of  $f(\mathbf{x}_i)$,  with  error
$\varepsilon_i$.  If the  error is reasonably small, an estimate  of the /global
optimum/ $\mathbf{x}^{\ast}$ in this example is the $\mathbf{x}_i$ that produces
the binary with  the smallest estimated execution time  $y^{\ast}$, the smallest
$y_i \in \mathbf{y}$.

Assuming we are capable of cheaply evaluating $f$ for a large set of experiments
$\mathbf{X}$, and that we are not  interested in building statistical models for
the  performance  of  our  application,  we  can  directly  optimize  $f$  using
stochastic descent  methods, or  gradient- and hessian-based  methods if  $f$ is
suitably  convex and  differentiable.  These  function minimization  methods are
discussed in Chapter\nbsp{}[[Methods for Function Minimization]].

If we are not capable of directly measuring $f$, if it is unreasonably expensive
or time-consuming  to do so,  or if constructing statistical  performance models
for our application is of crucial  importance, we can employ the surrogate-based
methods   discussed   in   Chapters\nbsp{}[[Learning:   Building   Surrogates]],
\ref{chap:ed}, and [[Online Learning]].   These methods use different strategies
to construct a /surrogate model/
#+begin_export latex
\begin{align}
  \hat{f}_{\theta}: \mathcal{X} \to \mathbb{R}\text{, with }
  \theta \in \Theta\text{,}
\end{align}
#+end_export
where  $\theta$  is  a  parameter vector  usually  estimated  from  measurements
$(\mathbf{X},\mathbf{y})$.   The  function   $\theta  :  \mathcal{X}^{n}  \times
\mathbb{R}^{n} \to \Theta$ represents the  /estimation process/, that uses the
/observations/  $\mathbf{y}  \in  \mathbb{R}^{n}$   from  an  experimental  design
$\mathbf{X} =  [\mathbf{x}_1 \;\dots\; \mathbf{x}_n]^{\top}$.  The  /parameter vector/
$\theta(\mathbf{X},\mathbf{y})$ in the /parameter space/  $\Theta$ will be used to
compute the estimate.

The constructed surrogate model $\hat{f}_{\theta(\mathbf{X},\mathbf{y})}$ can be
used as a tool to attempt to  describe and optimize the underlying real function
$f$,   provided   we   are   able    to   construct   a   useful   estimate   of
$\theta(\mathbf{X},\mathbf{y})$.  We will  discuss in Chapters\nbsp{}[[Learning:
Building  Surrogates]]   and\nbsp{}\ref{chap:ed}  methods   that  use   the  $m$
individual  parameter  estimates  $\theta(\mathbf{X},  \mathbf{y})  =  [\theta_1
\;\dots\; \theta_m]^{\top}$ to assess the significance of specific /factors/.

If  we use  the  Ordinary Least  Squares  (OLS) estimator  for  $\theta$ in  our
compiler flag  example, the parameter estimates  $\theta_2,\dots,\theta_m$ could
correspond  to each  one of  the $p$  flags.  In  this case,  $\theta_1$ is  the
estimate of  the intercept of the  linear model, and  $m = p +  1$. Optimization
methods  using  the  Bayesian  inference framework,  such  as  Gaussian  Process
Regression,  which we  discuss in  Chapter\nbsp{}[[Learning: Building  Surrogates]],
associate a  probability distribution to  $\theta(\mathbf{X},\mathbf{y})$, which
propagates  with $\hat{f}_{\theta}$  and can  be exploited  in our  optimization
context.

Chapter\nbsp{}[[Learning:  Building Surrogates]]  differentiates between  parametric
methods which make the hypothesis that  the number $m = \vert{}\theta\vert{}$ of
estimated  parameters  is  finite  and often  interpretable,  and  nonparametric
methods, which operate in parameter spaces of infinite dimension.

Table\nbsp{}[[tab:notation]] summarizes the notation  and concepts we have discussed
so  far, tying  those concepts  to  the compiler  flag  example we  used in  the
discussion.   The notation  and the  basic concepts  we have  described in  this
section, although referring to abstract entities, enable a uniform discussion of
different optimization methods in the next chapters.

#+NAME: tab:notation
#+CAPTION: Summary of the notation, concepts, and examples discussed in this chapter,
#+CAPTION: and common to the autotuning methods discussed in further chapters
#+ATTR_LATEX: :align p{.24\columnwidth}p{.195\columnwidth}p{.475\columnwidth} :booktabs t :font \scriptsize
|-------------------------------------------------------------------------------------------------------------+---------------------------------+----------------------------------------------------------------------|
| Symbol                                                                                                      | Concept                         | Example                                                              |
|-------------------------------------------------------------------------------------------------------------+---------------------------------+----------------------------------------------------------------------|
| $\mathcal{X}$                                                                                               | Search space                    | All possible compiler flag assignments                               |
| $\mathbf{x} = [x_1 \;\dots\; x_p]^{\top} \in \mathcal{X}$                                                   | Input variable                  | A specific flag assignment                                           |
| $\mathbf{X} = [\mathbf{x}_1 \;\dots\; \mathbf{x}_n]^{\top} \subseteq \mathcal{X}$                           | Experimental design             | A set of flag assignments                                            |
| $p = \vert{}\mathbf{x}\vert{}$                                                                              | Search space dimension          | The number of flags to assign                                        |
| $n = \vert{}\mathbf{X}\vert{}$                                                                              | Number of experiments           | Size of the set of flags to compile and measure                      |
| $f: \mathcal{X} \to \mathbb{R}$                                                                             | Function to minimize            | Performance metric, such as the execution time of a binary           |
| $y_i = f(\mathbf{x}_i) + \varepsilon_i$                                                                     | Observable quantity             | Execution time with flags $\mathbf{x}_i$, with error $\varepsilon_i$ |
| $\mathbf{y} = [y_1 \;\dots\; y_n]^{\top} \in \mathbb{R}^{n}$                                                | Observations                    | List of execution times for all flags                                |
| $\mathbf{x}^{\ast}: y^{\ast} = f(\mathbf{x}^{\ast}) \leq f(\mathbf{x}), \forall \mathbf{x} \in \mathcal{X}$ | Global optimum                  | Flag assignment with smallest execution time                         |
| $\Theta$                                                                                                    | Parameter space                 | All possible values of the coefficients of a linear model            |
| $\theta(\mathbf{X}, \mathbf{y}) \in  \Theta$                                                                | Parameter vector                | OLS estimate of the coefficients of a linear model                   |
| $m = \vert{}\theta\vert{}$                                                                                  | Number of parameters            | Number of OLS coefficient estimates                                  |
| $\hat{f}_{\theta}: \mathcal{X} \to \mathbb{R}$                                                              | Surrogate for $f$               | A linear model fit used for /predictions/                            |
| $\hat{y} = \hat{f}_{\theta}(\mathbf{x})$                                                                    | Estimate of $f$ at $\mathbf{x}$ | A prediction of execution time for a flag assignment       |
|-------------------------------------------------------------------------------------------------------------+---------------------------------+----------------------------------------------------------------------|

Before moving on to the descriptions of derivative-based and stochastic function
minimization methods and their application to autotuning problems, we discuss in
the next section some  of the properties of search spaces  that are relevant for
both autotuning and mathematical optimization.

*** Search Spaces
Consider a more  abstract optimization problem than the  compiler flag selection
from  the  last  section,  consisting  of finding  the  global  optimum  of  the
paraboloid surface defined by the Booth function,
#+begin_export latex
\begin{align}
y_a = f(\mathbf{x} = [x_1, x_2]^{\top}) = (x_1 + 2x_2 - 7)^{2} + (2x_1 + x_2 - 5)^{2}, \; x_1,x_2 \in [-10, 10]\text{.} \label{eq:f0}
\end{align}
#+end_export
In our notation, the search space for  this example is $\mathcal{X} = (x_1, x_2)
\in \mathbb{R}^{2},  \; x_1,x_2 \in  [-10,10]$, with global optimum  $y^{\ast} =
f(\mathbf{x}^{\ast} = [1, 3]) = 0$.

#+NAME: fig:simple-search-spaces
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Contour plots and slices through the global optimum, marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$, for search spaces
#+CAPTION: defined by variations of the Booth function.
#+CAPTION: Panels (a) and (d) correspond to Equation\nbsp{}\ref{eq:f0}, panels
#+CAPTION: (b) and (e) to Equation\nbsp{}\ref{eq:f1}, and panels (c) and (f)
#+CAPTION: to Equation\nbsp{}\ref{eq:f2}.
[[file:img/simple_search_space.pdf]]

Besides the  search space defined  by the  observations $y_a$, we  will consider
search spaces for two variations
#+begin_export latex
\begin{align}
y_b &= f(\mathbf{x}) + \varepsilon, \; \text{with } \varepsilon \thicksim \mathcal{N}(0, \sigma^2)\text{, and} \label{eq:f1} \\
y_c &= f(\mathbf{x}) + \varepsilon, \; \text{with } \; x_1 \in [3, -6]\text{.} \label{eq:f2}
\end{align}
#+end_export
The underlying objective function in  this example has a closed-form expression,
but in the context of our applications we consider that we can never observe the
true $f(\mathbf{x})$, even in ideal experimental conditions.  In that sense, the
observations $y_b$  closer to  a real application,  and incorporate  the unknown
effects and  measurement errors  to which the  underlying objective  function is
subject, represented by the  normally distributed random variable $\varepsilon$,
with  mean  $0$  and  variance   $\sigma^{2}$.   There  are  often  algorithmic,
theoretical, or practical /constraints/ on  the allowed combinations of parameters
of a given  objective function.  The observations $y_c$  represent this scenario
by incorporating constraints on the parameter $x_1$.

Panels (a),  (b), and (c) of  Figure\nbsp{}[[fig:simple-search-spaces]] show contour
plots   for  our   search  space   variations  in   Equations\nbsp{}\ref{eq:f0},
\ref{eq:f1}, and \ref{eq:f2} respectively. The  global optimum in each variation
is  represented by  a  red cross,  and its  location  changes between  scenarios
because of  $\varepsilon$. Panels (d), (e),  and (f) show slices  of panels (a),
(b),   and   (c),  respectively,   that   pass   through  the   global   optimum
$\mathbf{x}^{\ast}   =   (x_{1}^{\ast},   x_{2}^{\ast})$  for   fixed   $x_1   =
x_{1}^{\ast}$.

#+NAME: fig:optima-convexity
#+ATTR_LATEX: :width .66\textwidth :placement [t]
#+CAPTION: Illustrating the relationship between convexity
#+CAPTION: of a function over a compact set and the presence
#+CAPTION: of local minima. Panels (a) and (b) match the functions
#+CAPTION: on the same panels of Figure\nbsp{}\ref{fig:simple-search-spaces}
[[file:img/optima_convexity_annotated.pdf]]

The noise-free example  shown in panel (a)  has no local optima,  and its smooth
surface can  be quickly navigated  by the derivative-based methods  discussed in
Chapter\nbsp{}[[Methods for Function Minimization]]. Such  methods aim to follow the
direction  of /greatest  descent/ in  the neighborhood  of a  given point.  In a
contour plot,  this direction is  always orthogonal  to the contour  lines.  The
panel (a) from Figure\nbsp{}[[fig:optima-convexity]]  illustrates the /convexity/ of
our noise-free function. Informally, a line segment connecting any two points in
the   graph  of   a  convex   function  will   not  cross   the  graph   of  the
function. Convexity of a function over a  compact set implies the existence of a
single global optimum, whereas lack of convexity implies the existence of /local
optima/, as happens in  our noisy functions, and is highlighted  in panel (b) of
Figure\nbsp{}[[fig:optima-convexity]].

Local optima  are by  definition surrounded  by higher  values of  the objective
function,  and can  thus trap  optimization methods  that do  not plan  for such
situations,   such   as   a   naive   implementation   of   a   derivative-based
method. Adapting the step  size of a derivative-based method is  one way to deal
with functions with many local optima, such as the ones on panels (b) and (c) of
Figure\nbsp{}[[fig:simple-search-spaces]].

Objective functions  including constraints can  present much harder  problems to
optimization methods, if  certain conditions are met. For example,  on panel (c)
of Figure\nbsp{}[[fig:simple-search-spaces]],  we have constraints that  cut contour
lines in such a way that prevents  attempts to move inside the feasible space in
the  direction of  greatest descent.   A derivative-based  method would  have to
drastically  decrease its  step size  upon reaching  the constraint  border, and
coast along the border in small steps  until it finds a more appealing direction
of descent.

This  more abstract  and  simple example  aimed  to illustrate  that  it can  be
non-trivial to find the  global optimum of the simplest of  search spaces, if we
consider  the   significant  challenges   introduced  by  unknown   effects  and
measurement error.   The additional  challenges introduced by  the time  cost to
obtain measurements, which are  discussed in Chapter\nbsp{}\ref{chap:ed}, guided
the selection  of the  optimization methods  studied in  this thesis.   The next
chapter  discusses   derivative-based  and   stochastic  methods   for  function
minimization.

** Methods for Function Minimization
This  chapter  aims  to  present  the  intuition  guiding  the  construction  of
derivative-based and  stochastic methods for function  minimization.  We discuss
the key hypotheses of these groups of methods, and for which autotuning problems
they can be most effective.

We will put  aside for the moment  the idea of using observations  to estimate a
parameter  vector  and  construct   a  surrogate  function  $\hat{f}(\mathbf{x},
\theta(\mathbf{X},\mathbf{y}))$. This  idea will return in  later chapters.  The
methods discussed  in this chapter  do not  construct a surrogate  function, and
thus attempt  to directly  optimize the  objective function  $f(\mathbf{x})$. In
this sense,  because they need to  know how to evaluate  it during optimization,
these methods  make the hypothesis  that the  objective function is  known.  The
effectiveness of methods based on  derivatives requires additional properties of
$f(\mathbf{x})$ to  be known or  estimable, such as  its first and  second order
derivatives, which imposes additional constraints on objective functions.

Evaluating derivatives  to determine the next  best step or using  heuristics to
explore a search space cannot be  done parsimoniously, because a large number of
function evaluations is  required to estimate derivatives  when closed-forms are
unknown, and to explore a search  space in the expectation of leveraging unknown
structure.      We     will     discuss     Design     of     Experiments     in
Chapter\nbsp{}\ref{chap:ed}, and present  optimization strategies for situations
where the cost of evaluating the objective function is prohibitive.

Methods based on  derivatives can be powerful, provided  their strong hypothesis
are  respected. We  now  briefly  define and  discuss  these  methods and  their
application to autotuning.

*** Methods Based on Derivatives
The derivatives  of a function $f$  at a point $\mathbf{x}$  provide information
about  the   values  of  $f$   in  a   neighborhood  of  $\mathbf{x}$.    It  is
straightforward  to   construct  optimization   methods  that  use   this  local
information, although iteratively leveraging it requires closed-form expressions
for  the derivatives  of $f$,  or estimates  obtained by  evaluating $f$  at the
neighborhoods of each point.

This  section  discusses  gradient  descent and  Newton's  method,  optimization
methods using first and second derivatives of $f$ respectively, which for convex
functions quickly  converge to the  global optimum.   We will use  examples with
Booth's function to discuss how noise and uncertainty impact these methods.

**** Gradient Descent
#+NAME: fig:booth-gradient
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Contour plots and direction of greatest descent $-\nabla{}f(\mathbf{x})$,
#+CAPTION: for search spaces  defined by variations of the Booth function.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: The global optimum is marked with a $\color{red}\boldsymbol{\times}$.
#+CAPTION: To aid visualization, vector magnitude was encoded by color intensity,
#+CAPTION: so that darker vectors have larger magnitude. The gradient along the
#+CAPTION: function's basin is near zero.
[[file:img/booth_gradient.pdf]]

The  gradient  $\nabla{}f(\mathbf{x})$ of  a  function  $f: \mathcal{X}  \to
\mathbb{R}$ at point  $\mathbf{x} = [x_1 \;\dots\; x_n]^{\top}$ is  defined as
#+begin_export latex
\begin{align}
  \nabla{}f(\mathbf{x} = [x_1 \;\dots\; x_n]^{\top}) =
        \left[f^{\prime}_{x_1}(\mathbf{x})
          \;\dots\;
          f^{\prime}_{x_n}(\mathbf{x})\right]^{\top}\text{,}
\end{align}
#+end_export
where  $f^{\prime}_{x_i}(\mathbf{x})$  is  the  partial  derivative  of  $f$  at
$\mathbf{x}$,    with     respect    to    variable    $x_i$.      The    vector
$\nabla{}f(\mathbf{x}_i)$ points to  the direction of /greatest  ascent/ in which,
from the perspective of $f(\mathbf{x}_i)$, the value of $f$ increases the most.

The gradient descent  method is one of the simplest  ways to leverage derivative
information  for  optimization.   It  consists  in  moving  iteratively  in  the
direction  of /greatest  descent/, opposite  the gradient,  from a  starting point
$\mathbf{x}_1$.  If we follow the opposite of the gradient at $\mathbf{x}_1$ for
additional points $\mathbf{x}_2,\dots,\mathbf{x}_n$, each iteration is written
#+begin_export latex
\begin{align}
  \mathbf{x}_{k} = \mathbf{x}_{k - 1} -
  \alpha_{k}\nabla{}f(\mathbf{x}_{k - 1}),
  \; k=2,\dots,n\text{,}
\end{align}
#+end_export
where $\alpha_k$  is the  step size  at iteration  $k$. The  step size  for each
iteration can  be a parameter  fixed at the  beginning of optimization,  but the
best $\alpha_k$ can alternatively be determined by searching along the direction
of greatest descent.

Figure\nbsp{}[[fig:booth-gradient]]  shows the  opposites  of the  gradients of  the
three variations of Booth's  function, described by Equations\nbsp{}\ref{eq:f0},
\ref{eq:f1},   and  \ref{eq:f2},   in  panels   (a),  (b),   (c),  respectively.
Figure\nbsp{}[[fig:booth-gradient-descent-path]]  shows, in  equally marked  panels,
the paths  taken by a gradient  descent algorithm where $\alpha_k$  is chosen at
each step, according to the values of  $f$ in the neighborhood $x_{k - 1}$.

#+NAME: fig:booth-gradient-descent-path
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Representation of paths taken by the gradient descent method,
#+CAPTION: with adaptive choice of $\alpha_k$,
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Contour plots and direction of greatest descent $-\nabla{}f(\mathbf{x})$
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_gradient_descent_path.pdf]]

The gradient descent method iterates along  the direction of greatest descent at
each point  and easily  reaches the optimum  on the search  space of  panel (a),
unless we make an unlucky choice of $\alpha_k$.  The descent paths on panels
(b)  and  (c)  are  not  so straightforward  since  the  several  local  minima,
represented by the crests and loops on  the contour lines, trap the descent path
if $\alpha_k$ is not carefully chosen.

The  situation   is  thornier  in  panel   (c),  where  an  unlucky   choice  of
$\mathbf{x}_1$ or $\alpha_k$ throws the  descent path against the top constraint
border,  forcing the  method to  zigzag along  the border  in short  steps. This
happens  in  this  particular  situation  in  panel  (c)  because  all  gradient
information  guides the  descent across  the constraint  border, but  the method
cannot cross it. Gradient  descent has a harder time on panels  (b) and (c) even
upon reaching  the basin  where the optimum  lies, because  gradient information
there is  also conflicting  due to noise  $\varepsilon$.  Gradient  descent gets
stuck in our example paths, but restarting strategies picking new $\mathbf{x}_1$
or $\alpha_k$ could help escaping the local minima along the basin, as is shown
in Figure\nbsp{}[[fig:booth-gradient-descent-path-restart]].

#+NAME: fig:booth-gradient-descent-path-restart
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Representation of paths taken by the gradient descent method with 4 restarts,
#+CAPTION: with adaptive choice of $\alpha_k$,
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Contour plots and direction of greatest descent $-\nabla{}f(\mathbf{x})$
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_gradient_descent_path_restart.pdf]]

**** Newton's Method
With Newton's method  we can improve upon the intuition  of descending along the
opposite  of  the gradient  of  $f$  by  using  the second  partial  derivatives
$f_{\mathbf{x}_{k -  1}}^{\prime\prime}$ to approximate $f$  in the neighborhood
of $\mathbf{x}_{k - 1}$ using its second Taylor polynomial
#+begin_export latex
\begin{align}
  f(\mathbf{x}) \approx f(\mathbf{x}_{k - 1}) \; + \;
    \nabla{}f(\mathbf{x}_{k - 1})^{\top}(\mathbf{x} - \mathbf{x}_{k - 1}) \; + \;
    \frac{1}{2}(\mathbf{x} - \mathbf{x}_{k - 1})^{\top}\mathbf{H}f(\mathbf{x}_{k - 1})
    (\mathbf{x} - \mathbf{x}_{k - 1})\text{,}
\end{align}
#+end_export
where  $\mathbf{x}$  is  in  the  neighborhood  of  $\mathbf{x}_{k  -  1}$,  and
$\mathbf{H}f(\mathbf{x}_{k - 1})$ denotes the Hessian of $f$, a square matrix of
second derivatives of $f$, with elements
#+begin_export latex
\begin{align}
  \left(\mathbf{H}f(\mathbf{x}_{k - 1})\right)_{i,j} =
  f^{\prime\prime}_{x_i,x_j}(\mathbf{x}_{k - 1})\text{.}
\end{align}
#+end_export
We are  not going  to consider  the approximation  of $f$  by the  second Taylor
polynomial to be an estimation process in the statistical sense, because it does
not involve dealing with measurement or modeling error.

The second Taylor  polynomial uses information about the  partial derivatives of
$f$  at $\mathbf{x}_{k  - 1}$  to  produce an  approximation of  $f$ for  points
$\mathbf{x}$ around  $\mathbf{x}_{k - 1}$.  If  we compute the gradient  of this
approximation  polynomial  and  set  it  to  zero,  we  obtain  the  next  point
$\mathbf{x}_k$, as well  as the iterative step of Newton's  method.  Starting at
$\mathbf{x}_1$, for points $\mathbf{x}_2, \dots, \mathbf{x}_n$, we have
#+begin_export latex
\begin{align}
\mathbf{x}_k = \mathbf{x}_{k - 1} -
\mathbf{H}f(\mathbf{x}_{k - 1})\nabla{}f(\mathbf{x}_{k - 1})\text{.} \label{eq:newton}
\end{align}
#+end_export

Provided the strong hypotheses of convexity and differentiability are respected,
derivative-based  methods  are  extremely effective.   In  particular,  Newton's
method converges to the global optimum in  a single step if $f$ is quadratic and
$\mathbf{H}f$ is positive definite.  This  happens in the following example with
Booth's function
#+begin_export latex
\begin{align}
y = f(\mathbf{x} = [x_1, x_2]^{\top}) = (x + 2y - 7)^{2} + (2x + y - 5)^{2}, \; x_1,x_2 \in [-10, 10]\text{,}
\end{align}
#+end_export
adapted from Kochenderfer and Wheeler\nbsp{}\cite{kochenderfer2019algorithms}.

If we start with $\mathbf{x}_1 = [9, 8]^{\top}$ and plug
#+begin_export latex
\begin{align}
  \nabla{}f(\mathbf{x}_1 = [9, 8]^{\top}) = [10 \cdot 9 + 8 \cdot 8 - 34,
    8 \cdot 9 + 10 \cdot 8 - 38]^{\top} = [120, 144]^{\top}\text{,}
\end{align}
#+end_export
and
#+begin_export latex
\begin{align}
  \mathbf{H}f(\mathbf{x}_1 = [9, 8]^{\top}) =
  \begin{bmatrix}
    10 & 8 \\
    8 & 10
  \end{bmatrix}
\end{align}
#+end_export
into the Newton's method update step in Equation\nbsp{}\ref{eq:newton},
we reach $\mathbf{x}^{\ast} = [1, 3]^{\top}$ in the next step
#+begin_export latex
\begin{align}
  \mathbf{x}_2 = \begin{bmatrix}
    9 \\
    8
  \end{bmatrix} -
  \begin{bmatrix}
    10 & 8 \\
    8 & 10
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    120 \\
    144
  \end{bmatrix} =
  \begin{bmatrix}
    1 \\
    3
  \end{bmatrix} = \mathbf{x}^{\ast}\text{.}
\end{align}
#+end_export

Function minimization methods work very  well if the objective function respects
their  strong hypotheses  of  convexity  and differentiability.   Unfortunately,
autotuning problems seldom fulfill the  conditions necessary for the application
of  such  methods.

#+NAME: fig:search-space-2
#+ATTR_LATEX: :width .6\textwidth :placement [h!]
#+CAPTION: An exhaustively measured search space, defined by loop blocking and unrolling
#+CAPTION: parameters, for a sequential GEMM kernel.
#+CAPTION: Reproduced from Seymour \textit{et al.}\nbsp{}\cite{seymour2008comparison}
[[file:img/seymour2008comparison.pdf]]

Typical  autotuning   search  spaces,  such   as  the  one  we   encountered  in
Chapter\nbsp{}[[The    Need   for    Autotuning]],    which    is   reproduced    in
Figure\nbsp{}[[fig:search-space-2]],  clearly  present  considerable  challenges  to
derivative-based methods,  due to  the abundance of  local minima,  valleys, and
ridges.   Since  we  have  closed-form   expression  for  the  search  space  in
Figure\nbsp{}[[fig:search-space-2]], we would have  to perform a considerable number
of  evaluations  of $f$  in  order  to estimate  its  derivative  at each  step.
Although gradient  descent and other  derivative-based methods can  be effective
and  are  historically  important,  measuring  $f$  is  usually  not  cheap  for
autotuning search spaces.  Therefore, minimizing  experimentation cost is also a
strong concern.

Before    discussing    methods    to     construct    surrogate    models    in
Chapter\nbsp{}[[Learning: Building Surrogates]], and  how to use such surrogates
to minimize experimental cost on  Chapter\nbsp{}\ref{chap:ed}, we will relax the
convexity and differentiability requirements  on objective functions and discuss
stochastic  methods  for  function  minimization,  and  their  applicability  to
autotuning.

*** Stochastic Methods
In this  section we  discuss some stochastic  methods for  function minimization
that  drop the  convexity and  differentiability requirements  of gradient-based
methods, becoming applicable to a wider range of autotuning problems at the cost
of providing no  convergence guarantees. In fact, these methods  have no clearly
stated hypotheses and  are based on search space  exploration heuristics.  Often
the best  possible understanding  of how  these heuristics  work comes  from the
intuition  and motivation  behind their  definition,  and from  the analysis  of
empirical tests.

There are  multiple ways  to categorize  heuristics.  Our choice  was to  make a
distinction  between single-state  and  population-based  methods.  In  summary,
single-state methods have update rules  mapping a single point $\mathbf{x}_k$ to
a point  $\mathbf{x}_{k + 1}$,  while rules  for population-based methods  map a
population $\mathbf{P}_k =  \{\mathbf{x}_1,\dots,\mathbf{x}_n\}$ to a population
$\mathbf{P}_{k  + 1}$,  which  may  retain, combine,  and  modify elements  from
$\mathbf{P}$.

We will first  discuss single-state methods, building up  to Simulated Annealing
from  Random  Walk,  then  we  discuss Genetic  Algorithms  and  Particle  Swarm
Optimization, representing widely used population-based methods.

**** Single-State Methods: Random Walk and Simulated Annealing
Random Sampling  is arguably the  simplest exploration heuristic,  consisting of
picking uncorrelated samples  from a uniform distribution over  the search space
$\mathcal{X}$.  There is no guarantee of finding local or global minima, but the
chances of  improving over a starting  point increase if the  objective function
has  many  local  minima.   Despite  its  simplicity,  Random  Sampling  can  be
surprisingly effective.

A simple way to derive a single-state  heuristic from Random Sampling is to take
correlated  samples,  so  that  each  sample  lies  in  a  neighborhood  of  its
predecessor, which is called Random Walk.  The neighborhood $N(\mathbf{x})$ of a
point is  the set of  points within distance $d$  from $\mathbf{x}$. One  way to
define it is
#+begin_export latex
\begin{align}
  N(\mathbf{x}) = \{\mathbf{x}_i \in \mathcal{X}: \mathbf{x}_i \neq \mathbf{x},
    \; \lVert{}\mathbf{x}_i - \mathbf{x}\rVert{}^{2} \leq d\}\text{.}
\end{align}
#+end_export
As  long  as  it is  possible  to  compute  distances  between the  elements  of
$\mathcal{X}$, we  will be  able to  construct the neighborhood  of a  point and
employ the stochastic methods we discuss in this section.

A random walk of length $n$ starting at $\mathbf{x}_1$ produces a sequence where
each point $\mathbf{x}_{k  > 1}$ is a random variable  with uniform distribution
over $N(\mathbf{x}_{k  - 1})$. There  is no guarantee that  we will ever  find a
better point  with respect  to the  objective function than  the one  we started
with.  A straightforward extension  of Random Walk is to pick at  each step $k >
1$ the first point $\mathbf{x}_k$ we  come across in $N(\mathbf{x}_{k - 1})$ for
which $f(\mathbf{x}_k)  < f(\mathbf{x}_{k  - 1})$.   This greedy  strategy would
require   measuring  the   value  of   $f$   for  possibly   many  elements   of
$N(\mathbf{x}_{k - 1})$  but it ensures that  we will only move  toward a better
point.

If are willing to  pay the cost to measure all the  points in $N(\mathbf{x}_{k -
1})$ we can choose the $\mathbf{x}^{\ast}_{k}$ that brings the best improvement,
for  which $f(\mathbf{x}^{\ast}_{k})  < f(\mathbf{x})$  for all  $\mathbf{x} \in
N(\mathbf{x}_{k - 1})$. This best improvement  strategy always moves to the best
point  in a  neighborhood, but  it can  still get  stuck in  lock minima  if the
current point is already the best one in its neighborhood. Adapting the distance
that  defines  a  neighborhood  can  help escape  local  minima,  but  the  best
improvement  strategy still  requires  measuring the  entire  neighborhood of  a
point.

Simulated Annealing  is a  probabilistic improvement  heuristic inspired  by the
process of annealing, where temperature is carefully controlled to first agitate
a material's crystalline  structure with higher temperature, and  then settle it
into  more  desirable  configurations.   Adapting  this  idea  to  optimization,
Simulated Annealing makes a compromise  between a greedy approach to exploration
and a  random walk.  At  each step $k$, we  pick a random  uniformly distributed
$\mathbf{x}_{k} \in N(\mathbf{x}_{k - 1})$ and  move to it if $f(\mathbf{x}_k) <
f(\mathbf{x}_{k  - 1})$.   In contrast  to a  greedy approach,  we also  move if
$f(\mathbf{x}_k) \geq f(\mathbf{x}_{k - 1})$ with probability $p$.

In analogy to the real annealing  process, $p$ starts high to enable exploration
of  the  search  space  and  then  decreases  to  force  a  descent  towards  an
optimum. All through  the process, a nonzero value of  $p$ permits the heuristic
to leave a local  minimum.  The probability $p_k$ of moving to  a worse point at
iteration $k$ is
#+begin_export latex
\begin{align}
  p_k = exp\left(-{\dfrac{f(\mathbf{x}_{k}) -
      f(\mathbf{x}_{k - 1})}{t_k}}\right)\text{,}
\end{align}
#+end_export
where  $t_k$ is  the  temperature at  iteration $k$,  and  can follow  different
decaying  rates. For  a  starting temperature  $t_1$  the logarithmic  annealing
schedule is
#+begin_export latex
\begin{align}
  t_k = \dfrac{t_1}{log(k + 1)}\text{.}
\end{align}
#+end_export

#+NAME: fig:booth-sima-path
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Representation of paths taken by the Simulated Annealing method
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Contour plots and direction of greatest descent
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_simulated_annealing_descent_path.pdf]]

Figure\nbsp{}[[fig:booth-sima-path]] shows the paths taken by Simulated Annealing on
search spaces defined by variations of Booth's function. In panel (a) we can see
the effects  of the  annealing schedule,  which enables  large detours  to worse
points early on, but forces descent on later iterations. Since this search space
has no local  minima, the descent eventually reaches the  optimum.  In panel (b)
the higher initial temperature also allows  escaping the many local minima found
during exploration, but as the path approaches the global optimum with the lower
temperature of later  iterations it gets trapped by one  the local minima around
the global optimum. Likewise, encountering a constraint border early on in panel
(c) is not a challenge for Simulated  Annealing, since it can bounce back toward
worse points, but on later iterations the  method is forced to wander around the
border like gradient descent, also getting trapped at local minima.

Single-state methods for optimization provide  heuristics for exploring a search
space, usually  based on the  neighborhoods of the  points that compose  a path.
The cost of  measuring the objective function $f$ for  an entire neighborhood is
prohibitive in  large dimensional search  spaces, but methods such  as Simulated
Annealing  provide  strategies  for  escaping local  minima  without  completely
evaluating  a neighborhood.   Despite  that, the  local  nature of  single-state
methods means that the final results  are heavily reliant on the starting point.
Restarting and performing  parallel searches are among the  strategies to reduce
dependence on initial choices,  as is the idea of using  a population of points,
which we discuss next.

**** Population-Based Methods: Genetic Algorithms and Particle Swarm Optimization
Instead  of progressing  from a  single starting  point $\mathbf{x}_1$  toward a
final state $\mathbf{x}_n$,  we can use heuristics for moving  a starting set of
points, or population,  $\mathbf{P}_1 = \{\mathbf{x}_{1},\dots,\mathbf{x}_{n}\}$
toward  a  final  population  $\mathbf{P}_n$.    For  simplicity,  we  keep  the
population  size constant.   The  inspiration for  population-based methods  for
function minimization  comes in general  from observations of processes  such as
evolution by  natural selection  and animal group  behavior.

The intuition behind population-based methods is that the points in a population
would provide variability that, when combined in specific ways, would eventually
lead to  better values of  the objective  function $f$.  Genetic  Algorithms, in
analogy to  the process of  evolution by  natural selection, select  the fittest
points  in  a  population  for  generating offspring.

Individuals can be selected according to multiple metrics, aiming to produce the
best  possible combinations  in an  iteration  but also  to maintain  population
variability.  Since  we do  not know how  mutations and  chromosome combinations
might  impact the  fitness of  an individual,  keeping worse  individuals during
optimization could pay off later.

#+NAME: fig:genetic-crossover
#+ATTR_LATEX: :width .9\textwidth :placement [t]
#+CAPTION: Some ways of producing offspring from two parents with binary chromosomes.
#+CAPTION: Crossover splits parent chromosomes and combine the resulting pieces. In general,
#+CAPTION: pieces from multiple splits can be combined.
#+CAPTION: Mutations are introduced randomly and correspond to flipping bits on binary chromosomes.
[[file:img/genetic_algorithms_crossover.pdf]]

The new  population is generated  by combining  the chromosomes of  each parent,
using strategies  that are also inspired  by natural processes such  as mutation
and  crossover. To  be able  to perform  these operations,  chromosomes must  be
encoded in a suitable representation. Individuals in the search space defined by
compiler flags, for example, a binary array indicating whether each flag is used
could be a suitable representation.  Figure\nbsp{}[[fig:genetic-crossover]] shows what the
mutation and crossover operations could look like on a binary encoding.

Genetic Algorithms have  the potential to explore a search  space more globally,
simultaneously maintaining several populations  distributed over a search space,
and have also the potential of escaping local minima by mutation and combination
of  chromosomes of  different individuals.   Figure\nbsp{}[[fig:booth-genetic-path]]
shows a  representation of the paths  a population in a  Genetic Algorithm could
take  while searching  for the  global minimum  on three  variations of  Booth's
function.   Each region  marked  by  dashed lines  represents  the  spread of  a
generation on a given optimization step,  and previous individuals are marked by
hollow points. The final generation is marked by the filled points.  In contrast
to Gradient Descent and Simulated Annealing,  a Genetic Algorithm do not seek to
measure  and minimize  local properties  of $f$,  and consequently  its behavior
would be less impacted  by the noisy scenarios on panels (b)  and (c). Since the
population can spread across the search space, it could still be possible to end
the process with individuals in different  local minima, which is represented in
panel (b).

#+NAME: fig:booth-genetic-path
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Color-coded generation snapshots of a Genetic Algorithm
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Hollow points and dashed lines mark members of previous populations
#+CAPTION: and the regions they covered, while filled points and complete
#+CAPTION: lines mark the final population.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Contour plots and direction of greatest descent
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_genetic_algorithm_descent_path.pdf]]

Differential Evolution  presents an alternative  to the strategies  of mutations
and  crossover, represented  in  Figure\nbsp{}[[fig:genetic-crossover]]. Consider  a
population $\mathbf{P}_{k  - 1}$,  and individuals  $\mathbf{x}_a, \mathbf{x}_b,
\mathbf{x}_c$ uniformly  distributed over the population  at step $k -  1$.  The
update step in Differential Evolution sets  the parameters, or components, of an
offspring  $\mathbf{x}_k$  to a  corresponding  component  of $\mathbf{x}_a$  or
$\mathbf{x}_d = w \cdot (\mathbf{x}_b - \mathbf{x}_c)$, according to
#+begin_export latex
\begin{align}
  \mathbf{x}_{k,i} = \begin{cases}
    \mathbf{x}_{d,i} &
    \mbox{if} \; i = j, \; \mbox{or with probability} \; p \\
    \mathbf{x}_{a,i} & \mbox{otherwise}
  \end{cases}\text{,}
\end{align}
#+end_export
where $j$  is a dimension favored  for updating picked  at random, and $w$  is a
weight given to  the difference between $\mathbf{x}_b$  and $\mathbf{x}_c$.  The
offspring $\mathbf{x}_k$ replaces  $\mathbf{x}_a$ in the population  at step $k$
if $f(\mathbf{x}_k) < f(\mathbf{x}_a)$.

A different approach to leveraging a  population for optimization is to think of
it as an  analogy for a swarm,  in a method called  Particle Swarm Optimization.
Each  individual $\mathbf{x}_{k}$  in  the  swarm keeps  track  of its  velocity
$\mathbf{v}_k$.   The  velocity  is  updated  at each  step,  and  points  to  a
combination of $\mathbf{x}^{best}$, the best position found by the swarm so far,
and  $\mathbf{x}_{k}^{best}$, the  individual's  personal best.   At each  step,
individual $\mathbf{x}_k$ updates its position to
#+begin_export latex
\begin{equation}
  \mathbf{x}_{k}^{\prime} = \mathbf{x}_k + \mathbf{v}_k\text{,}
\end{equation}
#+end_export
and its velocity  $\mathbf{v}_k$ according to
#+begin_export latex
\begin{equation}
  \mathbf{v}_{k}^{\prime} = \alpha_1\mathbf{v}_k +
  \alpha_2(\mathbf{x}^{best}  - \mathbf{x}_k) +
  \alpha_3(\mathbf{x}_{k}^{best} - \mathbf{x}_k)\text{,}
\end{equation}
#+end_export
where  $\alpha_1$,  $\alpha_2$,  and  $\alpha_3$  are  chosen  beforehand.   The
intuition behind  this method is that  the momentum of each  particle toward the
best points  found so far would  accelerate convergence and allow  escaping from
local minima.

In whichever way  we choose to combine individuals  in population-based methods,
these  heuristics  require  extensive exploration  and,  consequently,  abundant
evaluation of the  objective function.  By making virtually  no hypotheses these
methods inspired by natural processes become  applicable to a much broader range
of  search   spaces  than   derivative-based  methods,  while   abdicating  from
convergence  guarantees.    Population-based  methods  perform  a   more  global
optimization, reliant on the initial distribution of individuals over the search
space to provide variability.

*** Summary
The methods for  function minimization we have discussed in  this chapter do not
strive to be parsimonious. To be effective they require many estimates of values
of  $f$  and  may  require  additional  information,  such  as  $\nabla{}f$  and
$\mathbf{H}f$.  The strong hypotheses of derivative-based methods restrict their
applicability by invalidating convergence guarantees. Stochastic methods have no
such guarantees to begin with, and thus require costly exploration.

Figure\nbsp{}[[fig:booth-minimization]]  shows  representations of  optimization
trajectories for  Booth's function  made by Gradient  Descent with  restarts, on
panels (a), (b), and (c), Simulated Annealing,  on panels (d), (e), and (f), and
a Genetic Algorithm, on panels (g), (h), and (i). Restarting optimization from a
different point, seem on  panels (a), (b), and (c), is a  common and widely used
technique to reduce reliance on starting conditions.

Despite the high cost of exploration and strong hypotheses, methods for function
minimization are used for autotuning  and can indeed achieve interesting results
in  certain  problems.  We  will  present  our  results  with these  methods  on
Chapters\nbsp{}\ref{chap:cuda} and  \ref{chap:fpga}, where we also  review their
application  to  autotuning problems  in  different  domains.  The  explorations
performed by these  methods are not structured in a  way that favors statistical
analysis.   We will  postpone the  discussion of  how to  obtain well-structured
experimental  data  until  Chapter\nbsp{}\ref{chap:ed},  and  in  the  following
chapter we will  discuss learning methods that enable  building surrogate models
$\hat{f}$ and  identifying relationships between parameters  $\mathbf{X}$ in the
search space $\mathcal{X}$ and observations of the objective function $f$.

#+NAME: fig:booth-minimization
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Representation of paths taken by the gradient descent method,
#+CAPTION: with adaptive choice of $\alpha_k$,
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Panel groups (a,d,g), (b,e,h), and (c,f,i) correspond to
#+CAPTION: Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Panel groups (a,b,c), (d,e,f), and (g,h,i) correspond to
#+CAPTION: Gradient Descent with restarts, Simulated Annealing,
#+CAPTION: and a Genetic Algorithm, respectively.
#+CAPTION: Contour plots and direction of greatest descent $-\nabla{}f(\mathbf{x})$
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_minimization.pdf]]

** Learning: Building Surrogates
This  chapter  discusses two  methods  for  building surrogate  models.   Linear
Regression is  a parametric  method capable  of modeling  a restricted  class of
surrogates,  for  which  it  is relatively  simple  to  interpret  significance.
Gaussian  Process  Regression is  a  flexible  nonparametric method  capable  of
modeling a  large class  of surrogates, for  which interpreting  significance is
possible, but costly.

The  surrogate $\hat{f}_{\theta}:  \mathcal{X}  \to \mathbb{R}$  depends on  the
parameter  vector  $\theta(\mathbf{X},  \mathbf{y})$   in  the  parameter  space
$\Theta$   constructed  using   the  pair   $(\mathbf{X},  \mathbf{y})$,   where
$\mathbf{X}$ is a set of  experiments, and $\mathbf{y}$ the corresponding vector
of  observations of  the  objective  function $f$.   The  process  of using  the
available data to construct a surrogate  model is called /learning/ and involves
fitting the  surrogate model, which can  also be called training.   The class of
surrogates we can fit depends on each method's definition of $\Theta$.

In this chapter we assume that  an initial pair of $(\mathbf{X}, \mathbf{y})$ is
given, and  that we can no  longer observe $f(\mathbf{x})$ for  new $\mathbf{x}$
outside of  $\mathbf{X}$.  In this sense,  we do not  know $f$ but we  can still
make hypotheses  about it in  order to construct  surrogate models. For  the two
methods we discuss,  we present the hypotheses embedded in  each parameter space
$\Theta$ and  how each method fits  a surrogate.  We also  discuss strategies to
evaluate  the  quality  of  fit  of a  surrogate,  and  to  interpret  parameter
significance.

We assume  for now  that $\mathbf{X}  \sim \text{Uniform}(\mathcal{X})$,  and we
discuss how we  can construct better distributions of experiments  over a search
space on Chapter\nbsp{}\ref{chap:ed}.  We will  now discuss Linear Regression, a
method to build surrogates using linear models on the parameters $\theta$.

*** Linear Regression
We  will build  a surrogate  $\hat{f}_{\theta}$ for  a function  $f: \mathcal{X}
\to  \mathbb{R}$   using  a  fixed  experimental   design  $\mathbf{X}$  and
observations  $\mathbf{y}$. We make the hypothesis that $f$ is a linear model on
$\theta$, with error $\varepsilon$, written
#+begin_export latex
\begin{align}
  \label{eq:linear-model}
  f(\mathbf{X}) = \mathbf{X}\theta(\mathbf{X}, \mathbf{y}) + \varepsilon\text{.}
\end{align}
#+end_export
The  experimental  design  $\mathbf{X}$  is an  $n\times{}p$  matrix  of  column
vectors,  where   each  element   $\mathbf{x}_{1},\dots,  \mathbf{x}_n$   is  an
experiment.  We do not know how  well the linear model hypothesis represents the
true $f$, but we assume that  each experiment in $\mathbf{X}$ was run, producing
the  response  vector $\mathbf{y}  =  [y_1  \;\dots\; y_n]^{\top}$,  where  each
element  is  an  observation  of   $f$  subject  to  measurement  error.

Using $\mathbf{y}$ we can construct  parameter vectors $\theta$ in the parameter
space
#+begin_export latex
\begin{align}
  \Theta  = \{(\theta_0, \dots, \theta_{m-1}):
  \theta_0,\dots,\theta_{m  -   1}  \in
  \mathbb{R}\}\text{.}
\end{align}
#+end_export
For linear models  we can construct the optimal  parameter vector $\hat{\theta}$
using the Ordinary  Least Squares (OLS) estimator, which we  discuss in the next
section, and use it to write the linear model surrogate
#+begin_export latex
\begin{align}
  \label{eq:linear-model-surrogate}
  \hat{f}_{\theta}(\mathbf{X}) =
  \mathbf{X}\hat{\theta}(\mathbf{X},\mathbf{y})\text{.}
\end{align}
#+end_export
The   vector  $\hat{\mathbf{y}}$   produced   by  evaluating   $\hat{f}_{\theta}
(\mathbf{X})$ is  called the prediction  of the surrogate model  for experiments
$\mathbf{X}$, and  the distance between $\mathbf{y}$  and $\hat{\mathbf{y}}$ can
be used to optimize the parameter vector  and estimate the quality of fit of the
surrogate.

A key advantage  of a surrogate model  is that we can  use $\hat{f}_{\theta}$ to
estimate  the  value of  $f$  for  a  new design  $\mathbf{X}^{\prime}$  without
evaluating  $f(\mathbf{X}^{\prime})$.  For  well  chosen  $\mathbf{X}$ and  well
fitted $\theta$, these estimates can be accurate and useful. We will discuss how
to choose  experiments on Chapter\nbsp{}\ref{chap:ed},  and how to  evaluate the
quality of fit for linear models on Section\nbsp{}[[Assessing the Quality of Fit
of Linear Model Surrogates]].

**** Fitting the Model: The Ordinary Least Squares Estimator
For   simplicity,    we   will    for   now   use    $\theta$   to    refer   to
$\theta(\mathbf{X},\mathbf{y})$.  The sum of the squared differences between $f$
and $\hat{f}_{\theta}$ for all points in $\mathbf{X}$ is the squared model error
#+begin_export latex
\begin{align}
  \left\lVert\varepsilon\right\rVert^{2} =
  \left\lVert\mathbf{y} - \hat{\mathbf{y}}\right\rVert^{2} =
  (\mathbf{y} - \mathbf{X}\theta)^{\top}(\mathbf{y} - \mathbf{X}\theta)\text{,}
\end{align}
#+end_export
which is a quadratic function of $\theta$. We can therefore differentiate it
with respect to $\theta$ and set it to zero to obtain the OLS estimator
#+begin_export latex
\begin{align}
  \hat{\theta} = (\mathbf{X}^{\top}\mathbf{X})^{-1}
  \mathbf{X}^{\top}\mathbf{y}\text{,}
\end{align}
#+end_export
provided $\mathbf{X}^{\top}\mathbf{X}$  is invertible.  The variance of  the OLS
estimator is written
#+begin_export latex
\begin{align}
  \text{Var}(\hat{\theta}) = (\mathbf{X}^{\top}\mathbf{X})^{-1}\sigma^{2}\text{,}
\end{align}
#+end_export
assuming the observations $\mathbf{y}$  are uncorrelated and homoscedastic, with
constant  variance  $\sigma^{2}$.    Since  we  assume  in   this  chapter  that
$\mathbf{X}$  is fixed,  the  variance of  $\hat{\theta}$  shows explicitly  how
uncertainty  in  measurements  due  to  error propagates  to  the  linear  model
surrogate, and we  can use this uncertainty to compute  confidence intervals for
the  surrogate's  predictions.  We  will  use  the  fact  that the  variance  of
$\hat{\theta}$ depends  on $(\mathbf{X}^{\top}\mathbf{X})^{-1}$ when  we discuss
Optimal Design on Section\nbsp{}\ref{sec:optimal}.

An interesting interpretation  of the OLS estimator is that  it approximates the
observation vector  $\mathbf{y}$ by  its orthogonal  projection into  the vector
space spanned  by the columns  of $\mathbf{X}$.  Substituting  $\hat{\theta}$ on
the   surrogate  model\nbsp{}\ref{eq:linear-model-surrogate},   we  obtain   the
projection matrix
#+begin_export latex
\begin{align}
  H = \mathbf{X}(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\text{.}
\end{align}
#+end_export
The  difference   between  $\mathbf{y}$   and  its  projection   $H\mathbf{y}  =
\hat{\mathbf{y}}$ is  the model  error $\varepsilon$,  orthogonal to  the column
space         of         $\mathbf{X}$,         as         represented         in
Figure\nbsp{}[[fig:regression-projection]].   The  OLS  estimator  can  also  be
derived as a special case of  the Maximum Likelihood Estimator with $\varepsilon
\sim \mathcal{N}(\mu,\sigma^2)$.

#+NAME: fig:regression-projection
#+ATTR_LATEX: :width 0.48\textwidth :placement [t]
#+CAPTION: In Linear Regression, the prediction vector $\hat{\mathbf{y}}$
#+CAPTION: is the orthogonal projection of the observations vector $\mathbf{y}$
#+CAPTION: into the vector space
#+CAPTION: spanned by the columns of $\mathbf{X}$.
[[file:img/linear_regression_projection.pdf]]

We  assumed  that the  design  matrix  $\mathbf{X}$  was  used directly  in  the
surrogate  model\nbsp{}\ref{eq:linear-model-surrogate},  but   this  limits  the
models we can represent with $\theta_{0,\dots,m-1}$ to those with $m = p$ linear
terms on each  factor $\mathbf{x}_{1,\dots,p}$, without an  intercept term. More
generally, we  can have a  linear model surrogate  with $m\neq{}p$ terms.   In a
straightforward expansion  of the linear  model, we  can obtain $m$  model terms
from an  $n\times{}p$ design matrix $\mathbf{X}$  by using a basis  function set
$\mathcal{H} = \{h_{0,\dots,m-1}:  \mathcal{X} \to \mathbb{R}\}$, generating
the $n\times{}m$ model matrix
#+begin_export latex
\begin{align}
  \mathcal{M}\left(\mathbf{X} = \begin{bmatrix}
    \mathbf{x}_1 \\
    \vdots \\
    \mathbf{x}_n
  \end{bmatrix}_{n\times{}p}\right) =
  \begin{bmatrix}
    h_0(\mathbf{x_1}) & \dots & h_{m - 1}(\mathbf{x_1}) \\
    \vdots & \ddots & \vdots \\
    h_0(\mathbf{x_n}) & \dots & h_{m - 1}(\mathbf{x_n})
  \end{bmatrix}_{n\times{}m}\text{.}
\end{align}
#+end_export
The OLS  estimator does  not change if  we write the  linear model  surrogate in
Equation\nbsp{}\ref{eq:linear-model-surrogate}  with  $\mathcal{M}(\mathbf{X})$.
Unless we need to discuss the underlying model terms, we assume from now on that
$\mathbf{X}$ represents a suitable  model matrix $\mathcal{M} (\mathbf{X})$.  We
will now discuss how to evaluate the quality of fit of a linear model surrogate.

***** Attempt to formalize Model Matrices (Better to use Basis Expansion) :noexport:
The  class  of  linear  models  is   relatively  limited  but  allows  for  some
flexibility.  In  a simple and  less abstract  example, consider a  search space
$\mathcal{X} = \{(x_1,  x_2): x_1, x_2 \in \mathbb{R}\}$  and objective function
$f$,  for which  we collected  experimental data  $\mathbf{y} =  f(\mathbf{X}) +
\varepsilon$, for  an $n\times{}p$ design  $\mathbf{X}$.  We construct  a linear
model    surrogate,     for    which     the    $n\times{}m$     model    matrix
$\mathcal{M}(\mathbf{X})$, corresponding to a linear model with all second order
effects, is written
#+begin_export latex
\begin{align}
  \mathcal{M}(\mathbf{X}) =
  \begin{bmatrix}
    1 & x_{1,1} & x_{1,2} & x_{1,1}^{2} & x_{1,2}^{2} & x_{1,1}x_{1,2} \\
    \multicolumn{6}{c}{$\vdots$} \\
    1 & x_{n,1} & x_{n,2} & x_{n,1}^{2} & x_{n,2}^{2} & x_{n,1}x_{n,2}
  \end{bmatrix}\text{,}
\end{align}
#+end_export
with $m = 6$, and the $m\times{}1$ parameter vector
#+begin_export latex
\begin{align}
  \theta(\mathbf{X}, \mathbf{y}) = [\theta_0 \; \dots \; \theta_5]^{\top}
\end{align}
#+end_export
is  constructed  using the  OLS  estimator  and  the  data we  collected.   From
Equation\nbsp{}\ref{eq:linear-model-surrogate}, the  resulting prediction vector
is
#+begin_export latex
\begin{align}
  \hat{\mathbf{y}} = \begin{bmatrix}
    \hat{y_1} \\
    \vdots \\
    \hat{y_n}
  \end{bmatrix} = \begin{bmatrix}
    \theta_0 + \theta_1x_{1,1} + \theta_2x_{1,2} + \theta_3x_{1,1}^{2} +
    \theta_4x_{1,2}^{2} + \theta_5x_{1,1}x_{1,2} + \varepsilon_1\\
    \vdots \\
    \theta_0 + \theta_1x_{n,1} + \theta_2x_{n,2} + \theta_3x_{n,1}^{2} +
    \theta_4x_{n,2}^{2} + \theta_5x_{n,1}x_{n,2} + \varepsilon_n\\
  \end{bmatrix}\text{.}
\end{align}
#+end_export

**** Assessing the Quality of Fit of Linear Model Surrogates
We will construct  three linear model surrogates for the  Booth function and use
the   model    fits   to    discuss   the    assessment   of    model   quality.
Figure\nbsp{}[[fig:regression-fitting]]  shows  the   three  surrogate  model  fits,
constructed using  the same design $\mathbf{X}$  with $n = 10$  random uniformly
distributed measurements of the Booth function
#+begin_export latex
\begin{align}
  f(\mathbf{x} = [x_1, x_2]^{\top}) = (x_1 + 2x_2 - 7)^{2} +
  (2x_1 + x_2 - 5)^{2}, \; x_1,x_2 \in [-10, 10]\text{,}
\end{align}
#+end_export
subject  to  measurement  error  $\varepsilon$.  Panel  (a)  shows  the  surface
produced  by measuring  $f$ in  its entire  domain. The  10 random  measurements
composing $\mathbf{X}$ are highlighted. Panels (b),  (c), and (d) show the three
surrogates, which used the basis function sets
#+begin_export latex
\begin{align}
  \mathcal{H}_{(b)} &= \{\;h_0(\mathbf{x}) = 1,\;h_1(\mathbf{x}) = x_1,
  \;h_2(\mathbf{x}) = x_2)\;\}\text{,} \label{eq:hb} \\
  \mathcal{H}_{(c)} &= h_{(b)} \cup \{\;h_3(\mathbf{x}) = x_1^{2},
  \;h_4(\mathbf{x}) = x_2^{2})\;\}\text{, and} \label{eq:hc} \\
  \mathcal{H}_{(d)} &= h_{(c)} \cup \{\;h_5(\mathbf{x}) = x_1x_2 )\;\}\text{.}
   \label{eq:hd}
\end{align}
#+end_export

Panels (b), (c), and (d) also show  the training, testing, and true Mean Squared
Error (MSE) for each surrogate on the design $\mathbf{X}$, written
#+begin_export latex
\begin{align}
  MSE_{\hat{f}_{\theta}(\mathbf{X})} &= \dfrac{1}{n}
  \sum_{i = 1}^{n}{\left(\left(f(\mathbf{x}_i) + \varepsilon\right) -
    \hat{f}_\theta(\mathbf{x}_i)\right)^2}\text{,} \label{eq:mse-train} \\[1em]
  MSE_{\hat{f}_{\theta}} &= \dfrac{1}{\vert{}\mathcal{X}\vert{}}
  \sum_{i = 1}^{\vert{}\mathcal{X}\vert{}}{\left(\left(f(\mathbf{x}_i) + \varepsilon\right) -
    \hat{f}_\theta(\mathbf{x}_i)\right)^2}\text{, and} \label{eq:mse-test} \\[1em]
  MSE_{f} &= \dfrac{1}{\vert{}\mathcal{X}\vert{}}
  \sum_{i = 1}^{\vert{}\mathcal{X}\vert{}}{\left(f(\mathbf{x}_i) -
    \hat{f}_\theta(\mathbf{x}_i)\right)^2}\text{.} \label{eq:mse-true}
\end{align}
#+end_export

The training error $MSE_{\hat{f}_{\theta}(\mathbf{X})}$ can be computed with the
design  points and  the fitted  model.  In  real applications  we can  sometimes
compute the  testing error $MSE_{\hat{f}_\theta}$  for the entire  search space,
but usually we settle for the error on a testing set distinct from $\mathbf{X}$.
We can almost  never compute the true  error $MSE_{f}$ of our  surrogate, but in
our toy example  we can use the Booth  function without the error term  to get a
sense of how well our models generalize.  As we could expect, since the true $f$
in   our  example   can  be   represented  by   the  basis   functions  on   set
$\mathcal{H}_{(d)}$, the surrogate  from panel (c) has the smallest  MSE and the
generalizes the best, although it still differs from the true $f$.

#+NAME: fig:regression-fitting
#+ATTR_LATEX: :width \textwidth :placement [h!]
#+CAPTION: Three linear model surrogates for the Booth function, with a
#+CAPTION: $\color{red}\boldsymbol{\times}$ marking the
#+CAPTION: global optimum and best surrogate predictions.
#+CAPTION: The fixed experimental design $\mathbf{X}$ used to fit all
#+CAPTION: surrogates is marked by
#+CAPTION: $\color{blue}\boldsymbol{\times}$ s.
#+CAPTION: Panel (a) shows noisy measurements of Booth's function,
#+CAPTION: panels (b), (c), and (d) show surrogate predictions for models fit with
#+CAPTION: basis functions sets
#+CAPTION: from Equations\nbsp{}\ref{eq:hb}, \ref{eq:hc}, and \ref{eq:hd}, respectively
[[file:img/booth_3d_linear_regression.pdf]]

#+latex: \clearpage

We can  use the mean  $\bar{\mathbf{y}}$ of  the observations of  experiments in
$\mathbf{X}$ as a surrogate, for which the MSE is written
#+begin_export latex
\begin{align}
  MSE_{\bar{\mathbf{y}}} &= \dfrac{1}{n}
  \sum_{i = 1}^{n}{\left(\left(f(\mathbf{x}_i) + \varepsilon\right) -
    \bar{\mathbf{y}}\right)^2}\text{,} \label{eq:mse-mean}
\end{align}
#+end_export
shown in  panel (a) from Figure\nbsp{}[[fig:regression-fitting]]. We can use
this surrogate to compute the adjusted coefficient of determination
#+begin_export latex
\begin{align}
  \bar{R}^{2}  = 1  - \left(\dfrac{n - 1}{n - p - 1} \cdot
  \dfrac{MSE_{\hat{f}_{\theta}(\mathbf{X})}}
        {MSE_{\bar{\mathbf{y}}}}\right)\text{,}
\end{align}
#+end_export
which compares the squared  error of a given linear model with  the error of the
model that  predicts the mean of  all observations, adjusted by  the flexibility
introduced by new parameters.

In general, we can measure the complexity  of a surrogate model by the number of
parameters that we  need to estimate when fitting it  to experimental data.  The
bias of a surrogate, for a fixed  point $\mathbf{x}_0$, is the expected value of
the  distance between  the  surrogate's prediction  and $f(\mathbf{x}_0)$,  over
surrogate  fits using  a large  number of  different experimental  designs.  The
squared bias is written
#+begin_export latex
\begin{align}
  \text{Bias}^{2}\left(\hat{f}_{\theta}(\mathbf{x}_0)\right) =
  \left(E\left[\hat{f}_{\theta}(\mathbf{x}_0)\right] - f(\mathbf{x}_0)\right)^2\text{,}
\end{align}
#+end_export
because $E[f(\mathbf{x}_0)] = f(\mathbf{x}_0)$.   As we increase the flexibility
of  a  model  by adding  parameters  we  allow  the  surrogate model  to  better
approximate design  points, reducing its  bias.  Concomitantly, we  increase the
number of  different parameter vectors  that can describe the  experimental data,
increasing the surrogate's variance
#+begin_export latex
\begin{align}
  \text{Var}\left(\hat{f}_{\theta}(\mathbf{x}_0)\right) =
  E\left[\hat{f}_{\theta}(\mathbf{x}_0) - E\left[\hat{f}_{\theta}(\mathbf{x}_0)\right]\right]^{2}\text{.}
\end{align}
#+end_export
The total model  error for the prediction of $\mathbf{x}_0$  still has to factor
in the irreducible error $\varepsilon$  associated with the measurements of $f$,
and is written
#+begin_export latex
\begin{align}
  \text{Error}\left(\mathbf{x}_0\right) =
  \text{Var}(\varepsilon) +
  \text{Bias}^{2}\left(\hat{f}_{\theta}(\mathbf{x}_0)\right) +
  \text{Var}\left(\hat{f}_{\theta}(\mathbf{x}_0)\right)\text{.}
\end{align}
#+end_export
Increasing surrogate  complexity reduces bias  but increases variance,  and this
trade-off is central to selecting and assessing the quality of surrogate models,
however we define the parameter space $\Theta$.

**** Inference: Interpreting Significance with ANOVA
The variance of the OLS estimator enables us to compute confidence intervals and
/p/-values for the effects of each model term, or factor, for a single surrogate
model fit.   In a frequentist interpretation  a 95% confidence interval  for the
estimate of  a mean  of a factor's  effect is interpreted  as the  interval that
would contain  95% of our estimates,  were we to repeat  the estimation multiple
times.   The  /p/-value  of  a   factor  effect's  estimate  is  interpreted  in
frequentist inference  as the  probability of  observing an  effect at  least as
large as what was  observed, if the factor's true effect is  zero. When they can
be computed, confidence intervals are in general more useful than /p/-values for
judging the accuracy of a factor  effect's estimate, because they are explicitly
defined in the context of the magnitude of that estimate.

Analysis of Variance (ANOVA) is a more refined statistical tool for significance
testing, able  to estimate relative factor  significance. The steps of  an ANOVA
test are  grouping the observations  $\mathbf{y}$ by factors and  factor levels,
computing separate group means, and  testing the significance of the differences
between group means with an /F/-test.

The ANOVA test can  be understood as a special case of the  linear model we have
discussed in this section,  in which case its formal hypotheses  are the same as
the  linear   model's,  that   is,  that   the  observations   $\mathbf{y}$  are
uncorrelated, the residuals are normally  distributed, and the variances of each
group are homoscedastic.

Running an ANOVA test as a special  case of the linear model consists of running
/F/-tests for  multiple models  $\mathbf{y} = \mathbf{X}_i\theta$,  with specially
constructed  model  matrices   of  indicator  variables  for   group  and  group
interaction membership.  A  detailed description of ANOVA in  relation to linear
models      can      be     found      Chapter      6      of     Dobson      /et
al./\nbsp{}\cite{dobson2018introduction},       among       other       reference
texts\nbsp{}\cite{agresti2015foundations,seber2015linear}.

**** Linear Models: Interpretable but Biased Surrogates
Linear regression can  be successful in learning  and interpreting relationships
between factors and an objective function $f$.  If the underlying functions have
complex  structure that  cannot be  sufficiently  well represented  by a  finite
number of parameters,  linear models might not be useful  beyond identifying the
strongest factor  effects. The following  section will discuss  Gaussian Process
Regression,  a  nonparametric approach  that  fits  a  model by  conditioning  a
probability distribution over functions.

*** Gaussian Process Regression
Similarly to the  surrogate model we built using linear  models, in this section
we  will fit  a surrogate  $\hat{f}_{\theta}: \mathcal{X}  \to \mathbb{R}$  to a
fixed experimental  design $\mathbf{X}$ and observations  $\mathbf{y}$.  We make
the hypothesis that the observations $y_1, \dots, y_n$ are normally distributed.
This is  a reasonable  hypothesis to  make, especially  after our  discussion on
linear models, where our hypothesis over $f$ was
#+begin_export latex
\begin{align}
  \label{eq:model-noisy}
  f(\mathbf{X}) = \mathbf{X}\theta + \varepsilon\text{,}
\end{align}
#+end_export
with $\varepsilon \sim \mathcal{N}(0,\sigma^{2})$, which makes $f(\mathbf{X})$ a
multivariate random variable, written
#+begin_export latex
\begin{align}
  \label{eq:lin-mod-prob}
  f(\mathbf{X}) \sim \mathcal{N}(\mathbf{X}\theta,
  \sigma^{2}\text{\textbf{I}})\text{,}
\end{align}
#+end_export
where *I* is the $n\times{}p$ identity matrix.

Another way to  state this key idea is  to say that we make  the hypothesis that
$f$ is  a Gaussian  Process, that  is, that it  belongs to  the class  of models
containing all the functions whose values on any set of experiments $\mathbf{X}$
can be  represented by a  single sample  of a multivariate  normal distribution,
with dimension $n = \vert{}\mathbf{X}\vert{}$.   We can write this hypothesis as
a prior probability distribution
#+begin_export latex
\begin{align}
  \label{eq:gauss-prior}
  f(\mathbf{X}) = [y_1\;\dots\;y_n]^{\top} \sim
  \mathcal{N}(\boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0)\text{,}
\end{align}
#+end_export
where the mean vector $\boldsymbol{\mu}_0$ is  usually a vector of $n$ zeros and
the covariance  matrix $\boldsymbol{\Sigma}_0$  is computed  using a  kernel and
depends on $\mathbf{X}$.

This   prior   is  more   general   than   the   linear  model   hypothesis   in
Equation\nbsp{}\ref{eq:lin-mod-prob},  but  it  generalizes  even  further.   In
Chapter       /6/      of       /Gaussian       Processes      for       Machine
Learning/\nbsp{}\cite{williams2006gaussian},  Rasmussen  and  Williams  describe
relationships and  equivalences of Gaussian  Processes to other methods  such as
Splines, Support  Vector Machines, and  OLS estimation.  They also  describe how
Neural Networks  can be  represented by specific  Gaussian Processes  in Section
/4.2.3/.

#+NAME: fig:2d-gaussian
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Effects of three covariance matrices on a multivariate normal distribution
#+CAPTION: with mean vector $\boldsymbol{\mu} = [0\; 0]^{\top}$ and covariance matrix
#+CAPTION: $\boldsymbol{\Sigma}$ as
#+CAPTION: shown on the upper right corner of each plot
[[file:img/2d_gaussian_uncorrelated_annotated.pdf]]

In contrast to  the linear model, the definition and  exploration of a parameter
space  $\Theta$  for  a  Gaussian  Process  surrogate  is  done  indirectly,  by
controlling kernel parameters and noise  amplitudes.  A first approach to define
the parameter space could be explicitly  searching for the covariance matrix and
mean vector that best fit the data,  where the size of the parameter space would
increase  as  more  training  data   becomes  available.   For  a  given  design
$\mathbf{X}$ of size $n$, this parameter space would be
#+begin_export latex
\begin{align}
  \Theta_0 = \left\{\;\left(\boldsymbol{\mu}_{\mathbf{X},\mathbf{y}},\;
  \boldsymbol{\Sigma}_{\mathbf{X}}\right),\;
  \boldsymbol{\mu}_{\mathbf{X},\mathbf{y}} \in \mathbb{R}^{n},\;
  \boldsymbol{\Sigma}_{\mathbf{X}} \in
  \mathbb{R}^{n\times{}n}\;\right\}\text{.}
\end{align}
#+end_export
Note that  the mean vector depends  on the observations $\mathbf{y}$,  while the
covariance matrix depends only on $\mathbf{X}$.  In a multivariate Gaussian, the
diagonal of  the covariance  matrix contains the  variances associated  with its
dimensions, and the off-diagonal elements  contain the covariances between pairs
of  dimensions.    Figure\nbsp{}[[fig:2d-gaussian]]  shows   the  impact   of  three
covariance matrices, which must always be symmetric and positive semi-definite.

Instead  of  looking for  the  best  parameter  vector  in $\Theta_0$,  we  will
condition  the prior  Gaussian distribution  to the  observed data,  obtaining a
posterior distribution.  The conditioned distribution representing our surrogate
model is also a multivariate normal written
#+begin_export latex
\begin{align}
  \label{eq:gauss-posterior}
  \hat{f}_{\theta}(\mathbf{X}) \sim f(\mathbf{X}) \; \vert{} \; \mathbf{X}, \mathbf{y}\text{.}
\end{align}
#+end_export
We will  see how to  compute this  posterior in the  next section. Note  that it
would require  an infinite parameter  vector to fit  a Gaussian Process  for all
points in a  search space $\mathcal{X}$ consisting of a  single real number.  In
this sense, Gaussian Process Regression is a nonparametric method.

#+NAME: fig:matern52-50d-mv-samples
#+ATTR_LATEX: :width .75\textwidth :placement [t]
#+CAPTION: Reinterpreting the unrolled dimensions of 100 samples of a 20 dimension
#+CAPTION: multivariate normal, on the left panel, to obtain 100 samples of functions
#+CAPTION: evaluated on 20 different input points, on the right panel
[[file:img/matern52_50d_mv_samples.pdf]]

To build an intuitive understanding of  Gaussian Processes it can be helpful to
think  of  them as  a  reinterpretation  of the  dimensions  of  a sample  of  a
multivariate  normal distribution.   If we  take 100  samples of  a 20-dimension
Gaussian and unroll each  dimension into a single axis, we end  up with the left
panel  of  Figure\nbsp{}[[fig:matern52-50d-mv-samples]].    Each  column  of  points
contains the 100 values on each dimension  of our samples, and each dimension is
correlated according to the distribution's covariance matrix. The values for one
arbitrary sample are marked in larger red dots.

The right panel of  Figure\nbsp{}[[fig:matern52-50d-mv-samples]] shows the same data
in the left panel  but we now interpret the values of  each dimension $d_i$, for
all   100   samples,  as   a   distribution   of   values  for   the   surrogate
$\hat{f}_{\theta}$     evaluated     at      point     $\mathbf{x}_i$.      Each
$\hat{f}_{\theta}(\mathbf{x}_i)$ is correlated to  other values of the surrogate
function, with  covariance given  by the  distribution's covariance  matrix.  In
this  example we  used the  Matérn  kernel, discussed  in the  next section,  to
compute the covariance  matrix.  This prior over functions  directly estimates a
mean and its associated variance, for each  value of the surrogate.  We will now
discuss how to  fit a Gaussian Process to observed  data, generating predictions
of means and variances conditioned to observations.

**** Fitting the Model: Posterior Distributions over Functions
Before fitting our surrogate, we must compute the covariance matrix of the prior
Gaussian   distribution   from   Equation\nbsp{}\ref{eq:gauss-prior}   using   a
covariance function, or kernel, $K: \mathcal{X}^{2} \to \mathbb{R}$. For any
pair  $(\mathbf{x},   \mathbf{x}^{\prime})  \in  \mathcal{X}^{2}$,   the  kernel
determines how strong the  covariance between $\hat{f}_{\theta}(\mathbf{x})$ and
$\hat{f}_{\theta}(\mathbf{x}^{\prime})$  should   be,  based  on   the  distance
$\lVert{}\mathbf{x}                -               \mathbf{x}^{\prime}\rVert{}$.
Figure\nbsp{}[[fig:radial-basis-kernels]]  shows  four   exponential  kernels,  also
called    radial    basis   functions,    whose    formulas    are   shown    in
Table\nbsp{}\ref{tab:kernel-expressions}.

#+begin_export latex
\begin{table}
  \caption{Expressions for the covariance functions, or kernels, shown in
    Figure \ref{fig:radial-basis-kernels}.
    The variables $v$ and $l$ are kernel parameters that can themselves be estimated.
    The Matérn kernel depends on the gamma function $\Gamma$ and on the
    Bessel function of the second kind $K_{v}$.
    We refer the reader to
    Chapter \textit{4} of Rasmussen and
    Williams \cite{williams2006gaussian} for detailed definitions
    and discussions}
  \label{tab:kernel-expressions}
  \begin{center}
    \begin{tabular}{ll}
      \toprule
      Kernel & Expressions\\
      \midrule
      Exponential & \(k(\mathbf{x},\mathbf{x}^{\prime}) =
      exp\left(-\dfrac{\lVert{}\mathbf{x} -
        \mathbf{x}^{\prime}\rVert{}}{l}\right)\)\\
      \addlinespace[1em]
      Squared Exponential & \(k(\mathbf{x},\mathbf{x}^{\prime}) =
      exp\left(-\dfrac{\lVert{}\mathbf{x} -
        \mathbf{x}^{\prime}\rVert{}^{2}}{2l^{2}}\right)\)\\
      \addlinespace[1em]
      Matérn & \(k(\mathbf{x},\mathbf{x}^{\prime}) =
      \dfrac{1}{2^{v - 1}\Gamma(v)}\left(\dfrac{\sqrt{2v}}{l}\lVert{}\mathbf{x} -
      \mathbf{x}^{\prime}\rVert{}\right)^{v} K_v\left(\dfrac{\sqrt{2v}}{l}\lVert{}\mathbf{x} -
      \mathbf{x}^{\prime}\rVert{}\right)\) \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}
#+end_export

The actual  parameter space $\Theta$  over which optimization is  performed when
fitting a Gaussian Process surrogate is composed by the parameters of the chosen
covariance  kernel, called  hyperparameters.   Typical  hyperparameters are  the
Section  /5.1/  of  Rasmussen  and  Williams\nbsp{}\cite{williams2006gaussian},  .
Explicitly  listing the  hyperparameters for  a Gaussian  Process fit  using the
exponential kernel from Table\nbsp{}\ref{tab:kernel-expressions} would result in
#+begin_export latex
\begin{align}
  k(\mathbf{x},\mathbf{x^{\prime}}) = \sigma_{f}^{2}exp\left(-\dfrac{\lVert{}\mathbf{x} -
    \mathbf{x}^{\prime}\rVert{}}{l}\right)\text{,}
\end{align}
#+end_export
and the parameter space $\Theta$ would be defined as
#+begin_export latex
\begin{align}
  \Theta = \left\{\; \sigma_f, l \in \mathbb{R}\;\right\}\text{.}
\end{align}
#+end_export
We  can then  use the  conditioned posterior  distribution, discussed  below, to
determine   the  best   specific   hyperparameter  values   by  minimizing   the
cross-validated  mean   squared  error,  for   example,  which  we   discuss  in
Section\nbsp{}[[Assessing the Quality of Fit of Gaussian Process Surrogates]], or by
maximizing the posterior likelihood.

The posterior  distribution $\hat{f}_{\theta}$  is computed by  conditioning the
prior  distribution from  Equation\nbsp{}\ref{eq:gauss-prior}  to observed  data
$(\mathbf{X}_k,\mathbf{y}_k)$,      obtaining      the      distribution      in
Equation\nbsp{}\ref{eq:gauss-posterior}, which is  also a Gaussian distribution,
and can be written
#+begin_export latex
\begin{align}
  \label{eq:gauss-posterior-dist}
  \hat{f}_{\theta}(\mathbf{X}_k) \sim
  \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\text{,}
\end{align}
#+end_export
where the  conditioned vector  mean, of size  $\vert{}\mathbf{X}_{k}\vert{}$, is
#+begin_export latex
\begin{align}
  \label{eq:posterior-mean}
  \boldsymbol{\mu}_{k} = \mathbf{K}(\mathbf{X}_k,\mathbf{X}_{k - 1})
  \; \mathbf{K}(\mathbf{X}_{k - 1}, \mathbf{X}_{k - 1})^{-1}
  \; \mathbf{y}_{k - 1}^{\top}\text{,}
\end{align}
#+end_export
and   the  conditioned   $\vert{}   \mathbf{X}_{k}   \vert{}  \times{}   \vert{}
\mathbf{X}_{k} \vert{}$ covariance matrix is written
#+begin_export latex
\begin{align}
  \label{eq:posterior-cov}
  \boldsymbol{\Sigma}_k = \mathbf{K}(\mathbf{X}_k, \mathbf{X}_k)\;
  - \;\mathbf{K}(\mathbf{X}_k, \mathbf{X}_{k - 1})
  \;\mathbf{K}(\mathbf{X}_{k - 1}, \mathbf{X}_{k - 1})^{-1}
  \;\mathbf{K}(\mathbf{X}_{k - 1}, \mathbf{X}_k)\text{.}
\end{align}
#+end_export
The   function  $\mathbf{K}$   in  Equations\nbsp{}\ref{eq:posterior-mean}   and
\ref{eq:posterior-cov}  produces   the  covariance  matrices   corresponding  to
applying one of the covariance kernels $k(\mathbf{x}, \mathbf{x}^{\prime})$ from
Table\nbsp{}\ref{tab:kernel-expressions}  to all  pairs of  points on  the input
designs   for    $\mathbf{K}$.    The    complete   text   of    Rasmussen   and
Williams\nbsp{}\cite{williams2006gaussian},      the      Chapter      6      of
Bishop\nbsp{}\cite{bishop2006pattern}, and  the Chapter  15 of  Kochenderfer and
Wheeler\nbsp{}\cite{kochenderfer2019algorithms} are among the texts that present
detailed discussions and derivations of important properties of Gaussian Process
Regression.

#+NAME: fig:radial-basis-kernels
#+ATTR_LATEX: :width 0.5\textwidth :placement [t]
#+CAPTION: Covariance of points $(\mathbf{x}, \mathbf{x}^{\prime})$
#+CAPTION: according to four covariance functions based on the distance
#+CAPTION: $\lVert{}\mathbf{x} - \mathbf{x}^{\prime}\rVert{}$.
#+CAPTION: Expressions for each kernel are shown in
#+CAPTION: Table\nbsp{}\ref{tab:kernel-expressions}, and Matérn kernels use parameters
#+CAPTION: $v_1,v_2 = \{\frac{3}{2},\frac{5}{2}\}$
[[file:img/radial_basis_kernels.pdf]]

The  left  panel  of  Figure\nbsp{}[[fig:matern52-50d-fitting]]  shows  300  sampled
functions  from   the  prior   in  Equation\nbsp{}\ref{eq:gauss-prior},   for  a
50-dimension Gaussian  distribution, with covariance  matrix $\mathbf{\Sigma}_0$
given by the Matérn  kernel from Table \nbsp{}\ref{tab:kernel-expressions}, with
$v =  \frac{5}{2}$. The starting  mean vector $\boldsymbol{\mu}_0$ is  zero.  We
see     that,     for     all     radial    basis     functions     shown     in
Figure\nbsp{}[[fig:radial-basis-kernels]], the  covariance between  inputs decreases
as the  distance between  inputs increases,  approaching zero.   Using different
kernels we can  control properties of sampled functions, such  as smoothness and
periodicity.   The center  panel of  Figure\nbsp{}[[fig:matern52-50d-fitting]] shows
samples from the conditioned posterior  after a single observation $(\mathbf{x},
\mathbf{y})$,   computed  using   Equations\nbsp{}\ref{eq:gauss-posterior-dist},
\ref{eq:posterior-mean},  and  \ref{eq:posterior-cov}.   The right  panel  shows
samples from  the posterior after  observing two additional  observations.  Note
that all 300  functions sampled from the conditioned  distributions pass exactly
through the observations on the center and rightmost panels.

#+NAME: fig:matern52-50d-fitting
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Fitting a Gaussian Process to three noise-free observations.
#+CAPTION: The left panel shows 300 samples from a Gaussian prior,
#+CAPTION: using the Matérn kernel to compute the covariance matrix.
#+CAPTION: The center and right panels show 300 samples from the
#+CAPTION: posterior distributions conditioned by one, then two more, successive
#+CAPTION: noise-free observations
[[file:img/matern52_50d_fitting.pdf]]

#+NAME: fig:matern52-50d-fitting-noisy
#+ATTR_LATEX: :width \textwidth :placement [h]
#+CAPTION: Fitting a Gaussian Process to three noisy observations,
#+CAPTION: in the same conditions and with the same panel structure
#+CAPTION: in Figure\nbsp{}\ref{fig:matern52-50d-fitting}
[[file:img/matern52_50d_fitting_noisy.pdf]]

Although  in   specific  autotuning  applications  involving   deterministic  or
extremely  fast processes  we can  observe consistent  measurements and  produce
useful  models  with  fits  that  assume  noiseless  measurements,  such  as  in
Figure\nbsp{}[[fig:matern52-50d-fitting]], we  can also  produce a  Gaussian Process
fit     that    incorporates     the     uncertainty     from    noisy     data.
Figure\nbsp{}[[fig:matern52-50d-fitting-noisy]] shows 300 prior samples, in the left
panel, and results of conditioning the  prior distribution to one and then three
observations, under  the assumption  that the  underlying objective  function is
subject  to measurement  error $\varepsilon  \sim \mathcal{N}(0,  \sigma^2)$, as
described in  Equation\nbsp{}\ref{eq:model-noisy}.  In  this scenario,  we write
the        mean       vector        and       covariance        matrix       for
Equation\nbsp{}\ref{eq:gauss-posterior-dist} as
#+begin_export latex
\begin{align}
  \label{eq:posterior-mean-noisy}
  \boldsymbol{\mu}_{k} = \mathbf{K}(\mathbf{X}_k,\mathbf{X}_{k - 1})
  \; (\mathbf{K}(\mathbf{X}_{k - 1}, \mathbf{X}_{k - 1})^{-1} +
  \sigma^{2}\mathbf{I})\;\mathbf{y}_{k - 1}^{\top}\text{,}
\end{align}
#+end_export
and
#+begin_export latex
\begin{align}
  \label{eq:posterior-cov-noisy}
  \boldsymbol{\Sigma}_k = \mathbf{K}(\mathbf{X}_k, \mathbf{X}_k)\;
  - \;\mathbf{K}(\mathbf{X}_k, \mathbf{X}_{k - 1})
  \;(\mathbf{K}(\mathbf{X}_{k - 1}, \mathbf{X}_{k - 1})^{-1} +
  \sigma^{2}\mathbf{I})
  \;\mathbf{K}(\mathbf{X}_{k - 1}, \mathbf{X}_k)\text{.}
\end{align}
#+end_export

Before  discussing  quality  of  fit assessment  metrics  for  Gaussian  Process
surrogates, we will discuss how to  incorporate hypotheses over the search space
to fitted surrogates using basis functions, which we described when we discussed
linear regression.   Trend functions can  be added to the  surrogate's predicted
mean  to  leverage  underlying  trends  in data,  that  should  be  followed  on
prediction regions far from measurements.

Figure\nbsp{}[[fig:gp-trends]] shows Gaussian Process surrogates fitted with and
without  model   trends.   Black  circles   represent  six  measurements   of  a
single-input  function,  and  each  line  represents  a  model  trend.   We  are
considering noise-free  measurements in these  fits, so all surrogates  agree on
the predictions  at the  measurements.  We  can see  that predictions  for input
between or  sufficiently far from  measurements present stronger  influence from
the  underlying trend.   A trend  can help  leverage previous  knowledge of  the
relationships  between  factors  and  the  values  of  the  objective  function,
especially when the cost  of measuring a single point is  expensive, or when new
experiments cannot be performed. However, if we can choose which measurements to
perform   we   can   use   space-filling    designs,   which   we   discuss   on
Chapter\nbsp{}\ref{chap:ed},  to decrease  or sometimes  remove the  need for  a
model trend.

The fits and predictions shown in Figure\nbsp{}[[fig:gp-trends]] were computed using
the  /DiceKriging/ /R/  package\nbsp{}\cite{roustant2018dicekriging}.  We  refer
the reader to the detailed descriptions of trend functions and their application
to  Gaussian Process  Regression presented  in  Section /2.1/  of the  package's
accompanying paper\nbsp{}\cite{roustant2012dicekriging}.

#+NAME: fig:gp-trends
#+ATTR_LATEX: :width 0.5\textwidth :placement [t]
#+CAPTION: Gaussian Process surrogates using three different model trends,
#+CAPTION: fit to six noise-free observations of a single-input objective function,
#+CAPTION: marked with black circles
[[file:img/gp_trends.pdf]]

As we did for the linear model  surrogates discussed earlier in this chapter, in
the  next  section  we will  assess  the  quality  of  fit of  Gaussian  Process
surrogates, and quantify their prediction error  on a testing set outside of the
measurements used for fitting.

**** Assessing the Quality of Fit of Gaussian Process Surrogates
We will construct  three Gaussian Process surrogates for the  Booth function and
use   the   model  fits   to   discuss   the   assessment  of   model   quality.
Figure\nbsp{}[[fig:gp-fitting]] shows  the three  surrogate fits,  constructed using
the  same  design  $\mathbf{X}$  with  $n =  10$  random  uniformly  distributed
measurements of the Booth function
#+begin_export latex
\begin{align}
  f(\mathbf{x} = [x_1, x_2]^{\top}) = (x_1 + 2x_2 - 7)^{2} +
  (2x_1 + x_2 - 5)^{2}, \; x_1,x_2 \in [-10, 10]\text{,}
\end{align}
#+end_export
subject  to  measurement  error  $\varepsilon$.  Panel  (a)  shows  the  surface
produced by measuring $f$ in its entire  domain and the MSE of the constant mean
predictor $MSE_{\bar{\mathbf{y}}}$.  The 10  random measurements in $\mathbf{X}$
are highlighted.  Panels (b), (c), and (d) show the three surrogates, which used
model    trends    using    the     basis    function    sets    described    by
Equations\nbsp{}\ref{eq:hb}, \ref{eq:hc}, and \ref{eq:hd}.  Panels (b), (c), and
(d)  show the  testing  $MSE_{\hat{f}_{\theta}}$ and  true $MSE_{f}$  prediction
errors for  each surrogate, described by  Equations\nbsp{}\ref{eq:mse-test} and
\ref{eq:mse-true}.

We have assumed noisy measurements  when fitting the Gaussian Process surrogates
in this  example.  In  this setting,  we could  compute the  training prediction
error       $MSE_{\hat{f}_{\theta}(\mathbf{X})}$      as       described      by
Equation\nbsp{}\ref{eq:mse-train},  but  this would  not  work  for a  surrogate
assuming noise-free  measurements.  Since  all surrogates would  interpolate the
observations, the prediction  error for points in the training  set would always
be zero.   Still, analogously, in our  example we also controlled  the variances
representing      the     measurement      errors,      as     described      by
Equations\nbsp{}\ref{eq:posterior-mean-noisy} and \ref{eq:posterior-cov-noisy}.

The strategy we used for computing test prediction error for linear models would
not  be  accurate   for  this  example,  and  we  used   a  Leave-One-Out  (LOO)
cross-validation     strategy      to     compute     the      testing     error
$MSE^{LOO}_{\hat{f}_{\theta}}(\mathbf{X})$.  The strategy  consists of computing
the mean  of all $MSE_{\hat{f}_{\theta}(\mathbf{X})}$ for  different surrogates,
with the  same model trend,  fit to the testing  sets generated by  removing one
distinct training point at a time,  for all training points.  This strategy also
works to  compute the testing  prediction error for noise-free  Gaussian Process
fits.

#+NAME: fig:gp-fitting
#+ATTR_LATEX: :width \textwidth :placement [h!]
#+CAPTION: Three Gaussian Process Regression surrogates with noisy fits for the
#+CAPTION: Booth function, with a
#+CAPTION: $\color{red}\boldsymbol{\times}$ marking the
#+CAPTION: global optimum and best surrogate predictions.
#+CAPTION: The fixed experimental design $\mathbf{X}$ used to fit all
#+CAPTION: surrogates is marked by
#+CAPTION: $\color{blue}\boldsymbol{\times}$ s.
#+CAPTION: Panel (a) shows noisy measurements of Booth's function,
#+CAPTION: panels (b), (c), and (d) show surrogate predictions for models fit with
#+CAPTION: linear, quadratic, and quadratic plus interactions trends,
#+CAPTION: respectively
[[file:img/booth_3d_gp.pdf]]

#+latex: \clearpage

**** Inference: Interpreting Significance with Sobol Indices
Sobol  indices\nbsp{}\cite{sobol1993sensitivity}  are  a  variance-based  global
sensitivity analysis method, which computes the relative importance of the input
factors  $\mathbf{x}_1,\dots,\mathbf{x}_p$  of  an  objective  function  $f$  by
decomposing the variance observed in measurements of $f$.  The first-order Sobol
index $S_i$ for each factor $\mathbf{x}_i$ represents the normalized variance of
the expected value of $f$ given a fixed factor value, written
#+begin_export latex
\begin{align}
  S_i = \dfrac{V_{\mathbf{x}_i}\left(
    E_{\mathbf{X} \setminus \mathbf{x}_i}\left(f(\mathbf{X})
    \;\vert\;\mathbf{x}_i\right)\right)}
  {V\left(f(\mathbf{X})\right)}\text{.}
\end{align}
#+end_export
The variance  of a  function can be  further decomposed to  produce $2^{p}  - 1$
indexes  that measure  the  sensitivity  of factor  interactions  of  up to  the
$p^{\text{th}}$  level.  Additionally,  we  can aggregate  the  effects  of  the
interactions of a factor and compute  total-effect indices, in order to decrease
the total number of  indices to compute.

Sobol sensitivity indices can be estimated using  $N (p + 2)$ evaluations of the
objective  function, where  $N$ controls  the accuracy  of the  estimate and  is
typically in the order of thousands. The Monte Carlo method described in Section
/4.1.6/  of  Saltelli  /et al./\nbsp{}\cite{saltelli2008global}  determines  how  to
construct  the experiment  matrices  containing all  samples,  and computes  the
sensitivity  indices  using the  estimators  described  in  the book  and  later
reviewed and summarized by the authors\nbsp{}\cite{saltelli2010variance}.

**** Gaussian Processes: Flexible but Hard to Interpret Surrogates
Gaussian Process Regression is a nonparametric learning method with low bias and
high  variance,  that  can  produce   surrogate  models  capable  of  describing
observations with  essentially arbitrary precision, without  entirely committing
to structure outside of observations.  Although  we can attempt to interpret the
results produced by a Gaussian Process fit  with Sobol indices, it is not always
possible or viable to collect sufficient data to reach accurate results.  Recent
work  interprets  Gaussian  Process  fits by  identifying  the  contribution  of
different  covariance  kernels  to  the  fitted  model.   Such  approaches  help
constructing  useful models  in  high-dimensional spaces,  and  are valuable  to
experimental research, such as the studies we present in this thesis.
#+latex: \todo[inline]{Add reference to PhD thesis from Rasmussen's student}

*** Summary
This chapter presented Linear and Gaussian Process Regression, two large classes
of  surrogate  model  construction  methods.   From the  point  of  view  of  an
experimenter, a  practical difference  between these two  classes lies  in their
balance  of  interpretability  and  generality.  Linear  models  are  easier  to
interpret,  they have  high bias  and  low variance,  and  can be  used to  test
hypotheses    about    search    space     structure,    or    leverage    known
relationships. Gaussian  Process Regression is  hard to interpret, has  low bias
and high variance, and  can be used to model search spaces for  which it is hard
to elaborate  clear hypotheses, or where  simple structure does not  exist to be
exploited.

In this chapter we assumed that the design matrix $\mathbf{X}$ is a given, or is
sampled from a fixed  set of observations.  In the next  chapter we will discuss
methods of Experimental Design that  enable the construction of carefully chosen
design matrices  for different purposes,  such as testing  hypotheses, improving
model fits, or minimizing the objective function.

** Experimental Design
:PROPERTIES:
:CUSTOM_ID: chap:ed
:END:

#+latex: \todo[inline]{TODO: fill in and adapt the sections below}

This chapter presents some elements  of Experimental Design (ED).  Complementing
the learning  methods we discussed  in the  previous chapter, ED  methods enable
choosing the experiments on a  design matrix $\mathbf{X}$ according to different
needs and criteria.  A well-constructed  design enables testing hypotheses about
the objective function, which  can lead either to a useful model or  to a set of
questions to ask and hypotheses to test.

This  chapter   is  organized  as   follows.   Section\nbsp{}\ref{sec:factorial}
introduces key concepts and presents 2-level Full Factorial designs.
Section\nbsp{}\ref{sec:screening}
Section\nbsp{}\ref{sec:optimal}
Section\nbsp{}\ref{sec:filling}
Section\nbsp{}\ref{sec:comparing}
Section\nbsp{}\ref{sec:summary-ed}

of /2-level screening  designs/, an efficient way to identify  main effects.  We
then discuss  techniques for the  construction of efficient designs  for factors
with arbitrary numbers and types of levels, and present /D-Optimal/ designs, the
method used by our approach.

*** Estimating Linear Effects
:PROPERTIES:
:CUSTOM_ID: sec:factorial
:END:

#+latex: \clearpage

#+NAME: fig:lineffect
#+CAPTION: Linear effects
#+ATTR_LATEX: :placement [h] :width .65\textwidth
[[./img/experimental_design/lin_effects.pdf]]

Figure\nbsp{}[[fig:lineffect]] shows the effects

**** 2-Level Factorial Designs

*** Parsimony for Linear Effects with Screening Designs
:PROPERTIES:
:CUSTOM_ID: sec:screening
:END:
- Super efficient but very limited
**** Plackett-Burman Designs
***** Paley Construction of Hadamard Matrices
- Describe algorithm
**** An Example of Screening with Plackett-Burman Designs
#+begin_export latex
\todo[inline]{TODO: Adapt this section from the CCGRID paper.
  It does not make sense to reference steps from the DLMT method here.}
#+end_export

Screening designs identify parsimoniously the main effects of 2-level factors in
the initial stages of studying a  problem. While interactions are not considered
at this stage, identifying main effects  early enables focusing on a smaller set
of factors on subsequent experiments.  A specially efficient design construction
technique   for    screening   designs    was   presented   by    Plackett   and
Burman\nbsp{}\cite{plackett1946design} in  1946, and is available  in the =FrF2=
package\nbsp{}\cite{gromping2014frf2}            of           the            =R=
language\nbsp{}\cite{team2018rlanguage}.

Despite  having  strong  restrictions  on   the  number  of  factors  supported,
Plackett-Burman designs enable the identification of main effects of $n$ factors
with $n  + 1$  experiments. Factors  may have  many levels,  but Plackett-Burman
designs  can  only  be  constructed  for  2-level  factors.   Therefore,  before
constructing a Plackett-Burman  design we must identify /high/  and /low/ levels
for each factor.

#+begin_export latex
\begin{table}[b]
    \centering
    \scriptsize
    \caption{A Plackett-Burman design for 7 2-level factors, where low and high levels are represented by $-1$ and $1$, respectively}
    \label{tab:screening}
    \begin{tabular}{@{}cccccccc@{}}
        \toprule
        Run & A & B & C & D & E & F & G \\ \midrule
        1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 \\
        2 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 \\
        3 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 \\
        4 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 \\
        5 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 \\
        6 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 \\
        7 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!30}1 & \cellcolor{gray!10}-1 & \cellcolor{gray!30}1 \\
        8 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1 & \cellcolor{gray!10}-1  \\ \bottomrule
    \end{tabular}
\end{table}
#+end_export

Assuming a linear  relationship between factors and the  response is fundamental
for  running ANOVA  tests using  a Plackett-Burman  design. Consider  the linear
relationship
#+LATEX: \vspace{-2pt}
#+BEGIN_EXPORT latex
\begin{equation}
\mathbf{Y} = \bm{\beta}\mathbf{X} + \varepsilon\text{,}
\label{eq:linear_assumption}
\vspace{-2pt}
\end{equation}\noindent
#+END_EXPORT
where $\varepsilon$  is the error  term, $\mathbf{Y}$ is the  observed response,
$\mathbf{X}  =  \left\{1,  x_1,\dots,x_n\right\}$  is the  set  of  $n$  2-level
factors, and $\bm{\beta} = \left\{\beta_0,\dots,\beta_n\right\}$ is the set with
the  /intercept/ $\beta_0$  and the  corresponding /model  coefficients/.  ANOVA
tests can  rigorously compute the significance  of each factor, we  can think of
that intuitively by noting that less significant factors will have corresponding
values in $\bm{\beta}$ close to zero.

The  next example  illustrates the  screening  methodology. Suppose  we wish  to
minimize  a performance  metric $Y$  of a  problem with  factors $x_1,\dots,x_8$
assuming values  in $\left\{-1,  -0.8, -0.6, \dots,  0.6, 0.8,  1\right\}$. Each
$y_i \in Y$ is defined as
#+LATEX: \vspace{-2pt}
#+BEGIN_EXPORT latex
\begin{align}
\label{eq:real_model}
y_i = & -1.5x_1 + 1.3x_3 + 3.1x_5 + \\
& -1.4x_7 + 1.35x_8^2 + 1.6x_3x_5 + \varepsilon\text{.} \nonumber
\vspace{-2pt}
\end{align}\noindent
#+END_EXPORT
Suppose that, for the purpose of this example, the computation is done by a very
expensive  black-box procedure.   Note  that factors  $\{x_2,x_4,x_6\}$ have  no
contribution to the  response, and we can think of  the error term $\varepsilon$
as representing not only noise, but  our uncertainty regarding the model. Higher
amplitudes of $\varepsilon$  might make isolating factors  with low significance
harder to justify.

To  study this  problem efficiently  we decided  to construct  a Plackett-Burman
design, which minimizes the experiments  needed to identify significant factors.
The analysis of this design will enable decreasing the dimension of the problem.
Table\nbsp{}\ref{tab:plackett} presents the Plackett-Burman design we generated.
It contains  high and low  values, chosen  to be $-1$  and $1$, for  the factors
$x_1,\dots,x_8$, and the observed response  $\mathbf{Y}$.  It is a required step
to add the 3 ``dummy'' factors $d_1,\dots,d_3$ to complete the 12 columns needed
to       construct       a        Plackett-Burman       design       for       8
factors\nbsp{}\cite{plackett1946design}.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(FrF2)
library(xtable)

set.seed(3138989)

get_cost <- function(data) {
    return((-1.5 * as.numeric(data$x1)) + (1.3 * as.numeric(data$x3)) +
           (1.6 * as.numeric(data$x1) * as.numeric(data$x3)) +
           (1.35 * as.numeric(data$x8) * as.numeric(data$x8)) +
           (3.1 * as.numeric(data$x5)) + (-1.4 * as.numeric(data$x7)) +
           rnorm(nrow(data), sd = 0.6))
}

objective_data <- expand.grid(seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2))

names(objective_data) <- c("x1", "x3", "x5",
                           "x7", "x8")

objective_data$Y <- get_cost(objective_data)

options(warn = -1)
design <- pb(12, factor.names = c("x1", "x2", "x3",
                                  "x4", "x5", "x6",
                                  "x7", "x8", "d1",
                                  "d2", "d3"))
options(warn = 0)

design$Y <- get_cost(design)

names(design) <- c("$x_1$", "$x_2$", "$x_3$",
                   "$x_4$", "$x_5$", "$x_6$",
                   "$x_7$", "$x_8$", "$d_1$",
                   "$d_2$", "$d_3$", "$Y$")

cap <- "Randomized Plackett-Burman design for factors $x_1, \\dots, x_8$, using 12 experiments and ``dummy'' factors $d_1, \\dots, d_3$, and computed response $\\mathbf{Y}$"
tab <- xtable(design, caption = cap, label = "tab:plackett")
align(tab) <- "ccccccccccccc"
print(tab, booktabs = TRUE,
      include.rownames = FALSE,
      caption.placement = "top",
      size = "\\scriptsize",
      table.placement="b",
      sanitize.text.function = function(x){x})
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct 15 18:04:00 2018
\begin{table}[b]
\centering
\caption{Randomized Plackett-Burman design for factors $x_1, \dots, x_8$, using 12 experiments and ``dummy'' factors $d_1, \dots, d_3$, and computed response $\mathbf{Y}$}
\label{tab:plackett}
\begingroup\scriptsize
\begin{tabular}{cccccccccccc}
  \toprule
$x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ & $x_8$ & $d_1$ & $d_2$ & $d_3$ & $Y$ \\
  \midrule
1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 13.74 \\
  -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & 10.19 \\
  -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & 9.22 \\
  1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 7.64 \\
  1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 8.63 \\
  -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & 11.53 \\
  -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & 2.09 \\
  1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 9.02 \\
  1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 10.68 \\
  1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 11.23 \\
  -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & 5.33 \\
  -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & 14.79 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

So    far,    we     have    performed    steps    1,    2,     and    3    from
Figure\nbsp{}[[fig:doe_anova_strategy]].   We  use  our  initial  assumption  in
Equation\nbsp{}\eqref{eq:linear_assumption}  to  identify the  most  significant
factors   by    performing   an   ANOVA    test,   which   is   step    5   from
Figure\nbsp{}[[fig:doe_anova_strategy]].     The    results   are    shown    in
Table\nbsp{}\ref{tab:anova_linear}, where  the /significance/ of each  factor is
interpreted    from    the    F-test    and    $\text{Pr}(>\text{F})$    values.
Table\nbsp{}\ref{tab:anova_linear}  uses  ``$*$'',  as   is  convention  in  the
\texttt{R} language, to represent the significance values for each factor.

We      see     on      Table\nbsp{}\ref{tab:anova_linear}     that      factors
$\left\{x_3,x_5,x_7,x_8\right\}$ have at least  one ``$*$'' of significance. For
the purpose of  this example, this is  sufficient reason to include  them in our
linear  model  for  the  next  step.   We decide  as  well  to  discard  factors
$\left\{x_2,x_4,x_6\right\}$ from our  model, due to their  low significance. We
see that factor $x_1$ has a significance mark of ``\cdot'', but comparing F-test
and $\text{Pr}(>\text{F})$  values we decide  that they are fairly  smaller than
the values of factors that had no significance, and we keep this factor.

#+HEADER: :results graphics output :session *R* :exports none :eval no-export
#+HEADER: :file ./img/ccgrid19/main_effects.pdf
#+HEADER: :width 12 :height 4
#+BEGIN_SRC R
library(extrafont)

names(design) <- c("x1", "x2", "x3",
                   "x4", "x5", "x6",
                   "x7", "x8", "d1",
                   "d2", "d3", "Y")

regression <- lm(Y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = design)

par(family = 'serif')
MEPlot(regression, main = NULL, pch = 19,
       lwd = 0, cex.xax = 2.9, cex.main = 3.1,
       cex.axis = 1)
#+END_SRC

#+RESULTS:
[[file:./img/ccgrid19/main_effects.pdf]]

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)

options(warn = -1)
names(design) <- c("x1", "x2", "x3",
                   "x4", "x5", "x6",
                   "x7", "x8", "d1",
                   "d2", "d3", "Y")

regression <- aov(Y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = design)
s_regression <- as.data.frame(summary.aov(regression)[[1]])
s_regression <- s_regression[1:8, c("F value", "Pr(>F)")]

s_regression$stars <- symnum(s_regression[ , "Pr(>F)"], na = FALSE,
                             cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                             symbols = c("$***$", "$**$", "$*$", "$\\cdot$", " "))

names(s_regression) <- c("F value", "$\\text{Pr}(>\\text{F})$", "Signif.")

rownames(s_regression) <- c("$x_1$", "$x_2$", "$x_3$",
                            "$x_4$", "$x_5$", "$x_6$",
                            "$x_7$", "$x_8$")

cap <- "Shortened ANOVA table for the fit of the naive model, with significance intervals from the \\texttt{R} language"
x <- xtable(s_regression, caption = cap, digits = 3, display = c("s", "f", "f", "s"), label = "tab:anova_linear")
align(x) <- xalign(x)
options(warn = 0)
print(x, size = "\\small",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      sanitize.text.function = function(x){x},
      table.placement="t",
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct 15 18:04:10 2018
\begin{table}[t]
\centering
\caption{Shortened ANOVA table for the fit of the naive model, with significance intervals from the \texttt{R} language}
\label{tab:anova_linear}
\begingroup\small
\begin{tabular}{lrrl}
  \toprule
 & F value & $\text{Pr}(>\text{F})$ & Signif. \\
  \midrule
$x_1$ & 8.382 & 0.063 & $\cdot$ \\
  $x_2$ & 0.370 & 0.586 &   \\
  $x_3$ & 80.902 & 0.003 & $**$ \\
  $x_4$ & 0.215 & 0.675 &   \\
  $x_5$ & 46.848 & 0.006 & $**$ \\
  $x_6$ & 5.154 & 0.108 &   \\
  $x_7$ & 13.831 & 0.034 & $*$ \\
  $x_8$ & 59.768 & 0.004 & $**$ \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

Moving forward to steps 6,  7, and 8 in Figure\nbsp{}[[fig:doe_anova_strategy]],
we will build a linear model using factors $\left\{x_1,x_3,x_5,x_7,x_8\right\}$,
fit the model using  the values of $Y$ we obtained when  running our design, and
use model  coefficients to predict the  levels of each factor  that minimize the
real response. We  can do that because these factors  are numerical, even though
only discrete values are allowed.

We now proceed to  the prediction step, where we wish to  identify the levels of
factors  $\left\{x_1,x_3,x_5,x_7,x_8\right\}$ that  minimize  our fitted  model,
without running any new  experiments. This can be done by,  for example, using a
gradient descent  algorithm or  finding the  point where  the derivative  of the
function given by the linear regression equals to zero.

Table\nbsp{}\ref{tab:linear_prediction_comparison}  compares the  prediction for
$Y$     from    our     linear     model    with     the    selected     factors
$\left\{x_1,x_3,x_5,x_7,x_8\right\}$ with the actual global minimum $Y$ for this
problem.  Note  that factors  $\left\{x_2,x_4,x_6\right\}$ are included  for the
global minimum. This  happens here because of the error  term $\varepsilon$, but
could also be interpreted as due to model uncertainty.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)
library(dplyr)

names(design) <- c("x1", "x2", "x3",
                   "x4", "x5", "x6",
                   "x7", "x8", "d1",
                   "d2", "d3", "Y")

design <- lapply(design, function(x){return(as.numeric(as.character(x)))})

regression <- lm(Y ~ x1 + x3 + x5 + x7 + x8, data = design)
prediction <- predict(regression, newdata = objective_data)

comparison_data <- objective_data[prediction == min(prediction), ]
comparison_data <- bind_rows(comparison_data, objective_data[objective_data$Y == min(objective_data$Y), ])
rownames(comparison_data) <- c(" Lin.", "Min.")

names(comparison_data) <- c("$x_1$", "$x_3$", "$x_5$",
                            "$x_7$", "$x_8$", "$Y$")

comparison_data[ , "$x_2$"] <- c("--", -0.2)
comparison_data[ , "$x_4$"] <- c("--", 0.6)
comparison_data[ , "$x_6$"] <- c("--", 0.4)

comparison_data <- comparison_data[ , c("$x_1$", "$x_2$", "$x_3$",
                                        "$x_4$", "$x_5$", "$x_6$",
                                        "$x_7$", "$x_8$", "$Y$")]

names(comparison_data) <- c("$\\bm{x_1}$", "$x_2$", "$\\bm{x_3}$",
                            "$x_4$", "$\\bm{x_5}$", "$x_6$",
                            "$\\bm{x_7$}", "$\\bm{x_8}$", "$Y$")

cap <- "Comparison of the response $Y$ predicted by the linear model and the true global minimum. Factors used in the model are bolded"
x <- xtable(comparison_data, caption = cap, digits = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 3), label = "tab:linear_prediction_comparison")
align(x) <- xalign(x)
options(warn = 0)
print(x,
      size = "\\footnotesize",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      include.rownames = TRUE,
      sanitize.text.function = function(x){x},
      tabular.environment = "tabularx",
      width = "\\columnwidth",
      table.placement="b",
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct 15 18:04:58 2018
\begin{table}[b]
\centering
\caption{Comparison of the response $Y$ predicted by the linear model and the true global minimum. Factors used in the model are bolded}
\label{tab:linear_prediction_comparison}
\begingroup\footnotesize
\begin{tabularx}{\columnwidth}{lrlrlrlrrr}
  \toprule
 & $\bm{x_1}$ & $x_2$ & $\bm{x_3}$ & $x_4$ & $\bm{x_5}$ & $x_6$ & $\bm{x_7$} & $\bm{x_8}$ & $Y$ \\
  \midrule
 Lin. & -1.0 & -- & -1.0 & -- & -1.0 & -- & 1.0 & -1.0 & -1.046 \\
  Min. & 1.0 & -0.2 & -1.0 & 0.6 & -1.0 & 0.4 & 0.8 & 0.0 & -9.934 \\
   \bottomrule
\end{tabularx}
\endgroup
\end{table}
#+END_EXPORT

Using 12 measurements and a simple linear model, the predicted best value of $Y$
was  around $10\times$  larger  than the  global optimum.  Note  that the  model
predicted the  correct levels  for $x_3$  and $x_5$, and  almost for  $x_7$. The
linear model  predicted wrong  levels for  $x_1$, perhaps  due to  this factor's
interaction with  $x_3$, and  for $x_8$.   Arguably, it  would be  impossible to
predict the correct  level for $x_8$ using this linear  model, since a quadratic
term    composes   the    true   formula    of   $Y$.     As   we    showed   in
Figure\nbsp{}[[fig:sampling_comparison]],  a  D-Optimal  design using  a  linear
model  could detect  the significance  of a  quadratic term,  but the  resulting
regression will often lead to the wrong level.

We  can improve  upon this  result if  we introduce  some information  about the
problem and  use a more  flexible design  construction technique. Next,  we will
discuss the  construction of  efficient designs using  problem-specific formulas
and continue the optimization of our example.

*** Increasing Flexibility: Optimal Design
:PROPERTIES:
:CUSTOM_ID: sec:optimal
:END:
- Extensive theory development
  - General equivalence of design criteria

- A  "flexible" screening:  allows  to include  non-linear terms,  interactions,
  etc. if needed.

- Awesome but if  a parameter was not included  in the model or if  the model is
  too simple (e.g. only comprised a linear term where a quadratic one would have
  been needed, or  an important interaction was not included),  we won't be able
  to detect it (lack of fit).

**** The KL-Exchange Algorithm
- Add pseudocode, or schematic figure
- Fedorov's algorithm is a special case of this

**** An Example with D-Optimal Designs
#+begin_export latex
\todo[inline]{TODO: Adapt this section from the CCGRID paper.
  It does not make sense to reference steps from the DLMT method here.}
#+end_export

The  application of  ED  to autotuning  problems  requires design  construction
techniques  that  support factors  of  arbitrary  types  and number  of  levels.
Autotuning problems typically combine factors  such as binary flags, integer and
floating point numerical values, and  unordered enumerations of abstract values.
Because Plackett-Burman designs only support 2-level factors, we had to restrict
factor levels  to interval extremities in  our example.  We have  seen that this
restriction makes it  difficult to measure the significance  of quadratic terms.
We  will now  show  how to  optimize  our example  further  by using  /D-Optimal
designs/, which increase the number of  levels we can efficiently screen for and
enables detecting the significance of more complex terms.

To construct  a D-Optimal  design it  is necessary to  choose an  initial model,
which can be  done based on previous  experiments or on expert  knowledge of the
problem.  Once  a model  is selected, algorithmic  construction is  performed by
searching for the set of experiments that minimizes /D-Optimality/, a measure of
the /variance/ of the /estimators/  for the /regression coefficients/ associated
with the  selected model. This  search is  usually done by  swapping experiments
from  the current  candidate design  with experiments  from a  pool of  possible
experiments, according to  certain rules, until some stopping  criterion is met.
In  the  example  in  this  section   and  in  our  approach  we  use  Fedorov's
algorithm\nbsp{}\cite{fedorov1972theory}  for  constructing  D-Optimal  designs,
implemented in =R= in the =AlgDesign= package\nbsp{}\cite{wheeler2014algdesign}.

In our example, suppose that in addition to using our previous screening results
we decide  to hire an  expert in our problem's  domain. The expert  confirms our
initial assumptions that the factor $x_1$  should be included in our model since
it is usually significant for this kind  of problem and has a strong interaction
with factor $x_3$. She also mentions we should replace the linear term for $x_8$
by a quadratic term for this factor.

Using our previous screening and the domain knowledge provided by our expert, we
choose a new performance model and use  it to construct a D-Optimal design using
Fedorov's algorithm. Since  we need enough degrees of freedom  to fit our model,
we     construct    the     design    with     12    experiments     shown    in
Table\nbsp{}\ref{tab:d_optimal}.   Note that  the design  includes levels  $-1$,
$0$, and $1$ for factor $x_8$. The  design will sample from different regions of
the   search   space   due   to   the  quadratic   term,   as   was   shown   in
Figure\nbsp{}[[fig:sampling_comparison]].

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)
library(dplyr)
library(AlgDesign)

output <- optFederov(~ x1 + x3 + x5 + x7 + x8 + I(x8 ^ 2) + x1:x3,
                     nTrials = 12,
                     data = objective_data)

dopt_design <- output$design

dopt_regression <- lm(Y ~ x1 + x3 + x5 + x7 + x8 + I(x8 ^ 2) + x1:x3, data = dopt_design)
dopt_prediction <- predict(dopt_regression, newdata = objective_data)

dopt_data <- objective_data[dopt_prediction == min(dopt_prediction), ]
names(dopt_data) <- c("$x_1$", "$x_3$", "$x_5$",
                      "$x_7$", "$x_8$", "$Y$")

names(dopt_design) <- c("$x_1$", "$x_3$", "$x_5$",
                        "$x_7$", "$x_8$", "$Y$")

cap <- "D-Optimal design constructed for the factors $\\left\\{x_1,x_3,x_5,x_7,x_8\\right\\}$ and computed response $Y$"
x <- xtable(dopt_design, caption = cap, digits = c(1, 1, 1, 1, 1, 1, 3), label = "tab:d_optimal")
align(x) <- xalign(x)
options(warn = 0)
print(x,
      size = "\\footnotesize",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      include.rownames = FALSE,
      sanitize.text.function = function(x){x},
      table.placement="t",
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct 15 18:05:40 2018
\begin{table}[t]
\centering
\caption{D-Optimal design constructed for the factors $\left\{x_1,x_3,x_5,x_7,x_8\right\}$ and computed response $Y$}
\label{tab:d_optimal}
\begingroup\footnotesize
\begin{tabular}{rrrrrr}
  \toprule
$x_1$ & $x_3$ & $x_5$ & $x_7$ & $x_8$ & $Y$ \\
  \midrule
-1.0 & -1.0 & -1.0 & -1.0 & -1.0 & 2.455 \\
  -1.0 & 1.0 & 1.0 & -1.0 & -1.0 & 6.992 \\
  1.0 & -1.0 & -1.0 & 1.0 & -1.0 & -7.776 \\
  1.0 & 1.0 & 1.0 & 1.0 & -1.0 & 4.163 \\
  1.0 & 1.0 & -1.0 & -1.0 & 0.0 & 0.862 \\
  -1.0 & 1.0 & 1.0 & -1.0 & 0.0 & 5.703 \\
  1.0 & -1.0 & -1.0 & 1.0 & 0.0 & -9.019 \\
  -1.0 & -1.0 & 1.0 & 1.0 & 0.0 & 2.653 \\
  -1.0 & -1.0 & -1.0 & -1.0 & 1.0 & 1.951 \\
  1.0 & -1.0 & 1.0 & -1.0 & 1.0 & 0.446 \\
  -1.0 & 1.0 & -1.0 & 1.0 & 1.0 & -2.383 \\
  1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 4.423 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

#+HEADER: :results output latex :session *R* :exports none :eval no-export
#+BEGIN_SRC R
s_regression <- as.data.frame(summary.aov(dopt_regression)[[1]])
s_regression <- s_regression[1:7, c("F value", "Pr(>F)")]

rownames(s_regression) <- c("$x_1$", "$x_3$", "$x_5$",
                            "$x_7$", "$x_8$", "I($x_8^2$)",
                            "$x_1$:$x_3$")

s_regression$stars <- symnum(s_regression[ , "Pr(>F)"], na = FALSE,
                             cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                             symbols = c("$***$", "$**$", "$*$", "$\\cdot$", " "))

names(s_regression) <- c("F value", "Pr$(<\\text{F})$", "Signif.")

cap <- paste("Shortened ANOVA table for the fit of the naive model (", attributes(s_regression[ , "Signif."])$legend, ")", sep = "")
x <- xtable(s_regression, caption = cap, digits = 3, display = c("s", "f", "f", "s"))
align(x) <- xalign(x)
options(warn = 0)
print(x, size = "\\small",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      sanitize.text.function = function(x){x},
      table.placement="ht",
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct 15 18:05:50 2018
\begin{table}[ht]
\centering
\caption{Shortened ANOVA table for the fit of the naive model (0 ‘$***$’ 0.001 ‘$**$’ 0.01 ‘$*$’ 0.05 ‘$\cdot$’ 0.1 ‘ ’ 1)}
\begingroup\small
\begin{tabular}{lrrl}
  \toprule
 & F value & Pr$(<\text{F})$ & Signif. \\
  \midrule
$x_1$ & 461.596 & 0.000 & $***$ \\
  $x_3$ & 661.241 & 0.000 & $***$ \\
  $x_5$ & 721.313 & 0.000 & $***$ \\
  $x_7$ & 293.822 & 0.000 & $***$ \\
  $x_8$ & 2.297 & 0.204 &   \\
  I($x_8^2$) & 38.191 & 0.003 & $**$ \\
  $x_1$:$x_3$ & 398.202 & 0.000 & $***$ \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

We  now fit  this model  using the  results of  the experiments  in our  design.
Table\nbsp{}\ref{tab:correct_fit}  shows the  model fit  table and  compares the
estimated and  real model  coefficients. This example  illustrates that  the ED
approach can achieve  close model estimations using few  resources, provided the
approach  is  able to  use  user  input  to  identify significant  factors,  and
knowledge about the problem domain to tweak the model.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
s_regression <- as.data.frame(coef(summary.lm(dopt_regression)))
s_regression <- s_regression[, c("Estimate", "t value", "Pr(>|t|)")]

rownames(s_regression) <- c("Intercept", "$x_1$", "$x_3$", "$x_5$",
                            "$x_7$", "$x_8$", "$x_8^2$","$x_1x_3$")

s_regression$Significance <- symnum(s_regression[ , "Pr(>|t|)"], na = FALSE,
                                    cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                                    symbols = c("***", "**", "*", ".", " "))

names(s_regression) <- c("Estimated", "t value", "Pr$(>|\\text{t}|)$", "Signif.")
s_regression$Real <- c(0, -1.5, 1.3, 3.1, -1.4, 0, 1.35, 1.6)

s_regression <- s_regression[ , c("Real", "Estimated", "t value", "Pr$(>|\\text{t}|)$", "Signif.")]

cap <- "Correct model fit comparing real and estimated coefficients, with significance intervals from the \\texttt{R} language"
x <- xtable(s_regression, caption = cap, digits = 3, display = c("s", "f", "f", "f", "f", "s"), label = "tab:correct_fit")
align(x) <- xalign(x)
options(warn = 0)
print(x, size = "\\small",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      sanitize.text.function = function(x){x},
      table.placement="b",
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct 15 18:06:01 2018
\begin{table}[b]
\centering
\caption{Correct model fit comparing real and estimated coefficients, with significance intervals from the \texttt{R} language}
\label{tab:correct_fit}
\begingroup\small
\begin{tabular}{lrrrrl}
  \toprule
 & Real & Estimated & t value & Pr$(>|\text{t}|)$ & Signif. \\
  \midrule
Intercept & 0.000 & 0.050 & 0.305 & 0.776 &   \\
  $x_1$ & -1.500 & -1.452 & -14.542 & 0.000 & *** \\
  $x_3$ & 1.300 & 1.527 & 15.292 & 0.000 & *** \\
  $x_5$ & 3.100 & 2.682 & 26.857 & 0.000 & *** \\
  $x_7$ & -1.400 & -1.712 & -17.141 & 0.000 & *** \\
  $x_8$ & 0.000 & -0.175 & -1.516 & 0.204 &   \\
  $x_8^2$ & 1.350 & 1.234 & 6.180 & 0.003 & ** \\
  $x_1x_3$ & 1.600 & 1.879 & 19.955 & 0.000 & *** \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

Table\nbsp{}\ref{tab:prediction_comparisons} compares the global minimum of this
example with the predictions made by our initial linear model from the screening
step, and  our improved  model. Using screening,  D-Optimal designs,  and domain
knowledge,  we found  an optimization  within 10%  of the  global optimum  while
computing $Y$  only 24  times. We  were able to  do that  by first  reducing the
problem's dimension  when we eliminated  insignificant factors in  the screening
step.   We then  constructed  a more  careful exploration  of  this new  problem
subspace, aided  by domain knowledge provided  by an expert. Note  that we could
have reused some of the 12 experiments from the previous step to reduce the size
of the new design further.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
dopt_data[ , "$x_2$"] <- c("--")
dopt_data[ , "$x_4$"] <- c("--")
dopt_data[ , "$x_6$"] <- c("--")

tab_dopt_data <- dopt_data[ , c("$x_1$", "$x_2$", "$x_3$",
                                "$x_4$", "$x_5$", "$x_6$",
                                "$x_7$", "$x_8$", "$Y$")]

names(tab_dopt_data) <- c("$\\bm{x_1}$", "$x_2$", "$\\bm{x_3}$",
                          "$x_4$", "$\\bm{x_5}$", "$x_6$",
                          "$\\bm{x_7$}", "$\\bm{x_8}$", "$Y$")

dopt_comparison_data <- bind_rows(tab_dopt_data, comparison_data)
rownames(dopt_comparison_data) <- c(" Quad.", "Lin.", "Min.")

cap <- "Comparison of the response $Y$ predicted by our models and the true global minimum. Factors used in the models are bolded"
x <- xtable(dopt_comparison_data, caption = cap, digits = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 3), label = "tab:prediction_comparisons")
align(x) <- xalign(x)
options(warn = 0)
print(x,
      size = "\\footnotesize",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      include.rownames = TRUE,
      sanitize.text.function = function(x){x},
      table.placement="ht",
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct 15 18:06:07 2018
\begin{table}[ht]
\centering
\caption{Comparison of the response $Y$ predicted by our models and the true global minimum. Factors used in the models are bolded}
\label{tab:prediction_comparisons}
\begingroup\footnotesize
\begin{tabular}{lrlrlrlrrr}
  \toprule
 & $\bm{x_1}$ & $x_2$ & $\bm{x_3}$ & $x_4$ & $\bm{x_5}$ & $x_6$ & $\bm{x_7$} & $\bm{x_8}$ & $Y$ \\
  \midrule
 Quad. & 1.0 & -- & -1.0 & -- & -1.0 & -- & 1.0 & 0.0 & -9.019 \\
  Lin. & -1.0 & -- & -1.0 & -- & -1.0 & -- & 1.0 & -1.0 & -1.046 \\
  Min. & 1.0 & -0.2 & -1.0 & 0.6 & -1.0 & 0.4 & 0.8 & 0.0 & -9.934 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

We are able to explain the performance  improvements we obtained in each step of
the process, because we finish steps  with a performance model and a performance
prediction. Each  factor is  included or removed  using information  obtained in
statistical tests,  or expert  knowledge. If  we need  to optimize  this problem
again,  for a  different  architecture  or with  larger  input,  we could  start
exploring the search space  with a less naive model. We  could also continue the
optimization    of   this    problem    by   exploring    levels   of    factors
$\left\{x_2,x_4,x_6\right\}$.  The  significance of  these factors could  now be
detectable by  ANOVA tests since  the other factors are  now fixed. If  we still
cannot  identify any  significant factor,  it might  be advisable  to spend  the
remaining budget  using another exploration  strategy such as uniform  random or
latin hypercube sampling.

The process of  screening for factor significance using ANOVA  and fitting a new
model  using  acquired  knowledge  is  equivalent  to  steps  5,  6,  and  7  in
Figure\nbsp{}[[fig:doe_anova_strategy]].   In  the  next  section  we  evaluate  the
performance of our ED approach in two scenarios.

*** Exploration with Space-Filling Designs
:PROPERTIES:
:CUSTOM_ID: sec:filling
:END:
- Not very efficient for parameter estimate but good to evaluate the lack of fit
- Also good for variance minimization in GP

**** Quasi-Random Sequences for Low-Discrepancy Designs
- Low-discrepancy
- Sobol Sequences

*** Comparing Sampling Strategies
:PROPERTIES:
:CUSTOM_ID: sec:comparing
:END:
#+begin_export latex
\todo[inline]{TODO: Adapt this section from the CCGRID paper.}
#+end_export

#+NAME: fig:sampling_comparison
#+CAPTION: Exploration of the search space, using a fixed budget of 50 points. The red ``$+$'' represents the best point found by each strategy, and ``$\times$''s denote neighborhood exploration
#+ATTR_LATEX: :width .95\columnwidth :placement [b]
[[./img/ccgrid19/sampling_comparison.pdf]]

Figure\nbsp{}[[fig:sampling_comparison]] shows the contour of a search space defined
by a function of the form $z =  x^2 + y^2 + \varepsilon$, where $\varepsilon$ is
a local perturbation, and the exploration  of that search space by six different
strategies. In such a simple search space, even a uniform random sample can find
points  close  to  the  optimum,  despite  not  exploiting  geometry.   A  Latin
Hypercube\nbsp{}\cite{carnell2018lhs} sampling strategy  covers the search space
more evenly, but  still does not exploit the space's  geometry. Strategies based
on neighborhood exploration such as simulated annealing and gradient descent can
exploit local structures, but may get trapped in local minima. Their performance
is strongly dependent on search starting point. These strategies do not leverage
global  search   space  structure,   or  provide  exploitable   knowledge  after
optimization.

Measurement  of  the  kernels  optimized   on  the  performance  evaluations  in
Section\nbsp{}Performance Evaluation  can exceed 20 minutes,  including the time
of  code  transformation, compilation,  and  execution.   Measurements in  other
problem  domains  can  take  much  longer to  complete.   This  strengthens  the
motivation to consider search space  exploration strategies capable of operating
under  tight  budget constraints.   These  strategies  have been  developed  and
improved by  statisticians for  a long time,  and can be  grouped under  the ED
term.

The D-Optimal  sampling strategies shown on  the two rightmost bottom  panels of
Figure\nbsp{}[[fig:sampling_comparison]]  are  based  on the  ED  methodology,  and
leverage previous  knowledge about search  spaces for an  efficient exploration.
These  strategies   provide  transparent   analyses  that  enable   focusing  on
interesting  subspaces.  In  the  next  section  we  describe  our  approach  to
autotuning based on the ED methodology.

*** Summary
:PROPERTIES:
:CUSTOM_ID: sec:summary-ed
:END:
- Designs to obtain good-quality parameter estimates
  - Screening and D-opt for LM
  - SFD for GP

- Designs to test the model quality (lack of fit)
  - SFD fo LM

- If model  based, parameter significance and  estimation can be used  to reduce
  dimension and guide the optimization.

- With this ED approach, we have  a clear separation between the sampling phase
  and  the  interpretation  phase.  But  what if  no  parameter  really  appears
  significant anymore?

** Online Learning
#+begin_export latex
\todo[inline]{TODO: Fill in the sketched section below}
#+end_export

*** Independent Bandits
- simple (discrete choice, optimize $regret = \sum_t R_t$)
- UCB

*** Expected Improvement for Gaussian Process Regression
- (continuous choice, optimize $EI = \max_t R_t$)
- Also mention GP-UCB and contrast with  EGO. There are also variants for Linear
  Model (LinUCB)

*** Reinforcement Learning
- Unsupervised learning
- Actor-critic model

- Main motivation for  adding this section is to contextualize  results with GPR
  and Neural Nets. Could move to last Application chapter

*** Summary
- These  methods seamlessly  mix exploration  and exploitation  but the  overall
  objective  function  is  generally  the   regret,  which  makes  sense  for  a
  self-optimizing  system (e.g.   facebook)  but not  in  an autotuning  context
  (where EI is more meaningful)

** Towards Transparent and Parsimonious Methods for Automatic Performance Tuning
#+begin_export latex
\todo[inline]{TODO: Revisit semi-conclusions of each chapter, wrap up this part,
  hook to Applications part}
#+end_export

- use glassbox (ED based) approach to perform the optimization,
  always try to interpret the results
- 2 big methods based on different exploration/exploitation strategy:
  1. Evaluate parameter significance and reduce dimension (two phases,
     iterative)
  2. Expected Improvement (first a general exploration phase with a
     SFD, then seamlessly mix exploration and exploitation)
  Possibly combinations of both approaches could be used back and
  forth depending on the specific information we learn on the use
  case.
- In this thesis, we evaluate these ED-based approaches for several
  autotuning use cases and try to compare them with approaches that
  had beed previously proposed for these use cases.


#+begin_export latex
\todo[inline]{TODO: Adapt this paragraph}
#+end_export
The  effectiveness   of  stochastic  descent   methods  on  autotuning   can  be
limited\nbsp{}\cite{seymour2008comparison,balaprakash2011can,balaprakash2012experimental},
between other factors, by underlying hypotheses  about the search space, such as
the  reachability of  the  global optimum  and the  smoothness  of search  space
surfaces, which  are frequently not  respected. The derivation  of relationships
between  parameters  and  performance  from search  heuristic  optimizations  is
greatly hindered,  if not rendered impossible,  by the biased way  these methods
explore  parameters.   Some  parametric  learning methods,  such  as  Design  of
Experiments,  are  not widely  applied  to  autotuning.  These  methods  perform
structured  parameter  exploration,  and  can  be used  to  build  and  validate
performance     models,     generating    transparent     and     cost-effective
optimizations\nbsp{}\cite{mametjanov2015autotuning,bruel2019autotuning}.   Other
methods  from  the parametric  family  are  more  widely  used, such  as  Bandit
Algorithms\nbsp{}\cite{xu2017parallel}.  Nonparametric learning methods, such as
Decision   Trees\nbsp{}\cite{balaprakash2016automomml}   and  Gaussian   Process
Regression\nbsp{}\cite{parsa2019pabo}, are able to reduce model bias greatly, at
the  expense  of  increased  prediction  variance.   Figure\nbsp{}\ref{fig:tree}
categorizes some autotuning methods according to  some of the key hypotheses and
branching questions underlying each method.

#+NAME: fig:tree
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Some autotuning methods
[[file:img/tree.pdf]]

* Applications
** Research Methodology: Efforts for Reproducible Science
Sketch:

- Tools and such
- Explain difficulty of finding a needle in a haystack:
  - how to know whether we found the optimal value ?
  - how to know how far we are from the optimal value ?
  - how to know whether there is anything to find ?
  - how to know whether the geometry hypothesis we make are sound in
    an unknown space ?
  - ...

*** Reproducible Research
Conclusions  derived from  experimental data  can only  be considered  validated
after  it  is   possible  to  reproduce  them   under  independent  experimental
conditions.   This principle  has  guided experimental  research and  scientific
progress.   Experimental  researchers on  computer  science  are in  a  singular
position   to  promote   and  practice   reproducible  research,   because  some
computational  experiments  can  be  monitored, registered,  and  repeated  with
precision and  control that are  simply impossible in other  experimental fields
such as chemistry and biology.

**** Portuguese Text Previously Written                         :noexport:

- Add reference to repro crisis?
- Add mention to G. Fursin's efforts for ACM badging

Os esforços da  ACM\nbsp{}\cite{acm2021badging} são um bom  exemplo dos esforços
iniciais  que  podem ser  realizados  em  prol da  reprodutibilidade.   Diversas
conferências e  periódicos da  ACM adotam  um sistema  de insígnias  para marcar
trabalhos  cujos  esforços  para  reprodutibilidade  de  seus  experimentos  são
significativos.  A  nomenclatura utilizada  pela ACM  é derivada  do Vocabulário
Internacional   de    Metrologia   (VIM)\nbsp{}\cite{bipm2012international},   e
distingue entre resultados e conclusões que podem ser reproduzidos:

- Pela mesma equipe, nas mesmas condições experimentais (Repetibilidade)
- Por uma equipe diferente, nas mesmas condições experimentais (Replicabilidade)
- Por   uma   equipe   diferente,    em   condições   experimentais   diferentes
  (Reprodutibilidade)

O código e os dados que dão  suporte às conclusões de um estudo científico devem
ser submetidos  junto ao  documento que será  publicado.  Esses  /artefatos/ são
avaliados pelos  revisores e insígnias são  conferidas de acordo com  o nível de
reprodutibilidade   alcançado.    Outras    organizações   também   promovem   a
reprodutibilidade,    como   a    ReScience\nbsp{}\cite{rescience2021faq},   que
recentemente      promoveu      o      Desafio      de      10      Anos      da
Reprodutibilidade\nbsp{}\cite{rescience2020ten},   onde    pesquisadores   foram
incentivados a submeter artigos com a  reprodução de seus próprios resultados de
no mínimo 10 anos de idade.
*** Computational Documents with Emacs and Org mode
*** Versioning for Code, Text, and Data
** Compiler Parameters for CUDA Kernels
:PROPERTIES:
:CUSTOM_ID: chap:cuda
:END:
A Graphics Processing Unit (GPU) is a parallel computing coprocessor specialized
in accelerating vector  operations such as graphics  rendering.  General Purpose
computing  on GPUs  (GPGPU)  depends on  accessible  programming interfaces  for
languages such as C and Python that enable the use of GPUs in different parallel
computing domains.   The Compute Unified  Device Architecture (CUDA) is  a GPGPU
toolchain introduced by the NVIDIA corporation.

In this study we measured the performance improvements obtained with an ensemble
of stochastic methods for function minimization applied to the parameters of the
CUDA   compiler.    We   implemented    an   autotuner   using   the   OpenTuner
framework\nbsp{}\cite{ansel2014opentuner}  and   used  it  to  search   for  the
compilation parameters  that optimize  the performance  of 17  heterogeneous GPU
kernels,     12    of     which     are    from     the    Rodinia     Benchmark
Suite\nbsp{}\cite{che2009rodinia}.   We  used 3  different  NVIDIA  GPUs in  the
experiments, the  Tesla K40,  the GTX 980,  and the GTX  750.  We  published the
results of this  study, which we summarize in this  chapter, at the /Concurrency
and           Computation:           Practice          and           Experience/
journal\nbsp{}\cite{bruel2017autotuning}.

The optimizations we found often  beat compiler high-level optimization options,
such as the  /-O2/ flag.  The autotuner found compilation  options that produced
speedups,  in relation  to /-O2/,  of around  2 times  for the  \textit{Gaussian
Elimination} problem from the Rodinia Benchmark  Suite, 2 times for /Heart Wall/
problem, also  from Rodinia, and  4 times for  one of the  matrix multiplication
algorithms we  implemented.  We  observed that  the compilation  parameters that
optimize an algorithm  for a given GPU architecture will  not always achieve the
same performance in different hardware, which was a reasonable expectation.

#+begin_export latex
\begin{figure}[t]
    \centering
    \includegraphics[width=.62\textwidth]{./img/quali_brazil/overview_gpus-eps-converted-to.pdf}
    \caption{Autotuner representation and time scale of the experiments}
    \label{fig:overview-gpus}
\end{figure}
#+end_export

Figure\nbsp{}\ref{fig:overview-gpus}  shows a  representation  of the  autotuner
implemented in  this study, and illustrates  the time scale of  our experiments.
The autotuner,  in light  blue, receives as  input a GPU  kernel, a  target GPU,
input data  for the  kernel, and a  search space composed  of NVCC  flags. After
running an ensemble  of stochastic search methods, the autotuner  outputs a flag
selection.  The compilation  of the kernels we targeted in  this study takes few
seconds, so  it was  possible to  test thousands of  flag combinations  per hour
using a sequential autotuner.

The rest of this chapter  is organized as follows. Section\nbsp{}[[General-Purpose
Computing  on NVIDIA  GPUs]]  discusses NVIDIA  GPU  architecture and  programming
toolchain, and  presents related  work on GPU  performance tuning  and modeling.
Section\nbsp{}[[Autotuner  and Search  Space for  the NVCC  Compiler]] presents  the
search   space   of   CUDA   parameters   and   discusses   the   autotuner   we
implemented. Section\nbsp{}[[Target GPUs and Kernels]] presents GPUs and the kernels
whose   compilation  we   attempted   to  optimize.    Section\nbsp{}[[Performance
Improvements  and  Parameter  Clustering  Attempt]]  presents  and  discusses  the
performance improvements  achieved by  the autotuner,  and attempts  to identify
significant   flags   using   clustering.    Section\nbsp{}[[Assessing   Parameter
Significance  with Screening]]  was  written  at a  later  moment, after  studying
Experimental  Design  methods,  and  revisits  this  problem  with  a  screening
experiment.   Finally,    Section\nbsp{}\ref{sec:gpu-summary}   summarizes   the
discussion and presents perspectives for future work.

*** General-Purpose Computing on NVIDIA GPUs
**** NVIDIA GPU Micro-Architecture
NVIDIA  GPU  architectures have  multiple  asynchronous  and parallel  Streaming
Multiprocessors (SMs)  which contain  Scalar Processors (SPs),  Special Function
Units (SFUs) and  load/store units. Each group of 32  parallel threads scheduled
by and SM,  or \textit{warp}, is able to read  from memory concurrently.  NVIDIA
architectures  vary in  a large  number of  features, such  as number  of cores,
registers, SFUs,  load/store units,  on-chip and  cache memory  sizes, processor
clock  frequency, memory  bandwidth, unified  memory spaces  and dynamic  kernel
launches.  Those differences are summarized  in the Compute Capability (C.C.) of
an NVIDIA GPU.

The hierarchical  memory of an NVIDIA  GPU contains global and  shared portions.
Global memory is  big, off-chip, has a  high latency and can be  accessed by all
threads of the  kernel.  Shared memory is small, on-chip,  has a low-latency and
can be  accessed only by threads  in a same SM.   Each SM has its  own shared L1
cache, and new architectures have  coherent global L2 caches.  Optimizing thread
accesses to different memory levels is essential to achieve good performance.

**** Compute Unified Device Architecture (CUDA)
The  CUDA programming  model and  platform enables  the use  of NVIDIA  GPUs for
scientific and general purpose computation.  A  single /main/ thread runs in the
CPU, launching and managing computations on  the GPU.  Data for the computations
has  to be  transferred from  the  main memory  to the  GPU's memory.   Multiple
computations launched by  the main thread, or /kernels/,  can run asynchronously
and  concurrently.  If  the  threads from  a same  warp  must execute  different
instructions  the CUDA  compiler  must  generate code  to  branch the  execution
correctly, making the program lose performance due to this /warp divergence/.

The CUDA language  extends C and provides a multi-step  compiler, called /NVCC/,
that   translates  CUDA   code  to   Parallel  Thread   Execution  code   (PTX).
\textit{NVCC} uses  the host's  C++ compiler in  several compilation  steps, and
also to generate code to be executed in the host.  The final binary generated by
\textit{NVCC} contains code for  the GPU and the host.  When  PTX code is loaded
by a  kernel at  runtime, it  is compiled to  binary code  by the  host's device
driver.  This binary code can be  executed in the device's processing cores, and
is  architecture-specific.   The  target  architecture can  be  specified  using
\textit{NVCC} parameters.

**** GPU Performance Models and Autotuning
The accuracy of a GPU performance model is subject to low level elements such as
instruction pipeline  usage and  small cache  hierarchies.  A  GPU's performance
approaches  its peak  when the  instruction pipeline  is saturated,  but becomes
unpredictable             when            the             pipeline            is
under-utilized\nbsp{}\cite{zhang2011quantitative,amaris2015simple}.  Considering
the               effects              of               small              cache
hierarchies\nbsp{}\cite{dao2015performance,picchi2015impact}  and  memory-access
divergence\nbsp{}\cite{sampaio2013divergence,baghsorkhi2010adaptive}   is   also
critical to a GPU performance model.

Guo and  Wang\nbsp{}\cite{guo2010auto} introduce a framework  for autotuning the
number of threads  and the sizes of  blocks and warps used by  the CUDA compiler
for  sparse  matrix   and  vector  multiplication  GPU   applications.   Li  /et
al./\nbsp{}\cite{li2009note} discuss the performance of autotuning techniques in
highly-tuned  GPU  General Matrix  to  Matrix  Multiplication (GEMMs)  routines,
highlighting  the   difficulty  in  developing   optimized  code  for   new  GPU
architectures.   Grauer-Gray  /et al./\nbsp{}\cite{grauer2012auto}  autotune  an
optimization  space  of  GPU  kernels  focusing  on  tiling,  loop  permutation,
unrolling,          and         parallelization.           Chaparala         /et
al./\nbsp{}\cite{chaparala2015autotuning}  autotune   GPU-accelerated  Quadratic
Assignment           Problem          solvers.            Sedaghati          /et
al./\nbsp{}\cite{sedaghati2015automatic}   build  a   decision  model   for  the
selection of sparse matrix representations in GPUs.

*** Autotuner and Search Space for the NVCC Compiler
This section discusses our autotuner implementation and the search space of NVCC
compiler parameters.

**** An Autotuner for CUDA Parameters using OpenTuner
The      autotuner      was       implemented      using      the      OpenTuner
framework\nbsp{}\cite{ansel2014opentuner},   and  used   a  multi-armed   bandit
algorithm that aims  to maximize the successes across the  autotuning process. A
success is  defined as finding  a program  configuration that improves  upon the
best  performance found  so  far.   The stochastic  method  which  finds such  a
configuration  gets its  score  increased  for that  tuning  session.  A  bandit
algorithm, called  /MABAUC/ in the  paper, for  Multi-Armed Bandit Area  Under the
Curve, considers  a sliding window  covering a  given number of  measurements to
compute the  scores of methods  in an ensemble, and  to decide which  method, or
``arm'',  to  play next.   The  ensemble  of methods  we  used  was composed  by
implementations of  the Nelder-Mead  algorithm and  three variations  of genetic
algorithms.  Figure\nbsp{}\ref{fig:gpu-stack} shows a  simplified version of the
steps necessary  to generate the object  code that will be  measured later.  The
code   for   our   autotuner   and   all  the   experiments   and   results   is
available\nbsp{}\cite{bruel2017autotuningURL}  under  the   GNU  General  Public
License.

#+begin_export latex
\begin{figure}[t]
    \centering
    \includegraphics[width=.4\textwidth]{./img/quali_brazil/gpu-stack-eps-converted-to.pdf}
    \caption{Simplified view of NVCC compilation}
    \label{fig:gpu-stack}
\end{figure}
#+end_export

**** Search Space for CUDA Parameters
Table\nbsp{}\ref{tab:flags}  presents   a  subset  of  the   CUDA  configuration
parameters\nbsp{}\cite{cuda2020driverURL}  used in  this study.   Parameters can
target different compilation  steps, namely the /PTX/  optimizing assembler, the
/NVLINK/  linker, and  the  /NVCC/  compiler.  We  compared  the performance  of
programs generated by tuned parameters  with the standard compiler optimizations
/opt-level/ equal  to /0/, /1/,  /2/, /3/.  Different optimization  levels could
also be selected during tuning.  We did not use compiler options that target the
host linker  or the library manager  since they do not  affect performance.  The
size of the search space defined by all possible combinations of flags is in the
order of $10^{6}$, making hand-optimization or exhaustive searches unwieldy.

#+begin_export latex
\begin{table}[t]
  \centering
  \scriptsize
  \caption{Description of flags in the search space}
  \begin{tabular}{lp{0.6\textwidth}}
    \toprule
    \textbf{Flag}&\textbf{Description} \\\midrule
    \addlinespace{}
    \texttt{no-align-double} & Specifies that \texttt{malign-double} should not be passed as a compiler argument on 32-bit platforms. \textbf{Step}: NVCC \\
    \addlinespace{}
    \texttt{use\_fast\_math} & Uses the fast math library, implies \texttt{ftz=true}, \texttt{prec-div=false}, \texttt{prec-sqrt=false} and \texttt{fmad=true}. \textbf{Step}: NVCC \\
    \addlinespace{}
    \texttt{gpu-architecture} & Specifies the NVIDIA virtual GPU architecture for which the CUDA input files must be compiled. \textbf{Step}: NVCC \textbf{Values}: \texttt{sm\_20}, \texttt{sm\_21}, \texttt{sm\_30}, \texttt{sm\_32}, \texttt{sm\_35}, \texttt{sm\_50}, \texttt{sm\_52} \\
    \addlinespace{}
    \texttt{relocatable-device-code} & Enables the generation of relocatable device code. If disabled, executable device code is generated. Relocatable device code must be linked before it can be executed. \textbf{Step}: NVCC \\
    \addlinespace{}
    \texttt{ftz} & Controls single-precision denormals support. \texttt{ftz=true} flushes denormal values to zero and \texttt{ftz=false} preserves denormal values. \textbf{Step}: NVCC \\
    \addlinespace{}
    \texttt{prec-div} & Controls single-precision floating-point division and reciprocals. \texttt{prec-div=true} enables the IEEE round-to-nearest mode and \texttt{prec-div=false} enables the fast approximation mode. \textbf{Step}: NVCC \\
    \addlinespace{}
    \texttt{prec-sqrt} & Controls single-precision floating-point squre root. \texttt{prec-sqrt=true} enables the IEEE round-to-nearest mode and \texttt{prec-sqrt=false} enables the fast approximation mode. \textbf{Step}: NVCC \\
    \addlinespace{}
    \texttt{def-load-cache} & Default cache modifier on global/generic load. \textbf{Step}: PTX \textbf{Values}: \texttt{ca}, \texttt{cg}, \texttt{cv}, \texttt{cs} \\
    \addlinespace{}
    \texttt{opt-level} & Specifies high-level optimizations. \textbf{Step}: PTX \textbf{Values}: \texttt{0 - 3} \\
    \addlinespace{}
    \texttt{fmad} & Enables the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA). \textbf{Step}: PTX \\
    \addlinespace{}
    \texttt{allow-expensive-optimizations} & Enables the compiler to perform expensive optimizations using maximum available resources (memory and compile-time). If unspecified, default behavior is to enable this feature for optimization level $\geqslant$O2. \textbf{Step}: PTX \\
    \addlinespace{}
    \texttt{maxrregcount} & Specifies the maximum number of registers that GPU functions can use. \textbf{Step}: PTX \textbf{Values}: \texttt{16 - 64} \\
    \addlinespace{}
    \texttt{preserve-relocs} & Makes the \texttt{PTX} assembler generate relocatable references for variables and preserve relocations generated for them in the linked executable. \textbf{Step}: NVLINK \\
    \bottomrule
  \end{tabular}
  \label{tab:flags}
\end{table}
#+end_export

*** Target GPUs and Kernels
This section  presents the GPU  testbed, the algorithm benchmark,  the autotuner
implementation and its search space.

**** Target GPU Architectures
To be  able to  show that  different GPUs require  different options  to improve
performance, and that it is possible  to achieve speedups in different hardware,
we  wanted   to  tune   our  benchmark   for  different   NVIDIA  architectures.
Table\nbsp{}\ref{tab:GPUs} summarizes the hardware  characteristics of the three
GPUs.

#+begin_export latex
\begin{table}[t]
  \centering
  \footnotesize
  \caption{Hardware specifications of the target GPU architectures}
  \begin{tabular}{llllllll}
    \toprule
    \textbf{Model}&\textbf{C.C.}&\textbf{Global Memory}&\textbf{Bus}&\textbf{Bandwidth}&\textbf{L2}&\textbf{Cores/SM}&\textbf{Clock} \\ \midrule
    Tesla-K40&3.5&12 GB&384-bit&276.5 GB/s&1.5 MB&2880/15&745 Mhz \\
    GTX-750&5.0&1 GB&128-bit&86.4 GB/s&2 MB&512/4&1110 Mhz \\
    GTX-980&5.2&4 GB&256-bit&224.3 GB/s&2 MB&2048/16&1216 Mhz \\ \bottomrule
  \end{tabular}
  \label{tab:GPUs}
\end{table}
#+end_export

**** Benchmark of CUDA Algorithms
We  composed a  benchmark  with  17 heterogeneous  GPU  kernels.  The  benchmark
contains  4 optimization  strategies for  /matrix multiplication/  counted as  a
single kernel, 1 vector addition problem,  1 solution for the /maximum sub-array
problem/\nbsp{}\cite{ferreira2014parallel},  2   /sorting/  algorithms   and  12
kernels from the Rodinia Benchmark Suite\nbsp{}\cite{che2009rodinia}.

#+begin_export latex
\begin{table}[b]
  \centering
  \footnotesize
  \caption{Rodinia~\cite{che2009rodinia} kernels used in the experiments}
  \begin{tabular}{lll}
    \toprule
    \textbf{Kernel} & \textbf{Berkeley Dwarf\cite{asanovic2009view}} & \textbf{Domain} \\\midrule
    B+Tree (BPT) & Graph Traversal& Search \\
    Back Propagation (BCK) & Unstructured Grid & Pattern Recognition \\
    Breadth-First Search (BFS) & Graph Traversal & Graph Algorithms \\
    Gaussian Elimination (GAU) & Dense Linear Algebra & Linear Algebra \\
    Heart Wall (HWL) & Structured Grid & Medical Imaging \\
    Hot Spot (HOT) & Structured Grid & Physics Simulation \\
    K-Means (KMN) & Dense Linear Algebra & Data Mining \\
    LavaMD (LMD) & N-Body & Molecular Dynamics \\
    LU Decomposition (LUD) & Dense Linear Algebra & Linear Algebra \\
    Myocyte (MYO) & Structured Grid & Biological Simulation \\
    Needleman-Wunsch (NDL) & Dynamic Programming & Bioinformatics \\
    Path Finder (PTF) & Dynamic Programming & Grid Traversal \\\bottomrule
  \end{tabular}
  \label{tab:Rodinia}
\end{table}
#+end_export

Table\nbsp{}\ref{tab:Rodinia} shows  the Rodinia  kernels contained  in our
benchmark and  the corresponding three-letter  code.  The other  kernels in
the benchmark were the following CUDA implementations:

#+begin_export latex
\begin{itemize}
    \item Matrix multiplications using:
        \begin{itemize}
             \item Global memory with non-coalesced accesses (MMU)
             \item Global memory with coalesced accesses (MMG)
             \item Shared memory with non-coalesced accesses to global memory
                 (MSU)
             \item Shared memory with coalesced accesses to global memory (MMS)
        \end{itemize}
    \item Simple vector addition algorithm (VAD)
    \item Solution for the Maximum
        Sub-Array Problem (MSA)~\cite{alves2004bsp,ferreira2014parallel}
    \item Quicksort (QKS) and Bitonicsort (BTN)
\end{itemize}
#+end_export

*** Performance Improvements and Parameter Clustering Attempt
This section presents the speedups achieved for all algorithms in the benchmark,
highlights  the most  significant speedups,  and discusses  the performance  and
accuracy of the autotuner.

**** Performance Improvements
Figure\nbsp{}[[fig:rodinia-speedups]] compares  the means of 30  measurements of the
tuned results for each kernel to  the high-level optimization /opt-level/ set to
/2/.  We see that  that the autotuned solution for the  Heart Wall problem (HWL)
in  the  /Tesla K40/  achieved  over  2 times  speedup  in  comparison with  the
high-level  CUDA optimizations.   The  tuned kernel  for  the /GTX-980/  reached
almost 2.5 times speedup for Gaussian  Elimination (GAU).  We also found smaller
speedups for most kernels, such as a 10% speedup for Path Finder (PTF) in /Tesla
K40/, and around 10% speedup for Myocyte (MYO) on /GTX-750/.

Figure\nbsp{}[[fig:adhoc-speedups]]  summarizes  the  results  for  the  other  CUDA
kernels  we implemented.   The autotuner  did  not improve  upon the  high-level
optimizations for BTN,  VAD, and QKS in any  of the GPUs in the  testbed, but it
found  solutions that  achieved speedups  for  at least  one GPU  for the  other
kernels.

We  were  not able  to  determine  the  hardware characteristics  that  impacted
performance.  We believe that the Maxwell GPUs, the /GTX-980/ and /GTX-750/, had
differing results  from the /Tesla  K40/ because  they are consumer  grade GPUs,
producing  less   precise  results,   and  configured  with   different  default
optimizations. The similarities between the compute capabilities of the GTX GPUs
could also explain  the observed differences from the K40.   The overall greater
computing power  of the /GTX-980/ could  explain its differing results  from the
/GTX-750/,  since  the  GTX  980  has a  greater  number  of  Cores,  SMs/Cores,
Bandwidth, Bus, Clock and Global Memory.

#+ATTR_LATEX: :width 0.77\textwidth :placement [ht]
#+NAME: fig:rodinia-speedups
#+CAPTION: Mean speedup over 30 repetitions of the tuned solutions
#+CAPTION: for Rodinia kernels versus the \textit{opt-level} $=2$ baseline
[[file:img/rodinia_speedups.pdf]]

#+ATTR_LATEX: :width 0.77\textwidth :placement [ht]
#+NAME: fig:adhoc-speedups
#+CAPTION: Mean speedup over 30 repetitions of the tuned solutions
#+CAPTION: for kernels we implemented versus the \textit{opt-level} $=2$ baseline
[[file:img/adhoc_speedups.pdf]]

***** Old Figures                                              :noexport:

#+begin_export latex
\begin{figure}[t]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/heartwall-0-Tesla-K40-Box-eps-converted-to.pdf}
        \caption{Boxplots for the Tesla K40, comparing autotuned results and high-level compiler optimizations for the Heart Wall problem (HWL)}
        \label{fig:K40hwl}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/gaussian-0-GTX-980-Box-eps-converted-to.pdf}
        \caption{Boxplots for the GTX 980 GPU, comparing autotuned results and high-level compiler optimizations for the Gaussian Elimination problem (GAU)}
        \label{fig:980gau}
    \end{minipage}%
\end{figure}
#+end_export

#+begin_export latex
\begin{figure}[h]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/Pathfinder-0-Tesla-K40-Box-eps-converted-to.pdf}
        \caption{Boxplots for the Tesla K40, comparing autotuned results and high-level compiler optimizations for the Path Finder problem (PTF)}
        \label{fig:K40ptf}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/myocyte-0-Tesla-K40-Box-eps-converted-to.pdf}
        \caption{Boxplots for the Tesla K40, comparing autotuned results and high-level compiler optimizations for the Myocyte problem (MYO)}
        \label{fig:K40myo}
    \end{minipage}%
\end{figure}
#+end_export

#+begin_export latex
\begin{figure}[t]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/MatrixSummary-eps-converted-to.pdf}
        \caption{Summary of the speedups achieved versus \emph{-O2} in the matrix multiplication optimizations}
        \label{fig:matrixsummary}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/Summary-eps-converted-to.pdf}
        \caption{Summary of the speedups achieved versus \emph{-O2} in the other independent kernels}
        \label{fig:summary}
    \end{minipage}%
\end{figure}
#+end_export

#+begin_export latex
\begin{figure}[h]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/RodiniaSummary-eps-converted-to.pdf}
        \caption{Summary of the biggest speedups achieved versus \emph{-O2} in the Rodinia kernels}
        \label{fig:rodiniasummary}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/RodiniaSummary_small-eps-converted-to.pdf}
        \caption{Summary of the smaller speedups achieved versus \emph{-O2} in the Rodinia kernels}
        \label{fig:rodiniasummarysmall}
    \end{minipage}%
\end{figure}
#+end_export

**** CUDA Compiler Autotuner Performance
This   section  presents   an   assessment  of   the  autotuner's   performance.
Figures\nbsp{}[[fig:rodinia-tuning-progress]] and  [[fig:adhoc-tuning-progress]] present
the speedup of the best solution found across tuning time.  Points in each graph
represent the  performance, in  the \textit{y}-axis,  of the  best configuration
found  at the  corresponding tuning  time,  shown in  the \textit{x}-axis.   The
leftmost  point in  each graph  represents  the performance  of a  configuration
chosen  at  random by  the  autotuner.   Each  subsequent point  represents  the
performance of the best configuration  found across optimization.  Note that the
autotuner is able to quickly improve  upon the initial random configuration, but
the rate of improvement also decays quickly. The duration of all tuning runs was
two hours,  or 7200 seconds.  The  rightmost point in each  graph represents the
performance of the last improving configuration found by the autotuner.

#+latex: \clearpage

#+ATTR_LATEX: :width 0.9\textwidth :placement [t]
#+NAME: fig:rodinia-tuning-progress
#+CAPTION: Mean speedup over 30 repetitions of the tuned solutions
#+CAPTION: for Rodinia kernels versus the \textit{opt-level} $=2$ baseline,
#+CAPTION: across two hours of tuning. Notice the difference in the \textit{y}-axis
#+CAPTION: scales for each panel
[[file:img/rodinia_tuning_progress.pdf]]

#+ATTR_LATEX: :width 0.9\textwidth :placement [b]
#+NAME: fig:adhoc-tuning-progress
#+CAPTION: Mean speedup over 30 repetitions of the tuned solutions
#+CAPTION: for the kernels we implemented versus the \textit{opt-level} $=2$ baseline,
#+CAPTION: across two hours of tuning. Notice the difference in the \textit{y}-axis
#+CAPTION: scales for each panel
[[file:img/adhoc_tuning_progress.pdf]]

#+latex: \clearpage

***** Old Figures                                              :noexport:
#+begin_export latex
\begin{figure}[t]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/heartwall-0-Tesla-K40-Best-eps-converted-to.pdf}
        \caption{Best solutions found by the autotuner over time for the Heart Wall problem (HWL) in the Tesla K40}
        \label{fig:K40hwBest}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/gaussian-0-GTX-980-Best-eps-converted-to.pdf}
        \caption{Best solutions found by the autotuner over time for the Gaussian Elimination problem (GAU) in the GTX 980}
        \label{fig:980gauBest}
    \end{minipage}%
\end{figure}
#+end_export

#+begin_export latex
\begin{figure}[h]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/Pathfinder-0-Tesla-K40-Best-eps-converted-to.pdf}
        \caption{Best solutions found by the autotuner over time for the Path Finder problem (PTF) in the Tesla K40}
        \label{fig:K40ptfBest}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./img/quali_brazil/myocyte-0-Tesla-K40-Best-eps-converted-to.pdf}
        \caption{Best solutions found by the autotuner over time for the Myocyte problem (MYO) in the Tesla K40}
        \label{fig:K40myoBest}
    \end{minipage}
\end{figure}
#+end_export

**** Clustering Parameters found by Stochastic Experiments
#+begin_export latex
\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{lrr}
        \toprule
        \textbf{Flag} & \textbf{Cluster 0 (17\%)} & \textbf{Cluster 1 (83\%)} \\\midrule
        \texttt{no-align-double}               & on     & on     \\
        \texttt{use\_fast\_math}               & on     & on     \\
        \texttt{preserve-relocs}               & off    & off    \\
        \texttt{relocatable-device-code}       & true   & false  \\
        \texttt{ftz}                           & true   & true   \\
        \texttt{prec-div}                      & true   & false  \\
        \texttt{prec-sqrt}                     & true   & true   \\
        \texttt{fmad}                          & false  & false  \\
        \texttt{allow-expensive-optimizations} & false  & true   \\
        \texttt{gpu-architecture}              & sm\_20 & sm\_50 \\
        \texttt{def-load-cache}                & cv     & ca     \\
        \texttt{opt-level}                     & 1      & 3      \\
        \texttt{maxrregcount}                  & 42     & 44.6   \\\bottomrule
        \end{tabular}
    \caption{Parameter clusters for all Rodinia problems in the GTX 750}
    \label{tab:750RodiniaClusters}
\end{table}
#+end_export

We attempted to associate compilation parameters  to kernels and GPUs using WEKA
clustering algorithms\nbsp{}\cite{holmes1994weka}.   Although we could  not find
significant relations for most kernels we detected that the /ftz=true/ in
MMS and  the Compute Capabilities  3.0, 5.0 and 5.2  in GAU caused  the speedups
observed      in       the      GTX       980      for       these      kernels.
Table\nbsp{}\ref{tab:750RodiniaClusters}  shows   clusters  obtained   with  the
K-means WEKA algorithm for autotuned parameter sets for the Rodinia Benchmark in
the  GTX 750.   Unlike most  clusters  found for  all GPUs  and problems,  these
clusters  did not  contain  an equal  number of  instances.   The difficulty  of
finding associations between compiler  optimizations, kernels and GPUs justifies
the autotuning of compiler parameters in the general case.

*** Assessing Parameter Significance with Screening
This study was done in 2015, the first  year of this thesis.  Since at that time
we did  not have experience with  the methods for statistical  modeling which we
later studied, and  since the measurement time for each  kernel was sufficiently
small, we did not deal with the  problem of building a performance model for the
CUDA compiler  in this  study, and  originally attempted  to find  good compiler
configurations using stochastic methods  for function minimization.  Since then,
we performed a new experiment using screening and targeting the same problem, in
an attempt  to identify the flags  responsible for any improvements  we observe.
In this  section, before closing the  chapter, we will present  and discuss this
new screening experiment.

The data in this  section has not yet been published at the  time of the writing
of this thesis. We designed and ran screening experiments for the CUDA compiler,
adding the parameters described in Table\nbsp{}\ref{tab:new-flags} to the search
space defined  in Table\nbsp{}\ref{tab:flags}.  We  no longer had access  to the
GPUs from the testbed described in the previous section, and we targeted the two
GPUs described in Table\nbsp{}\ref{tab:screening-GPUs}.

#+begin_export latex
\begin{table}[t]
  \centering
  \scriptsize
  \caption{Flags added to the search space from Table~\ref{tab:flags}}
  \begin{tabular}{lp{0.6\textwidth}}
    \toprule
    \textbf{Flag}&\textbf{Description} \\\midrule
    \addlinespace{}
    \texttt{force-store-cache} & Force specified cache modifier on global/generic store. \textbf{Step}: PTX \textbf{Values}: \texttt{cs}, \texttt{cg} \\
    \addlinespace{}
    \texttt{force-load-cache} & Replaces \texttt{def-load-cache}, force specified cache modifier on global/generic load. \textbf{Step}: PTX \textbf{Values}: \texttt{cs}, \texttt{cg} \\
    \addlinespace{}
    \texttt{optimize} & Specificy optimization level for host code. \textbf{Step}: NVCC \textbf{Values}: \texttt{2}, \texttt{3} \\
    \bottomrule
  \end{tabular}
  \label{tab:new-flags}
\end{table}
#+end_export

For this experiment we picked  /Heart Wall/ (HWL), /Gaussian Elimination/ (GAU),
and  /Needleman-Wunsch/  (NDL),  the  three  Rodinia\nbsp{}\cite{che2009rodinia}
kernels  for   which  we  found  the   largest  speedups  in  the   last  study.
Table\nbsp{}\ref{tab:Rodinia} gives more details  on each kernel. We constructed
a Plackett-Burman design  for the 15 parameters in the  search space, defining a
minimum and  maximum value  for each parameter.   For categorical  parameters we
fixed two arbitrary levels, for the  optimization level parameters we picked the
equivalents  of /-O2/  and /-O3/  for both  host and  device code,  and for  the
parameter that controls the number of available registers, we picked the largest
number and the midpoint.

#+begin_export latex
\begin{table}[h]
  \centering
  \footnotesize
  \caption{Hardware specifications of the GPU architectures
    targeted in the screening experiments}
  \begin{tabular}{llllllll}
    \toprule
    \textbf{Model} & \textbf{C.C.} & \textbf{Global Memory} & \textbf{Bus} & \textbf{Bandwidth} & \textbf{L2} & \textbf{Cores} & \textbf{Clock} \\ \midrule
    Quadro M1200  & 5.0 & 4 GB & 128-bit & 80.19 GB/s & 2 MB & 640 & 1148 Mhz \\
    Titan X & 5.2 & 12 GB & 384-bit & 336.5 GB/s & 3 MB & 3072 & 1075 Mhz \\ \bottomrule
  \end{tabular}
  \label{tab:screening-GPUs}
\end{table}
#+end_export

The Plackett-Burman design  for 15 2-level factors contains  20 experiments, and
we performed 20  repetitions of each parameter assignment for  each of the three
kernels.  The  top and  bottom panels  of Figure\nbsp{}[[fig:effects-gpu-screening]]
show  for  each target  GPU  the  means and  95%  confidence  intervals for  the
coefficients  of  a  linear  model  fit to  the  screening  measurements,  which
represent the main effect estimates for each factor.

#+ATTR_LATEX: :width .9\textwidth :placement [t]
#+NAME: fig:effects-gpu-screening
#+CAPTION: Main effects estimates and 95% confidence intervals for the
#+CAPTION: \textit{high level} of the 15 factors in the Plackett-Burman design,
#+CAPTION: for target kernels measured on the Titan X and Quadro M1200 GPUs
[[file:img/gpu-screening/effects_gpu_screening.pdf]]

#+latex: \clearpage

Note that because the Plackett-Burman design matrix is orthogonal the confidence
intervals for the effect mean estimates all have the same size. In opposition to
the  exploration of  stochastic methods,  this well-balanced  design allows  the
detection of  factor main  effects, which  are mostly small,  except in  the HWL
kernel on the  Quadro GPU, where two factors present  large effects.  The linear
models  we built  for each  kernel using  screening results  do not  account for
interactions between  factors, and thus  their predictions favor turning  on the
factors whose mean estimates are negative  and have confidence intervals that do
not cross zero.

We generated  a large  set of  points in  the CUDA  parameters search  space and
picked the  points for  which the  models predict  the smallest  execution time.
Figure\nbsp{}[[fig:timing-gpu-screening]]  shows  the  performances  of  the  points
picked by the  models and the baseline for comparison,  for each Rodinia kernel.
The baseline for comparison consisted  of using only the PTX-stage /opt-level=3/
high-level  compiler  optimization.   There  was no  significant  difference  in
performance between second and third levels of /opt-level/ in these experiments.

The left panel shows results for the  Titan X GPU, where we found small speedups
for the GAU and NDL kernels, but small slowdowns for HWL.  The right panel shows
results for the  Quadro M1200 GPU, where we found  statistically significant but
small speedups  for GAU and NDL,  and 8 times  speedup for the HWL  kernel.  The
speedups      obtained     using      screening      are     highlighted      in
Figure\nbsp{}[[fig:speedup-gpu-screening]].

#+ATTR_LATEX: :width \textwidth :placement [t]
#+NAME: fig:timing-gpu-screening
#+CAPTION: Execution  time  of  baseline  and  model-predicted  NVCC  flag
#+CAPTION: configurations for three  Rodinia kernels on the Titan  X and
#+CAPTION: Quadro M1200 GPUs. Circles mark each  of the 20  measurements
#+CAPTION: of each flag  configuration, filled dots  and whiskers
#+CAPTION: mark the  mean  and 95%  confidence intervals
[[file:img/gpu-screening/timing_gpu_screening.pdf]]

#+ATTR_LATEX: :width \textwidth :placement [t]
#+NAME: fig:speedup-gpu-screening
#+CAPTION: Mean speedups found using the predictions of the screening model,
#+CAPTION: for three Rodinia
#+CAPTION: kernels on the Titan X and Quadro M1200 GPUs.
#+CAPTION: The horizontal dashed lines mark the PTX-stage \texttt{opt-level=3}
#+CAPTION: comparison baseline
[[file:img/gpu-screening/speedup_gpu_screening.pdf]]

*** Summary
:PROPERTIES:
:CUSTOM_ID: sec:gpu-summary
:END:
This study provides  evidence that it is possible to  improve the performance of
GPU  kernels  with  by  selecting  CUDA  compiler  parameters  and  flags.   The
inconclusive  clustering  attempts for  the  data  obtained from  heuristic  and
stochastic  explorations  of  the  search spaces  emphasize  the  importance  of
well-designed  experiment.   Later  experiments with  screening  reproduced  the
expressive  speedups   found  for   some  kernels   in  different   GPUs,  while
simultaneously  allowing  the  identification   of  the  flags  responsible  for
performance improvements. Future work in  this direction will explore the impact
of adding host  compiler optimizations and kernel-specific  parameters.  In this
larger search  space it would  be more interesting  to employ the  more flexible
Optimal  Design methods  we discuss  in Chapters\nbsp{}\ref{chap:laplacian}  and
\ref{chap:spapt}.

** High-Level Synthesis Kernels for FPGAs
:PROPERTIES:
:CUSTOM_ID: chap:fpga
:END:
A Field-Programmable Gate Array (FPGA)  is a reprogrammable circuit that enables
writing  specialized  hardware  specifications  for a  variety  of  applications
without  changing  the  underlying  chip.   In  recent  years  High  Performance
Computing tasks that require low power and latency have increasingly switched to
FPGAs.  Hardware  specifications are written  using low-level languages  such as
Verilog and VHDL,  creating a challenge for software engineers  to leverage FPGA
capabilities.  Developing strategies to decrease  the effort required to program
FPGAs is becoming more relevant with the overspread adaptation of FPGAs for data
centers\nbsp{}\cite{caulfield2016cloud,yu2019data,tarafdar2017enabling,shu2019direct,zhang2017feniks,tarafdar2018galapagos},
with direct vendor backing\nbsp{}\cite{intel2020fpgaURL,xilinx2020fpgaURL}.

Essential support for software engineers can be provided by High-Level Synthesis
(HLS),  where hardware  descriptions are  generated from  high-level code.   HLS
compilers intend  to lower  the complexity  of hardware  design and  have become
increasingly valuable  as part of  the FPGA  design workflow, with  support from
vendor  HLS  tools\nbsp{}\cite{singh2011implementing,feist2012vivado} for  C/C++
and OpenCL.  The benefits of higher-level  abstractions often come with the cost
of  decreased performance,  making  FPGAs less  viable  as accelerators.   Thus,
optimizing  HLS  still  requires  domain  expertise  and  exhaustive  or  manual
exploration of design spaces and configurations.

High-Level Synthesis  is a challenging  problem, and  a common strategy  for its
solution              involves               the              divide-and-conquer
approach\nbsp{}\cite{coussy2009introduction}.   The most  important sub-problems
to  solve are  /scheduling/, where  operations  are assigned  to specific  clock
cycles,  and  /binding/, where  operations  are  assigned to  specific  hardware
functional    units,     which    can    be    shared     between    operations.
LegUp\nbsp{}\cite{canis2013legup} was  an initially  open-source HLS  tool, that
has  since  gone  paid   and  closed-source\nbsp{}\cite{legup2020URL},  and  was
implemented     as    a     compiler    pass     for    the     LLVM    Compiler
Infrastructure\nbsp{}\cite{chris2004llvm}.   LegUp   receives  code   in  LLVM's
intermediate  representation  as  input  and   produces  as  output  a  hardware
description in  Verilog, exposing configuration  parameters of its  HLS process,
which are set with a configuration file.

This  chapter  presents  our  implementation  of  an  autotuner  for  LegUp  HLS
parameters, also using  the OpenTuner framework, which we  published at ReConFig
in  2017\nbsp{}\cite{bruel2017autotuninghls}.  The  autotuner we  implemented in
this  study  targeted 8  hardware  metrics  obtained  from Altera  Quartus,  for
applications  of the  CHStone  HLS benchmark  suite\nbsp{}\cite{hara2008chstone}
targeting  the  Intel  StratixV  FPGA.  The  program  whose  configurations  are
explored  in this  study is  the LegUp  HLS compiler,  and the  search space  is
composed of approximately $10^{126}$ possible combinations of HLS parameters.

One  of  the  obstacles  we  faced was  the  impossibility  of  making  accurate
predictions for hardware metrics  from a set of HLS parameters,  or even for the
generated  Verilog,  requiring that  we  run  the  autotuner using  the  metrics
reported by  the lengthier process  of complete hardware synthesis  instead.  To
mitigate the extra  time cost, we implemented a  virtualized deployment workflow
which enabled  launching several distributed tuning  runs at the same  time.  We
present data showing that the  autotuner using stochastic methods from OpenTuner
found  optimized HLS  parameters  for CHStone  applications  that decreased  the
/Weighted  Normalized Sum/  (/WNS/)  of  hardware metrics  by  up  to 21.5%,  in
relation to the default LegUp configuration.

It  takes a  considerable time  to  synthesize hardware  from specifications  in
hardware description  languages, ranging from  minutes to hours. The  process of
generating  those  specifications from  high-level  C  code  is much  faster  in
comparison, taking only seconds.  Section\nbsp{}[[Autotuner and Search Space for
the  LegUp HLS  Compiler]] describes  in  more detail  the High-Level  Synthesis
process.

#+begin_export latex
\begin{figure}[htpb]
    \centering
    \includegraphics[width=.65\textwidth]{./img/quali_brazil/overview_fpgas_small-eps-converted-to.pdf}
    \caption{Autotuner representation and time scale of the experiments}
    \label{fig:overview-fpgas-small}
\end{figure}
#+end_export

Figure\nbsp{}\ref{fig:overview-fpgas-small}  shows   a  representation   of  our
autotuner for  this experiment,  and highlights the  time scales  involved.  The
autotuner,  represented by  the  light blue  box, receives  as  input a  CHStone
kernel, a target FPGA, input data for the kernel in the order of Kilobytes and a
search space  composed of LegUp's HLS  parameters. The autotuner outputs  an HLS
configuration for each kernel and FPGA.

#+begin_export latex
\begin{figure}[htpb]
  \centering
  \includegraphics[width=.65\textwidth]{./img/quali_brazil/overview_fpgas_big-eps-converted-to.pdf}
  \caption{Autotuner representation and time scale of more complex FPGA
    applications}
  \label{fig:overview-fpgas-big}
\end{figure}
#+end_export

The kernels from CHStone  presented further in Table\nbsp{}\ref{tab:chstone} are
very simple  benchmark kernels, and generating  hardware for them takes  time in
the order  of minutes.  Generating  hardware for more complex  FPGA applications
can  take  several  hours.   Figure\nbsp{}\ref{fig:overview-fpgas-big}  shows  a
representation  and  time scale  for  an  autotuner  that targets  complex  FPGA
applications.   Each  iteration   now  takes  several  hours,   and  limits  the
configurations that can be tested per hour.  We worked around this limitation in
this experiment by implementing a virtualized autotuner using Docker containers,
which enabled parallel measurements of different configurations.

The  rest of  this chapter  is organized  as follows.   Section\nbsp{}[[Autotuning
High-Level  Synthesis  for FPGAs]]  discusses  the  background  on HLS  tools  and
autotuning. Section\nbsp{}[[Autotuner and Search Space  for the LegUp HLS Compiler]]
presents the search  space defined by LegUp HLS parameters  and the autotuner we
implemented using OpenTuner. Section\nbsp{}[[Target Optimization Scenarios and HLS
Kernels]]  introduces  the  optimization scenarios  balancing  different  hardware
metrics we targeted  in this study, discusses the HLS  kernels we optimized, and
presents   the    settings   in   which   the    experiments   were   performed.
Section\nbsp{}[[Performance  Improvements using  Stochastic  Methods]] presents  and
discusses      the      results       on      each      scenario.       Finally,
Section\nbsp{}\ref{sec:fpga-summary} summarizes the chapter and discusses future
work.

*** Autotuning High-Level Synthesis for FPGAs
In this section we discuss background  work related to HLS tools, and autotuning
for FPGAs.

**** Tools for HLS
Various  research   and  vendor  tools   for  High-Level  Synthesis   have  been
developed\nbsp{}\cite{singh2011implementing,  feist2012vivado}.   Villareal  /et
al./\nbsp{}\cite{villarreal2010designing}   implemented    extensions   to   the
Riverside Optimizing Compiler for Configurable Circuits (ROCCC), which also uses
the LLVM  compiler infrastructure,  to add  support for  generating VHDL  from C
code.    Implemented   within   GCC,  GAUT\nbsp{}\cite{coussy2010gaut}   is   an
open-source HLS tool for generating VHDL  from C/C++ code.  Other HLS tools such
as                                  Mitrion\nbsp{}\cite{kindratenko2007mitrion},
Impulse\nbsp{}\cite{antola2007novel} and  Handel\nbsp{}\cite{loo2002handel} also
generate hardware descriptions  from C code.  We refer the  reader to the survey
from Nane  /et al./\nbsp{}\cite{nane2016survey} for a  comprehensive analysis of
recent approaches to HLS.

**** Autotuning for FPGAs
Recent  work  studies  autotuning  approaches   for  FPGA  compilation.   Xu  et
al.\nbsp{}\cite{xu2017parallel} uses distributed OpenTuner instances to optimize
the  compilation flow  from hardware  description to  bitstream.  They  optimize
configuration     parameters      from     the      Verilog-to-Routing     (VTR)
toolflow\nbsp{}\cite{luu2014vtr} and target frequency, wall-clock time and logic
utilization.  Huang  /et al./\nbsp{}\cite{huang2015effect}  study the  effect of
LLVM pass  ordering and  application in LegUp's  HLS process,  demonstrating the
complexity of the search space and the difficulty of its exhaustive exploration.
They exhaustively explore a subset of  LLVM passes and target logic utilization,
execution  cycles,   frequency,  and   wall-clock  time.    Mametjanov  \emph{et
al.}\nbsp{}\cite{mametjanov2015autotuning}   propose  a   machine-learning-based
approach to tune design parameters  for performance and power consumption.  Nabi
and Vanderbauwhede\nbsp{}\cite{nabi2016fast} present a model for performance and
resource utilization for designs based on an intermediate representation.

*** Autotuner and Search Space for the LegUp HLS Compiler
This section  describes our autotuner  implementation, the LegUp  HLS parameters
selected for tuning,  and the autotuning metrics used to  measure the quality of
HLS configurations.

**** Autotuner
We  implemented  our autotuner  with  OpenTuner\nbsp{}\cite{ansel2014opentuner},
using  ensembles  of  search  techniques  to  find  an  optimized  selection  of
LegUp\nbsp{}\cite{canis2015legup}   HLS  parameters,   according  to   our  cost
function, for 11 of the CHStone\nbsp{}\cite{hara2008chstone} kernels.

#+begin_export latex
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.8\textwidth]{./img/quali_brazil/fpga-stack-eps-converted-to.pdf}
    \caption{High-Level Synthesis compilation process. The autotuner search space at the HLS stage is highlighted in blue}
    \label{fig:fpga-stack}
\end{figure}
#+end_export

Figure \ref{fig:fpga-stack} shows  the steps to generate  a hardware description
from  C code.   It also  shows  the Quartus  steps to  generate bitstreams  from
hardware  descriptions and  to obtain  the  hardware metrics  we targeted.   Our
autotuner used LegUp's HLS parameters as  the search space, but it completed the
hardware generation  process to obtain  metrics from Quartus, as  represented by
the blue boxes in Figure \ref{fig:fpga-stack}.

#+begin_export latex
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.6\columnwidth]{./img/quali_brazil/fpga_docker_tuner-eps-converted-to.pdf}
    \caption{Autotuner Setup}
    \label{fig:autotuner}
\end{figure}
#+end_export

Figure \ref{fig:autotuner} shows our setup using Docker containers running LegUp
and Quartus.   This virtualization setup enabled  portable dependency management
and can  be used  to run  experiments in  distributed environments.   The arrows
coming  from  the  autotuner  to  the  containers  represent  the  flow  of  new
configurations generated  by search techniques,  and the arrows coming  from the
containers to  the autotuner  represent the  flow of measurements  for a  set of
parameters.  For CHStone  kernels measurements take approximately  10 minutes to
complete,  and the  majority  of  this time  is  spent  in Quartus's  synthesis,
mapping, and place and route steps.

**** High-Level Synthesis Parameters
We selected  an extensive  set of LegUp  High-Level Synthesis  parameters, shown
partially in  Table \ref{tab:params}. Each  parameter in  the first two  rows of
Table \ref{tab:params} has an $8$, $16$,  $32$ and $64$ bit variant.  /Operation
Latency/ parameters  define the number  of clock  cycles required to  complete a
given  operation when  compiled with  LegUp.  /Zero-latency/  operations can  be
performed in a single clock  cycle.  /Resource Constraint/ parameters define the
number of times a  given operation can be performed in  a clock cycle.  /Boolean
or Multi-Valued/  parameters are  used to  set various  advanced configurations.
For  example,  the  /enable_pattern_sharing/  parameter can  be  set  to  enable
resource sharing  for patterns  of computational operators,  as is  described by
Hadjis  /et   al./\nbsp{}\cite{hadjis2012impact}.   For  a  complete   list  and
description   of   each   parameter,    please   refer   to   LegUp's   official
documentation\nbsp{}\cite{legup2020docsURL}.

#+begin_export latex
\begin{table}[htpb]
\centering
\begin{tabular}{@{}p{0.10\columnwidth}p{0.84\columnwidth}@{}}
\toprule
Type & \multicolumn{1}{c}{Parameters} \\ \midrule
\parbox[t]{0.10\columnwidth}{\scriptsize{Operation \\ Latency}} & \scriptsize{\texttt{\textbf{altfp\_}[divide, truncate, fptosi, add, subtract, multiply, extend, sitofp], \textbf{unsigned\_}[multiply, divide, add, modulus], \textbf{signed\_}[modulus, divide, multiply, add, \textbf{comp\_}[o, u]], [local\_mem, mem]\textbf{\_dual\_port}, \textbf{reg}}} \\
%\addlinespace{}
\parbox[t]{0.10\columnwidth}{\scriptsize{Resource Constraint}} & \scriptsize{\texttt{\textbf{signed\_}[divide, multiply, modulus, add], \textbf{altfp\_}[multiply, add, subtract, divide], \textbf{unsigned\_}[modulus, multiply, add, divide], [shared\_mem, mem]\textbf{\_dual\_port}}} \\
%\addlinespace{}
\parbox[t]{0.10\columnwidth}{\scriptsize{Boolean~or Multi-value}} & \scriptsize{\texttt{\textbf{pattern\_share\_}[add, shift, sub, bitops], \textbf{sdc\_}[multipump, no\_chaining, priority], \textbf{pipeline\_}[resource\_sharing, all], \textbf{ps\_}[min\_size, min\_width, max\_size, bit\_diff\_threshold], \textbf{mb\_}[minimize\_hw, max\_back\_passes], \textbf{no\_roms}, \textbf{multiplier\_no\_chain}, \textbf{dont\_chain\_get\_elem\_ptr}, \textbf{clock\_period}, \textbf{no\_loop\_pipelining}, \textbf{incremental\_sdc}, \textbf{disable\_reg\_sharing}, \textbf{set\_combine\_basicblock}, \textbf{enable\_pattern\_sharing}, \textbf{multipumping}, \textbf{dual\_port\_binding}, \textbf{modulo\_scheduler}, \textbf{explicit\_lpm\_mults}}} \\ \bottomrule
%%\addlinespace{}
\end{tabular}
\caption{Subset of All Autotuned LegUP HLS Parameters}
\label{tab:params}
\end{table}
#+end_export

**** HLS Autotuning Metrics
To  obtain values  for  hardware metrics  we needed  to  perform the  synthesis,
mapping and  place and route steps.   We used Quartus  to do so, and  selected 8
hardware metrics  reported by Quartus to  compose our cost or  fitness function.
From  the fitter  summary we  obtained 6  metrics.  /Logic  Utilization/ (/LUT/)
measures the number of logic elements  and is composed of Adaptive Look-Up Table
(ALUTs),  memory  ALUTs, logic  registers  or  dedicated logic  registers.   The
/Registers/ (/Regs./), /Virtual Pins/  (/Pins/), /Block Memory Bits/ (/Blocks/),
/RAM Blocks/ (/BRAM/) and /DSP Blocks/  (/DSP/) metrics measure the usage of the
resources indicated by their names.

From the  timing analysis we obtained  the /Cycles/ and /FMax/  metrics, used to
compute the /Wall-Clock  Time/ metric.  This metric composed  the cost function,
but /Cycles/ and /FMax/ were not individually  used. We chose to do that because
all  of our  hardware metrics  needed  to be  minimized except  for /FMax/,  and
computing /Wall-Clock  Time/ instead  solved that restriction.   The /Wall-Clock
Time/  $wct$ is  computed  by  $wct =  \text{\textit{Cycles}}  \times (\alpha  /
\text{\textit{FMax}})$, where $\alpha = 10^6$ because /FMax/ is reported in MHz.

The objective function used by the  autotuner to evaluate sets of HLS parameters
is written
#+begin_export latex
\begin{align} \label{eq:wnsm}
  f\left(\textbf{m}, \textbf{w}\right) =
  \dfrac{\sum\limits_{\substack{m \in \textbf{m} \\ w \in \textbf{w}}}
    {w\left(\dfrac{m}{m^{0}}\right)}}
       {\sum\limits_{w \in \textbf{w}}{w}}\text{,}
\end{align}
#+end_export
and computes a /Weighted Normalized Sum/  (/WNS/) of the measured metrics $m \in
\textbf{m}$, where $\textbf{m}$ is a vector  with measurements of the 8 hardware
metrics described previously.  Each weight $w \in \textbf{w}$ corresponds to one
of the  scenarios in Table\nbsp{}\ref{tab:scenarios}.   A value is  computed for
each metric  $m$ in relation to  an initial default value  $m^{0}$, measured for
each metric in a given kernel, using LegUp's default configuration.  For a given
set  measurement vector  $\textbf{x}$, $f(\textbf{x},  \textbf{w}) =  1.0$ means
that there was no improvement relative to the default HLS configuration.

*** Target Optimization Scenarios and HLS Kernels
This section describes the optimization  scenarios, the CHStone kernels, and the
experimental settings.

**** Optimization Scenarios
Table\nbsp{}\ref{tab:scenarios}   shows  the   assigned  weights   in  our   $4$
optimization scenarios.   The /Area/-targeting  scenario assigns low  weights to
wall-clock time  metrics.  The /Performance  and Latency/ scenario  assigns high
weights to  wall-clock time metrics  and also to  the number of  registers used.
The  /Performance/ scenario  assigns low  weights  to area  metrics and  cycles,
assigning a high  weight only to frequency. The /Balanced/  scenario assigns the
same weight to  every metric.  The weights  assigned to the metrics  that do not
appear  on  Table\nbsp{}\ref{tab:scenarios} are  always  $1$.   The weights  are
integers and powers of\nbsp{}2.

#+begin_export latex
\begin{table}[htpb]
    \centering
    \caption{Weights for Optimization Scenarios \\ (\textit{High} $= 8$, \textit{Medium} $= 4$, \textit{Low} $= 2$)}
    \label{tab:scenarios}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Metric & \textit{Area} & \textit{Perf. and Lat} & \textit{Performance} & \textit{Balanced} \\ \midrule
        \textit{LUT} & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{E3DBB3} Medium \\
        \textit{Registers} & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{E3DBB3} Medium & \cellcolor[HTML]{E3DBB3} Medium \\
        \textit{BRAMs} & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{E3DBB3} Medium \\
        \textit{DSPs} & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{E3DBB3} Medium \\
        \textit{FMax} & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{E3DBB3} Medium \\
        \textit{Cycles} & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{9B94B6} High & \cellcolor[HTML]{DD9583} Low & \cellcolor[HTML]{E3DBB3} Medium \\ \bottomrule
    \end{tabular}
    %\addlinespace
\end{table}
#+end_export

We  compared results  when  starting  from a  /Default/  configuration with  the
results when starting at a /Random/ set of parameters. The default configuration
for the StratixV was  provided by LegUp and the comparison  was performed in the
/Balanced/ optimization scenario.

**** Kernels
To test  and validate  our autotuner  we used  11 kernels  from the  CHStone HLS
benchmark suite\nbsp{}\cite{hara2008chstone}.   CHStone kernels  are implemented
in the C  language and contain inputs and previously  computed outputs, allowing
for correctness checks to be performed for all kernels.

#+begin_export latex
\begin{table}[htpb]
\centering
\small
\caption{Autotuned CHStone Kernels}
\label{tab:chstone}
\begin{tabular}{@{}p{0.1\columnwidth}p{0.6\columnwidth}@{}}
\toprule
 Kernel & Short Description \\ \midrule
 blowfish & Symmetric-key block cypher \\
 aes & Advanced Encryption Algorithm (AES) \\
 adpcm & Adaptive Differential Pulse Code Modulation dec. and enc. \\
 sha & Secure Hash Algorithm (SHA) \\
 motion & Motion vector decoding from MPEG-2 \\
 mips & Simplified MIPS processor \\
 gsm & Predictive coding analysis of systems for mobile comms. \\
 dfsin & Sine function for double-precision floating-point numbers \\
 dfmul & Double-precision floating-point multiplication \\
 dfdiv & Double-precision floating-point division \\
 dfadd & Double-precision floating-point addition \\ \bottomrule
%% \addlinespace{}
\end{tabular}
\end{table}
#+end_export

Table \ref{tab:chstone} provides short descriptions of the 11 CHStone kernels we
used. We were not able to compile the  /jpeg/ CHStone kernel, so did not use it.
All experiments targeted the /Intel StratixV 5SGXEA7N2F45C2/ FPGA.

**** Experiments
We    performed   $10$    tuning    runs   of    $1.5h$    for   each    kernel.
Section\nbsp{}[[Performance  Improvements using  Stochastic  Methods]] presents  the
mean  relative improvements  for each  kernel  and individual  metric. The  code
needed  to  run  the experiments  and  generate  the  figures,  as well  as  the
implementation of the autotuner and all data we generated, is open and hosted at
GitHub\nbsp{}\cite{bruel2017hlsURL}.

The   experimental    settings   included   Docker   for    virtualization   and
reproducibility,  /LegUp  v4.0/, /Quartus  Prime  Standard  Edition v16.0/,  and
/CHStone/\nbsp{}\cite{hara2008chstone}.   All experiments  were  performed on  a
machine with two /Intel Xeon CPU E5-2699 v3/ with 18 /x86\under{}64/ cores each,
and  503GB of  RAM.  The  instructions and  the code  to reproduce  the software
experimental       environment      are       open      and       hosted      at
GitHub\nbsp{}\cite{bruel2017dockerfileURL}.

*** Performance Improvements using Stochastic Methods
This section presents summaries of the results from 10 autotuning runs of $1.5h$
in the scenarios from Table\nbsp{}\ref{tab:scenarios}.  Results are presented in
/heatmaps/   where  each   row   has  one   of  the   11   CHStone  kernels   in
Table\nbsp{}\ref{tab:chstone} and each column has  one of the 8 hardware metrics
and their  /Weighted Normalized Sum/  (/WNS/) as described  in Section\nbsp{}[[HLS
Autotuning Metrics]].

Cells on heatmaps show the ratio of tuned to initial values of a hardware metric
in a  CHStone kernel, averaged  over 10 autotuning  runs.  The objective  of the
autotuner is to minimize all hardware  metrics, except for /FMax/, whose inverse
is minimized. Cell values less than $1.0$  always mark an improvement on a given
metric.  Darker  blue squares in  the following heatmaps mark  improvements, and
darker red squares mark worse values in relation to the starting point.

#+begin_export latex
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.5\columnwidth]{./img/quali_brazil/heatmap_comp_stratixV-eps-converted-to.pdf}
    \caption{Comparison of the  absolute values for Random  and Default starting
      points in the Balanced scenario}
    \label{fig:comp}
\end{figure}
#+end_export

Figure\nbsp{}\ref{fig:comp}  compares the  ratios  of absolute  values for  each
hardware metric for  /Default/ and /Random/ starts, in  the /Balanced/ scenario.
Cell  values less  than $1.0$  mean that  the /Default/  start achieved  smaller
absolute  values that  the  /Random/ start.   Cells with  ``--''  mean that  the
/Default/  start could  not find  a set  of HLS  parameters that  produced valid
output during  any of the $1.5h$  tuning runs. The /Default/  start found better
values for most metrics.

The /Random/  start found better  values for /DSP/,  /Pins/ and /FMax/  for some
kernels.  For  example, it found values  49% smaller, 6% smaller  and 53% larger
for  /DSP/,  /Pins/ and  /FMax/,  respectively,  for  the /dfsin/  kernel.   The
/Default/ start  found better values  for /Regs/  and /Cycles/ for  all kernels.
For example, it found values 53% smaller for /Regs/ and /Cycles/ for the /dfmul/
kernel, and 56%  and 55% smaller for /Regs/ and  /Cycles/, respectively, for the
/mips/ kernel.

The /Random/ start found  worst values in most cases because of  the size of the
search  space  and  the  stochastic  nature of  the  explorations  performed  by
OpenTuner methods.  In such scenarios,  knowing and leveraging a reasonably good
starting configuration for a given kernel is extremely important.  The remaining
results  in this  Section  used  one of  the  /Default/ starting  configurations
provided by LegUp, specifically targeted to Stratix V boards.

Figure \ref{fig:balanced} shows the results  for the /Balanced/ scenario.  These
results are the baseline for evaluating  the autotuner in other scenarios, since
all metrics  had the  same weight.   The optimization  target was  the /Weighted
Normalized Sum/  (/WNS/) of  hardware metrics,  but we  were also  interested in
changes in other metrics, as their  relative weights changed.  In the /Balanced/
scenario we expected to see smaller improvements of /WNS/ due to the competition
of concurrent improvements on every metric.

The autotuner  found values  of /WNS/  16% smaller for  the /adpcm/  and /dfdiv/
kernels, and  15% smaller for /dfmul/.   Even for the /Balanced/  scenario it is
possible to see that some  metrics decreased while others decreased consistently
over the 10 tuning runs.  /FMax/ and  /DSP/ had the larger improvements for most
kernels, for  example, 51% greater  /FMax/ in /adpcm/  and 69% smaller  /DSP/ in
/dfmul/.  /Cycles/,  /Regs/ and /Pins/ had  the worst results in  this scenario,
with 34% larger /Cycles/ in /dfdiv/, 15% larger /Regs/ in /dfdiv/ and 17% larger
/Pins/ in /gsm/.   Other metrics had smaller improvements or  no improvements at
all in most kernels.

#+begin_export latex
\begin{figure}[htpb]
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{./img/quali_brazil/heatmap_default_stratixV-eps-converted-to.pdf}
        \caption{Relative improvement for all metrics in the \textit{Balanced}
        scenario}
        \label{fig:balanced}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{./img/quali_brazil/heatmap_default_stratixV_area-eps-converted-to.pdf}
        \caption{Relative improvement for all metrics in the \textit{Area}
        scenario}
        \label{fig:area}
    \end{minipage}%
\end{figure}
#+end_export

Figure \ref{fig:area}  shows the  results for the  /Area/ scenario.   We believe
that the  greater coherence  of optimization objectives  is responsible  for the
greater improvements of  /WNS/ in the following scenarios.   The autotuner found
values of  /WNS/ 23% smaller for  /dfdiv/, 18% smaller for  /dfmul/, and smaller
values overall in comparison with the /Balanced/ scenario.  Regarding individual
metrics, the  values for /FMax/ were  worse overall, with 14%  smaller /FMax/ in
/gsm/ and  62% greater /Cycles/,  for example.   As expected for  this scenario,
metrics related to area had better improvements than in the /Balanced/ scenario,
with 73% and 72% smaller /DSP/ for /dfmul/ and /dfdiv/ respectively, 33% smaller
/Blocks/ and /BRAM/ in /dfdiv/ and smaller values overall for /Regs/ and /LUTs/.

Figure \ref{fig:perf}  shows the  results for  the /Performance/  scenario.  The
autotuner  found values  of  /WNS/  23% smaller  for  /dfmul/,  19% smaller  for
/dfdiv/, and smaller values overall than in the /Balanced/ scenario.  /FMax/ was
the only  metric with  a /High/  weight in  this scenario,  so most  metrics had
improvements close  overall to the  /Balanced/ scenario.  The values  for /FMax/
were best overall, with better improvements  in most kernels.  For example, 41%,
30%,  44%  and  37%  greater  /FMax/ in  /dfdiv/,  /dfmul/,  /dfsin/  and  /aes/
respectively.

#+begin_export latex
\begin{figure}[htpb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{./img/quali_brazil/heatmap_default_stratixV_perf-eps-converted-to.pdf}
        \caption{Relative improvement for all metrics in the
        \textit{Performance} scenario}
        \label{fig:perf}
    \end{minipage}%
    \hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{./img/quali_brazil/heatmap_default_stratixV_perflat-eps-converted-to.pdf}
        \caption{Relative improvement for all metrics in the
        \textit{Performance and Latency} scenario}
        \label{fig:perflat}
    \end{minipage}
\end{figure}
#+end_export

Figure \ref{fig:perflat}  shows the  results for  the /Performance  and Latency/
scenario.  The  autotuner found  values of  /WNS/ 24%  smaller for  /adpcm/, 18%
smaller for /dfdiv/, and smaller values overall than in the /Balanced/ scenario.
/Regs/, /Cycles/ and /FMax/ had higher weights in this scenario, and also better
improvements overall.   For example, 16% and  15% smaller /Regs/ in  /dfdiv/ and
/sha/  respectively,   23%  and  11%   smaller  /Cycles/  in  /sha/   and  /aes/
respectively, and 53% greater /FMax/ in  /adpcm/.  Although /FMax/ had the worst
improvements in relation  to the /Balanced/ scenario, the  /Wall-Clock Time/ was
still decreased by the smaller values of /Cycles/.

Figure \ref{fig:wns-comp} summarizes the average  improvements on /WNS/ in the 4
scenarios over 10  runs. Only the /Weighted Normalized Sum/  of metrics directly
guided optimization. With  the exception of /dfadd/ in  the /Balanced/ scenario,
the  autotuner decreased  /WNS/  for all  kernels  in all  scenarios  by 10%  on
average,  and  up   to  24%  for  /adpcm/  in  the   /Performance  and  Latency/
scenario. The figure also shows the average decreases for each scenario.

#+begin_export latex
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.5\columnwidth]{./img/quali_brazil/heatmap_wns_comparison-eps-converted-to.pdf}
    \caption{Relative improvement for \textbf{WNS} in all scenarios}
    \label{fig:wns-comp}
\end{figure}
#+end_export

*** Summary
:PROPERTIES:
:CUSTOM_ID: sec:fpga-summary
:END:
This  study highlights  the importance  of starting  positions for  an autotuner
based  on stochastic  methods. This  becomes more  relevant as  the size  of the
search space  and the number of  targeted metrics increase.  The  flexibility of
our virtualized approach is evidenced  by the results for different optimization
scenarios.  Improvements in /WNS/ increased when higher weights were assigned to
metrics that express a coherent objective such as area, performance and latency.
The improvements of metrics related to those objectives also increased.

Kernels  with  large  measurement  time  still presented  a  challenge  for  the
autotuning approach we used in this problem, even with distributed measurements,
because the stochastic methods implemented  in OpenTuner reach better results if
longer explorations of  the search space are performed.   An Experimental Design
approach such as  the one we later  developed and applied to  other problems, as
described in Chapter\nbsp{}\ref{chap:laplacian}, would be ideal for this type of
problem.  Since the HLS toolchain used in this study became proprietary software
and FPGA programming is still a  very closed domain, requiring paid licenses and
still expensive hardware, we did not  have the opportunity to revisit this study
with the Experimental Design methods we later studied.

Future work in this direction will study the impact of different starting points
on the  final tuned values in  each optimization scenario, for  example we could
start tuning for /Performance/ at the  best autotuned /Area/ value.  We expected
that starting  positions tailored  for each target  kernel will  enable the
autotuner  to  find  better  /WNS/  values faster.   We  will  also  apply  this
autotuning methodology  to HLS  tools that  enable a  fast prediction  of metric
values.   These tools  will  enable  the exploration  of  the trade-off  between
prediction accuracy and the time to measure an HLS configuration.

** Parameters for an OpenCL Laplacian Kernel
:PROPERTIES:
:CUSTOM_ID: chap:laplacian
:END:
This  chapter introduces  our Experimental  Design approach  to autotuning,  and
presents  an  application   were  we  continued  the   exploration,  started  by
Masnada\nbsp{}\cite{masnada2016semi},  of performance  optimization methods  for
the  search space  defined by  the parameters  of a  Laplacian GPU  kernel.  The
kernel  was implemented  using  BOAST\nbsp{}\cite{videau2017boast}, a  framework
that enables writing and optimizing  HPC applications using metaprogramming, and
is  publicly hosted  on  GitHub\nbsp{}\cite{boast2021source}.   We targeted  the
/Nvidia  K40c/ GPU,  and the  objective function  we minimized  was the  time to
compute each pixel.   We evaluated the performance of  nine optimization methods
for this  kernel, including sampling strategies,  stochastic methods, variations
on linear  regression, our  Experimental Design  approach, and  Gaussian Process
Regression.   Our  transparent  and parsimonious  Experimental  Design  approach
achieved  the  most consistent  results  across  the  methods we  tested,  which
motivated searching  for a more  comprehensive set  of applications in  which we
could evaluate the performance of our approach. This more comprehensive study is
described in  Chapter\nbsp{}\ref{chap:spapt}.  The initial steps  of the studies
presented in  this chapter and  the next were  published together in  the CCGRID
conference\nbsp{}\cite{bruel2019autotuning}.

The  remainder  of this  chapter  is  organized as  follows.   Section\nbsp{}[[The
Laplacian  Kernel]] presents  the target  kernel and  the associated  search space
exposed   by  the   BOAST  implementation.    Section\nbsp{}[[A  Transparent   and
Parsimonious Approach to Autotuning using Optimal Design]] presents our adaptation
of a sequential approach to autotuning, starting with modeling assumptions about
the  search  space  and  iteratively  refining  models  based  on  ANOVA  tests.
Section\nbsp{}[[Building  a  Performance  Model]]  discusses  our  initial  modeling
hypotheses.  Section\nbsp{}[[Looking at a Single DLMT Run]] looks at a single run of
our approach, linking the  factor levels that were chosen at  each step to ANOVA
/p/-values,   and   comparing   to   the   levels   of   the   global   minimum.
Section\nbsp{}[[Evaluation of  Optimization Methods]]  evaluates the  performance of
each method.  Finally,  Section\nbsp{}\ref{sec:laplacian-summary} the discussion
and concludes the chapter.

*** The Laplacian Kernel
The Laplacian $\Delta{}f$ of a function $f$ with inputs $x$ and $y$ is written
#+begin_export latex
\begin{align}
  \Delta{}f = \frac{\partial^{2} f}{\partial x^{2}} +
  \frac{\partial^{2} f}{\partial y^{2}}\text{,}
\end{align}
#+end_export
and can be interpreted as providing a measure of how much the values of $f$ in a
neighborhood  of  a point  $(x,  y)$  deviate, on  average,  from  the value  of
$f(x,y)$.  The discrete Laplacian operator  is commonly used in image processing
algorithms such as edge detection, where it is typically composed with smoothing
filters.  In the discrete setting the Laplacian of an image can be computed by a
convolution filter,  where a single  kernel encodes the smoothing  and Laplacian
filters. Figure\nbsp{}[[fig:laplacian]] shows the effect of applying a /Laplacian of
Gaussian/ convolution filter to detect edges in a picture.

#+NAME: fig:laplacian
#+CAPTION: Edge-detection effect of a
#+CAPTION: /Laplacian of Gaussian/ filter
#+ATTR_LATEX: :placement [t] :width .65\textwidth
[[./img/laplacian/flower.jpg]]

**** BOAST Code and the OpenCL Kernel
Our Experimental  Design approach can be  applied to any autotuning  domain that
expresses optimization as  a search problem, but the  performance evaluations we
present  in  this  thesis  were  obtained  in  the  domain  of  source-to-source
transformation. Several  frameworks, compilers, and autotuners  provide tools to
generate             and              optimize             architecture-specific
code\nbsp{}\cite{hartono2009annotation,videau2017boast,tiwari2009scalable,yi2007poet,ansel2009petabricks}.
We  used  BOAST\nbsp{}\cite{videau2017boast}  to   generate  code  for  GPUs  by
generating  an  OpenCL  Laplacian   kernel,  optimizing  parameters  controlling
vectorization, loop transformations, and data structure size and copying.

#+begin_export latex
\label{fig:cpu-laplacian}
\begin{figure}[t]
\lstset{language=C,basicstyle=\ttfamily\scriptsize,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
void kernel(int width, int height, uint8_t *source, uint8_t *destination){
    int i, j, d, ii, jj, tmp;
    for (j = 1; j < height - 1; j++) {
        for (i = 1; i < width - 1; i++) {
            // Process RGB components separately
            for (d = 0; d < 3; d ++) {
                tmp = 0;
                for(jj = -1; jj < 1; jj++) {
                    for(ii = -1; ii < 1; ii++) {
                        if(ii == 0 && jj == 0) {
                            tmp += 9 * source[d + (3 * (i + (width * j)))];
                        } else {
                            tmp -= source[d + (3 * ((i + ii) + (width * (j + jj))))];
                        }
                    }
                }
                // Clamp to valid pixel values
                destination[d + (3 * (i + (width * j)))] = tmp < 0 ? 0 :
                    (tmp > 255 ? 255 : tmp);
            }
        }
    }
}
\end{lstlisting}
\caption{A CPU Laplacian kernel written in C}
\end{figure}
#+end_export

Figure\nbsp{}\ref{fig:cpu-laplacian} shows a Laplacian kernel naively written in
C. In this code,  the RGB channels of an image are stored  a single /int/ array,
representing a  color image. A three  by three convolution kernel  is applied to
each color channel separately.

For comparison, the  BOAST Ruby script for generating  a parameterized Laplacian
kernel is  partially shown in Figure\nbsp{}\ref{fig:boast-generator}.   A sample
of      the       resulting      OpenCL       kernel      is       shown      in
Figure\nbsp{}\ref{fig:gpu-laplacian-generated}.   The  difference in  complexity
between the naive implementation in Figure\nbsp{}\ref{fig:cpu-laplacian} and the
parameterized  generated   version  helps   motivate  the  usage   of  automated
optimization methods for autotuning.

#+begin_export latex
\clearpage
\label{fig:boast-generator}
\begin{figure}[t]
\lstset{language=Ruby,basicstyle=\ttfamily\tiny,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
def laplacian(options)
  default_options = {:x_component_number => 1,
                     :vector_length => 1,
                     :y_component_number => 1,
                     :temporary_size => 2,
                     :vector_recompute => false,
                     :load_overlap => false}
  # [...] (Omitted)
  # Begining of the generated procedure, based on the baseline
  p = Procedure("kernel", [psrc, pdst, width, height]) {
    # [...] (Omitted)
    # Using vector_length to compute the number of vectors
    vector_number = (x_component_number.to_f/vector_length).ceil
    total_x_size = vector_recompute ? vector_number *
                                      vector_length : x_component_number
    # [...] (Omitted)
    # Using y_component_number to compute offsets
    y_offset = y_component_number + 1
    # Controlling load_overlap
    if not load_overlap then
      total_load_window = total_x_size + 6
      tempload = []
      ranges = split_in_ranges(total_load_window, vector_length)
      ranges.each { |r|
        tempload.push( Int("tempload#{r.begin}_#{r.end}", :size => 1,
                           :vector_length => (r.end - r.begin + 1), :signed => false) )
      }
      decl *(tempload)
    else
      tempnn = (0..2).collect { |v_i|
        (0...vector_number).collect { |x_i|
          (0...(y_component_number+2)).collect { |y_i|
            Int("temp#{x_i}#{v_i}#{y_i}", :size => 1,
                :vector_length => vector_length, :signed => false)
          }
        }
      }
      decl *(tempnn.flatten)
    end
    # [...] (Omitted)
    # Using temporary_size to allocate data
    tempcnn = (0..2).collect { |v_i|
      (0...vector_number).collect { |x_i|
        (0...(y_component_number+2)).collect { |y_i|
          Int("tempc#{x_i}#{v_i}#{y_i}", :size => temporary_size,
              :vector_length => vector_length)
        }
      }
    }
    decl *(tempcnn.flatten)
    # [...] (Omitted)
    # Generate kernel and clamp to valid pixel values
    (0...vector_number).each { |v_i|
      (0...y_component_number).each { |y_i|
        pr rescnn[v_i][y_i] === - tempcnn[0][v_i][y_i] -
                                tempcnn[1][v_i][y_i] -
                                tempcnn[2][v_i][y_i] -
                                tempcnn[0][v_i][y_i + 1] +
                                tempcnn[1][v_i][y_i + 1] *
                                "(#{temp_type})9" -
                                tempcnn[2][v_i][y_i + 1] -
                                tempcnn[0][v_i][y_i + 2] -
                                tempcnn[1][v_i][y_i + 2] -
                                tempcnn[2][v_i][y_i + 2]
        pr resnn[v_i][y_i] === clamp(rescnn[v_i][y_i],
                                     "(#{temp_type})0",
                                     "(#{temp_type})255",
                                     :returns => rescnn[v_i][y_i])
      }
    }
    # [...] (Omitted)
  }
  # Variable p contains the complete generated OpenCL code
  pr p
  k.procedure = p
  return k
end
\end{lstlisting}
\caption{Excerpts  of the  BOAST code,  in  Ruby, that  generates the  Laplacian
  OpenCL      kernel.     The      code      is      publicly     hosted      at
  GitHub~\cite{boast2021laplacian}}
\end{figure}
\clearpage
#+end_export

#+begin_export latex
\clearpage
\label{fig:gpu-laplacian-generated}
\begin{figure}[t]
\lstset{language=C,basicstyle=\ttfamily\tiny,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
__kernel void kernel(const __global uchar * psrc, __global uchar * pdst,
                     const int width, const int height){
    int y, x, w;
    x = (get_global_id(0)) * (1);
    y = (get_global_id(1)) * (3);
    w = (width) * (3);
    x = (x < 3 ? 3 : (x > w - (11) ? w - (11) : x));
    y = (y < 1 ? 1 : (y > height - (4) ? height - (4) : y));
    uchar8 tempload0_7, tempload8_11, tempload12_13,
           res00, res01, res02, tempc000;
    int8 tempc001, tempc002, tempc003, tempc004, tempc010, tempc011, tempc012,
         tempc013, tempc014, tempc020, tempc021, tempc022, tempc023, tempc024,
         resc00, resc01, resc02;
    tempload0_7 = vload8(0, &psrc[x + -3 + (w) * (y + -1)]);
    tempload8_11 = vload4(0, &psrc[x + 5 + (w) * (y + -1)]);
    tempload12_13 = vload2(0, &psrc[x + 9 + (w) * (y + -1)]);
    tempc000 = convert_int8( (uchar8)(tempload0_7.s01234567) );
    tempc010 = convert_int8( (uchar8)(tempload0_7.s345,tempload0_7.s67,tempload8_11.s012) );
    tempc020 = convert_int8( (uchar8)(tempload0_7.s67,tempload8_11.s0123,
                                      tempload12_13.s01) );
    tempload0_7 = vload8(0, &psrc[x + -3 + (w) * (y + 0)]);
    tempload8_11 = vload4(0, &psrc[x + 5 + (w) * (y + 0)]);
    tempload12_13 = vload2(0, &psrc[x + 9 + (w) * (y + 0)]);
    tempc001 = convert_int8( (uchar8)(tempload0_7.s01234567) );
    tempc011 = convert_int8( (uchar8)(tempload0_7.s345,tempload0_7.s67,tempload8_11.s012) );
    tempc021 = convert_int8( (uchar8)(tempload0_7.s67,tempload8_11.s0123,
                                      tempload12_13.s01) );
    tempload0_7 = vload8(0, &psrc[x + -3 + (w) * (y + 1)]);
    tempload8_11 = vload4(0, &psrc[x + 5 + (w) * (y + 1)]);
    tempload12_13 = vload2(0, &psrc[x + 9 + (w) * (y + 1)]);
    tempc002 = convert_int8( (uchar8)(tempload0_7.s01234567) );
    tempc012 = convert_int8( (uchar8)(tempload0_7.s345,tempload0_7.s67,tempload8_11.s012) );
    tempc022 = convert_int8( (uchar8)(tempload0_7.s67,tempload8_11.s0123,
                                      tempload12_13.s01) );
    tempload0_7 = vload8(0, &psrc[x + -3 + (w) * (y + 2)]);
    tempload8_11 = vload4(0, &psrc[x + 5 + (w) * (y + 2)]);
    tempload12_13 = vload2(0, &psrc[x + 9 + (w) * (y + 2)]);
    tempc003 = convert_int8( (uchar8)(tempload0_7.s01234567) );
    tempc013 = convert_int8( (uchar8)(tempload0_7.s345,tempload0_7.s67,
                                      tempload8_11.s012) );
    tempc023 = convert_int8( (uchar8)(tempload0_7.s67,tempload8_11.s0123,
                                      tempload12_13.s01) );
    tempload0_7 = vload8(0, &psrc[x + -3 + (w) * (y + 3)]);
    tempload8_11 = vload4(0, &psrc[x + 5 + (w) * (y + 3)]);
    tempload12_13 = vload2(0, &psrc[x + 9 + (w) * (y + 3)]);
    tempc004 = convert_int8( (uchar8)(tempload0_7.s01234567) );
    tempc014 = convert_int8( (uchar8)(tempload0_7.s345,tempload0_7.s67,tempload8_11.s012) );
    tempc024 = convert_int8( (uchar8)(tempload0_7.s67,tempload8_11.s0123,
                                      tempload12_13.s01) );
    // Kernel computation
    resc00 =  -(tempc000) - (tempc010) - (tempc020) - (tempc001) + (tempc011) *
        ((int)9) - (tempc021) - (tempc002) - (tempc012) - (tempc022);
    res00 = convert_uchar8( clamp(resc00, (int)0, (int)255) );
    resc01 =  -(tempc001) - (tempc011) - (tempc021) - (tempc002) + (tempc012) *
        ((int)9) - (tempc022) - (tempc003) - (tempc013) - (tempc023);
    res01 = convert_uchar8( clamp(resc01, (int)0, (int)255) );
    resc02 =  -(tempc002) - (tempc012) - (tempc022) - (tempc003) + (tempc013) *
        ((int)9) - (tempc023) - (tempc004) - (tempc014) - (tempc024);
    res02 = convert_uchar8( clamp(resc02, (int)0, (int)255) );
    vstore8( res00, 0, &pdst[x + 0 + (w) * (y + 0)] );
    vstore8( res01, 0, &pdst[x + 0 + (w) * (y + 1)] );
    vstore8( res02, 0, &pdst[x + 0 + (w) * (y + 2)] );
}
\end{lstlisting}
\caption{A sample OpenCL Laplacian kernel generated by BOAST}
\end{figure}
\clearpage
#+end_export

*** A Transparent and Parsimonious Approach to Autotuning using Optimal Design
In  this  section   we  discuss  in  detail  our   iterative,  transparent,  and
parsimonious      Experimental      Design     approach      to      autotuning.
Figure\nbsp{}[[fig:doe_anova_strategy]] presents  an overview.  In step  1 we define
the factors and levels  that compose the search space of  the target problem, in
step 2  we select an  initial performance  model, and in  step 3 we  generate an
experimental design. We run the experiments in step 4 and then, as we discuss in
the  next  section, we  identify  significant  factors  with  an ANOVA  test  in
step 5. This  enables selecting and fitting  a new performance model  in steps 6
and  7.  The  new  model  is used  in  step 8  for  predicting  levels for  each
significant factor.  We then go back to  step 3, generating a new design for the
new problem subspace with the remaining factors.  Informed decisions made by the
user at each step guide the outcome of each iteration.

#+NAME: fig:doe_anova_strategy
#+CAPTION: Overview of the ED approach to autotuning we implemented
#+ATTR_LATEX: :width .6\columnwidth :placement [t]
[[./img/ccgrid19/doe_anova_strategy.pdf]]

Step 1 of our approach is to define target factors and which of their levels are
worth exploring.   Then, the user  must select  an initial performance  model in
step  2.   Compilers typically  expose  many  2-level  factors  in the  form  of
configuration flags, and the  performance model for a single flag  can only be a
linear term,  since there are  only 2  values to measure.   Interactions between
flags  and numerical  factors  such as  block  sizes in  CUDA  programs or  loop
unrolling amounts  are also common. Deciding  which levels to include  for these
kinds of factors requires more careful  analysis. For example, if we suspect the
performance model has  a quadratic term for a certain  factor, the design should
include at least  three factor levels. The ordering between  the levels of other
compiler parameters,  such as  /-O(0,1,2,3)/, is not  obviously translated  to a
number.   Factors  like these  are  named  /categorical/,  and must  be  treated
differently when constructing designs in step 3 and analyzing results in step 5.

After the design  is constructed in step  3, we run each  selected experiment in
step 4. This step can run in parallel since experiments are independent. Not all
target programs  run successfully  in their entire  input range,  making runtime
failures common  in this step.  The user can decide  whether to construct  a new
design  using the  successfully  completed  experiments or  to  continue to  the
analysis step if enough experiments succeed.

After running the ANOVA  test in step 5, the user  should apply domain knowledge
to analyze the ANOVA table and  determine which factors are significant. Certain
factors  might  not  appear  significant  and should  not  be  included  in  the
regression  model. Selecting  the model  after  the ANOVA  test in  step 6  also
benefits from domain knowledge.

A central assumption  of ANOVA is the /homoscedasticity/ of  the response, which
can  be interpreted  as  requiring  the observed  error  on  measurements to  be
independent of  factor levels and  of the number of  measurements.  Fortunately,
there are statistical  tests and corrections for lack  of homoscedasticity.  Our
approach uses the homoscedasticity check and correction by power transformations
from the =car= package\nbsp{}\cite{fox2011car} of the =R= language.

We fit  the selected model to  our design's data in  step 7, and use  the fitted
model in  step 8 to find  levels that minimize  the response. The choice  of the
method used to find  these levels depends on factor types  and on the complexity
of the  model and search  space. If  factors have discrete  levels, neighborhood
exploration might  be needed to  find levels  that minimize the  response around
predicted  levels. Constraints  might put  predicted levels  on an  undefined or
invalid region on the search space. This presents challenge, because the borders
of valid regions would have to be explored.

In  step  8 we  also  fix  factor levels  to  those  predicted to  achieve  best
performance.   The  user can  also  decide  the level  of  trust  placed on  the
prediction at this step, by keeping other levels available. In step 8 we perform
a reduction of problem dimension by  eliminating factors and decreasing the size
of the search space. If we  identified significant parameters correctly, we will
have restricted further search to better regions.

In  this   chapter  and   in  Chapter\nbsp{}\ref{chap:spapt}  we   evaluate  the
performance  of this  Experimental Design  approach to  autotuning in  different
applications.   The next  section discusses  the implementation  of a  Laplacian
kernel for GPUs and the resulting search space.

*** Building a Performance Model
This section  describes the search space  exposed by the BOAST  Laplacian kernel
and our initial modeling hypotheses.

**** Search Space
The   factors   and  levels   defining   the   search   space  are   listed   in
Table\nbsp{}[[tab:gpu_laplacian_factors]].   The  complete   search  space  contains
$1.9\times10^5$   configurations,  but   removing  invalid   configurations,  or
configurations that fail at runtime,  yields a search space with $2.3\times10^4$
configurations. The valid search space took 154 hours to be completely evaluated
on an /Intel Xeon E5-2630v2/ CPU, with /gcc/ version /4.8.3/ and /Nvidia/ driver
version /340.32/.


#+ATTR_LATEX: :booktabs t :align lll :font \small :float t :placement [t]
#+NAME: tab:gpu_laplacian_factors
#+CAPTION: Parameters of the Laplacian Kernel
|-------------------------------+-------------------------------+--------------------------------|
| Factor                        | Levels                        | Short Description              |
|-------------------------------+-------------------------------+--------------------------------|
| \textit{vector\_length}       | $2^0,\dots,2^4$               | Size of vectors                |
| \textit{load\_overlap}        | \textit{true}, \textit{false} | Load overlaps in vectorization |
| \textit{temporary\_size}      | $2,4$                         | Byte size of temporary data    |
| \textit{elements\_number}     | $1,\dots,24$                  | Size of equal data splits      |
| \textit{y\_component\_number} | $1,\dots,6$                   | Loop tile size                 |
| \textit{threads\_number}      | $2^5,\dots,2^{10}$            | Size of thread groups          |
| \textit{lws\_y}               | $2^0,\dots,2^{10}$            | Block size in $y$ dimension    |
|-------------------------------+-------------------------------+--------------------------------|
**** Modeling the Impact of Each Factor
This section briefly describes each factor and our modeling hypotheses regarding
their impact on performance.

***** Linear Terms
The effects  of the following  factors were  modeled with linear  terms, whether
because  they are  categorical  binary  factors or  to  attempt  to exploit  the
simplest relationship under uncertainty of the effects.

- The  /vector\under{}length/  factor  controls  the  vector  size  used  during
  computation. Architectures supporting vectorization provide speedups by saving
  instruction  decoding time,  but  vectors must  be  properly sized.   Although
  /NVIDIA/  GPUs did  not  support  vectorization at  the  time  of this  study,
  vectorization can still impact performance via cache effects
- The /load\under{}overlap/  factor is  binary, and  its effect  is consequently
  modeled with  a linear term.  The  parameter encodes the choice  of whether to
  save memory load instructions by overlapping vectors in memory
- The  /lws\under{}y/  factor controls  the  size  of  a  thread block,  in  the
  /y/-axis. We did not have assumptions about the behavior of this factor, so we
  attempted a linear term

***** Linear plus Inverse Terms
The effects  of the following  factors were modeled with  a linear term  plus an
inverse term. In  our initial model we  assumed that these factors  had a linear
effect on performance, but also that  a more complex relationship existed due to
different kinds of  overhead. In all cases, we modeled  the expected overhead as
an inverse term.

- The /elements\under{}number/  factor controls the  size of image  portion that
  will be  processed by each  thread, and  using smaller portions  implies using
  more  threads.   As  more  threads  are  used  we  expect  an  improvement  on
  performance, but also an overhead due to extra memory loads and to the cost of
  managing threads
- The /y\under{}component\under{}number/ factor controls tiling  on the /y/-axis of the
  target image,  inside each  thread. The  optimal tiling  size is  a compromise
  between fitting a tile in cache  and providing enough prefetched memory to not
  slow computation down
- The /threads\under{}number/  parameter controls the  size of an  OpenCL thread
  work  group.  Threads  in  the same  group  share data  and  can be  scheduled
  together.   Using more  smaller  groups can  improve  performance from  better
  scheduling, but imply in more management overhead

***** The Complete Initial Model
Putting together the  terms for all factors, the complete  initial model for the
time to compute a single pixel is written
#+BEGIN_EXPORT latex
\begin{align}
  \label{eq:gpu_laplacian_performance_model}
  \textit{time\_per\_pixel} \sim &  \textit{y\_component\_number} + \frac{1}{\textit{y\_component\_number}} \; + \nonumber \\
  & \textit{temporary\_size} + \textit{vector\_length} + \textit{load\_overlap} \; + \nonumber \\
  & \textit{lws\_y} + \frac{1}{\textit{lws\_y}} + \textit{elements\_number} + \frac{1}{\textit{elements\_number}} \; + \nonumber \\
  & \textit{threads\_number} + \frac{1}{\textit{threads\_number}}\text{,}
\end{align}
#+END_EXPORT
where coefficients were omitted. This model  was pruned at each iteration of our
method, fixing significant factors to their best predicted level.

*** Looking at a Single DLMT Run
This section  presents a more detailed  look at the iterative  process performed
during   the   optimization   of   the   Laplacian   kernel   by   our   method.
Table\nbsp{}\ref{tab:laplacian-anova} shows  the ANOVA  tests performed  at each
step   in   Figure\nbsp{}[[fig:dlmt-laplacian-steps]],  highlighting   the   factors
identified to  be significant  at each  step, which  were subsequently  fixed to
their  best predicted  levels. Note  that  the size  of the  model being  tested
decreases at each step, as factors are fixed.  These progressive restrictions to
slices of the search space enable the  ANOVA test to detect differences in means
that were previously unclear.

#+begin_export latex
\begin{table}[ht]
  \centering
  \small
  \caption{ANOVA tests at each step. Red lines mark model terms that were considered significant, with $p < 0.05$, and fixed in a given step}
  \label{tab:laplacian-anova}
  \begin{tabular}{llrrr}
    \toprule
    Step & Term & Sum Sq. & F-value & p($>$F) \\
    \midrule
    \multirow{11}{*}{$1^{\text{st}}$} & \textit{y\_component\_number} & $2.1 \times 10^{-18}$ & $7.3 \times 10^{-1}$ & $4.1 \times 10^{-1}$ \\
    & \textit{1/y\_component\_number} & $4.4 \times 10^{-18}$ & $1.6 \times 10^{0}$ & $2.4 \times 10^{-1}$ \\
    & \cellcolor{red!25}\textit{vector\_length} & \cellcolor{red!25}$1.3 \times 10^{-17}$ & \cellcolor{red!25}$4.4 \times 10^{0}$ & \cellcolor{red!25}$4.7 \times 10^{-2}$ \\
    & \cellcolor{red!25}\textit{lws\_y} & \cellcolor{red!25}$6.9 \times 10^{-17}$ & \cellcolor{red!25}$2.4 \times 10^{1}$ & \cellcolor{red!25}$3.5 \times 10^{-4}$ \\
    & \cellcolor{red!25}\textit{1/lws\_y} & \cellcolor{red!25}$1.8 \times 10^{-17}$ & \cellcolor{red!25}$6.2 \times 10^{0}$ & \cellcolor{red!25}$2.8 \times 10^{-2}$ \\
    & \textit{load\_overlap} & $9.1 \times 10^{-20}$ & $3.2 \times 10^{-2}$ & $8.6 \times 10^{-1}$ \\
    & \textit{temporary\_size} & $7.1 \times 10^{-18}$ & $2.5 \times 10^{0}$ & $1.4 \times 10^{-1}$ \\
    & \textit{elements\_number} & $3.1 \times 10^{-19}$ & $1.1 \times 10^{-1}$ & $7.5 \times 10^{-1}$ \\
    & \textit{1/elements\_number} & $1.3 \times 10^{-18}$ & $4.4 \times 10^{-1}$ & $5.2 \times 10^{-1}$ \\
    & \textit{threads\_number} & $7.2 \times 10^{-18}$ & $2.5 \times 10^{0}$ & $1.4 \times 10^{-1}$ \\
    & \textit{1/threads\_number} & $4.3 \times 10^{-18}$ & $1.5 \times 10^{0}$ & $2.4 \times 10^{-1}$ \\
    \midrule
    \multirow{8}{*}{$2^{\text{nd}}$} & \cellcolor{red!25}\textit{y\_component\_number} & \cellcolor{red!25}$1.2 \times 10^{-19}$ & \cellcolor{red!25}$2.1 \times 10^{1}$ & \cellcolor{red!25}$1.4 \times 10^{-3}$ \\
    & \cellcolor{red!25}\textit{1/y\_component\_number} & \cellcolor{red!25}$1.4 \times 10^{-20}$ & \cellcolor{red!25}$2.4 \times 10^{0}$ & \cellcolor{red!25}$1.5 \times 10^{-1}$ \\
    & \textit{load\_overlap} & $4.1 \times 10^{-21}$ & $7.3 \times 10^{-1}$ & $4.1 \times 10^{-1}$ \\
    & \textit{temporary\_size} & $1.4 \times 10^{-21}$ & $2.6 \times 10^{-1}$ & $6.2 \times 10^{-1}$ \\
    & \textit{elements\_number} & $6.0 \times 10^{-22}$ & $1.1 \times 10^{-1}$ & $7.5 \times 10^{-1}$ \\
    & \textit{1/elements\_number} & $2.7 \times 10^{-21}$ & $4.8 \times 10^{-1}$ & $5.0 \times 10^{-1}$ \\
    & \cellcolor{red!25}\textit{threads\_number} & \cellcolor{red!25}$7.2 \times 10^{-21}$ & \cellcolor{red!25}$1.3 \times 10^{0}$ & \cellcolor{red!25}$2.9 \times 10^{-1}$ \\
    & \cellcolor{red!25}\textit{1/threads\_number} & \cellcolor{red!25}$2.9 \times 10^{-20}$ & \cellcolor{red!25}$5.1 \times 10^{0}$ & \cellcolor{red!25}$4.0 \times 10^{-2}$ \\
    \midrule
    \multirow{4}{*}{$3^{\text{rd}}$} & \textit{load\_overlap} & $7.4 \times 10^{-25}$ & $3.8 \times 10^{0}$ & $1.1 \times 10^{-1}$ \\
    & \textit{temporary\_size} & $1.1 \times 10^{-22}$ & $5.7 \times 10^{2}$ & $2.4 \times 10^{-1}$ \\
    & \cellcolor{red!25}\textit{elements\_number} & \cellcolor{red!25}$9.3 \times 10^{-22}$ & \cellcolor{red!25}$4.7 \times 10^{3}$ & \cellcolor{red!25}$1.2 \times 10^{-8}$ \\
    & \cellcolor{red!25}\textit{1/elements\_number} & \cellcolor{red!25}$3.1 \times 10^{-22}$ & \cellcolor{red!25}$1.6 \times 10^{3}$ & \cellcolor{red!25}$1.9 \times 10^{-7}$ \\
    \bottomrule
  \end{tabular}
\end{table}
#+end_export

Factors were chosen  based on ANOVA tests where factors  with /p/-values below a
threshold  of  $p  <  0.05$  were  considered  significant.  The  threshold  for
significance must  be adjusted to  better fit the  target problem, and  could be
dispensed with  altogether, according  to the desired  degree of  automation and
trust  in the  initial modeling  assumptions. All  experiments with  this method
presented  in this  thesis were  completely automated  using fixed  significance
thresholds.

Figure\nbsp{}[[fig:dlmt-laplacian-steps]] shows the factors  that were eliminated in
each of the 3 steps performed by DLMT, and compares each factor's predicted best
level with  the level for  that factor in the  global optimum.

We see that  the chose factor level  matches the level on the  global optimum in
all factor fixing steps, except for the  number of threads. This seems to have a
small effect in the  final results, but could be fixed in  this specific case by
dropping the inverse term, since the level  on the global optimum is the highest
one  possible. This  is an  example of  how this  transparent approach  can help
learning  about the  problem  begin optimized  by  challenging initial  modeling
assumptions.  In this case, perhaps thread group management is a larger overhead
than expected, and it is better to have larger groups.

#+latex: \clearpage

#+NAME: fig:dlmt-laplacian-steps
#+CAPTION: Overview of a single DLMT run. Each panel shows a view of the
#+CAPTION: completely evaluated search space, from the perspective of one of
#+CAPTION: the factors eliminated in a given step.
#+CAPTION: A $\color{red}\boldsymbol{\times}$ marks the predicted best level of
#+CAPTION: each factor, and a $\color{blue}\boldsymbol{\times}$ marks the factor
#+CAPTION: level on the global optimum.
#+ATTR_LATEX: :placement [t] :width .85\columnwidth
[[./img/dopt_anova_experiments/model_sample_execution.pdf]]

#+latex: \clearpage

*** Evaluation of Optimization Methods
This  section presents  a performance  evaluation of  the approach  described in
Section\nbsp{}[[A  Transparent  and  Parsimonious  Approach  to  Autotuning  using
Optimal Design]],  comparing this  approach to  eight other  optimization methods.
The  performance  model described  in  the  previous  section  was used  by  the
Iterative Linear Model (LM) and the Quantile Regression (QR) methods, as well as
by   our   approach,  named   /D-Optimal   Designs,   with  Linear   Model   and
heteroscedasticity  correction  Transform/  (DLMT).   The LM  method  is  almost
identical to our approach,  described in Chapter\nbsp{}\ref{chap:laplacian}, but
it uses  a fixed-size random  sample of the  search space instead  of generating
D-Optimal  designs. Likewise,  the QR  method used  random samples  and Quantile
Regression, in an attempt to decrease the  impact of noisy outliers in the final
fit.  Additionally, we compared the performance  of our approach with nine other
methods, namely  uniform Random Sampling  (RS), Latin Hypercube  Sampling (LHS),
Greedy Search  (GS), Greedy Search with  Restart (GSR), and a  Genetic Algorithm
(GA).   At  a  later date,  after  the  CCGRID  publication,  we added  to  this
performance  comparison an  implementation of  Gaussian Process  Regression with
Expected Improvement acquisition function (GPR).   Each method performed at most
125 measurements over 1000 repetitions, without user intervention.

Since we measured  the entire valid search  space for this kernel,  we could use
the /slowdown/  relative to the  global optimum  to compare the  performances of
each method.  Table\nbsp{}\ref{tab:gpu_laplacian_compare_budget} shows the mean,
minimum, and  maximum slowdowns  in comparison  to the  global optimum  for each
method.   It  also shows  the  mean  and maximum  budget  used  by each  method.
Figure\nbsp{}[[fig:gpu_laplacian_comparison_histogram]] presents histograms with the
count of the slowdowns  found by each of the 1000  repetitions. Arrows point the
maximum slowdown  found by each  method.  Note that  maximum slowdown of  the GS
method was left out of range to help the comparison between the other method.

All methods  performed relatively well  in this kernel,  with only GS  not being
able to find  slowdowns smaller than 4  times in some of the  runs. As expected,
other search algorithms had results similar to RS. LM was able to find slowdowns
close to the  global optimum on most runs,  but some of the runs  could not find
slowdowns smaller than 4  times. Our approach reached a slowdown  of 1% from the
global optimum in  all of the 1000 runs  while using at most fewer  than half of
the allotted budget.

#+latex: \clearpage

#+NAME: fig:gpu_laplacian_comparison_histogram
#+CAPTION: Distribution of slowdowns in relation to the global optimum for 7
#+CAPTION: optimization methods on the Laplacian Kernel, using a budget of 125
#+CAPTION: points over 1000 repetitions
#+ATTR_LATEX: :placement [h] :width .9\columnwidth
[[./img/dopt_anova_experiments/comparison_histogram.pdf]]

#+latex: \clearpage

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)
library(dplyr)

gpr_lin_data <- read.csv("dopt_anova_experiments/data/gpr_03sd_nugget_best_points.csv") %>%
    select(slowdown, method, measurement_order, time_per_pixel) %>%
    mutate(method = "GPR")

names(gpr_lin_data) <- c("slowdown", "method", "point_number", "time_per_pixel")

df_all_methods = read.csv("./dopt_anova_experiments/data/complete_1000.csv",
                          strip.white = T, header = T) %>%
    select(slowdown, method, point_number) %>%
    bind_rows(gpr_lin_data) %>%
    filter(method %in% c("RS", "LHS", "GS", "GSR", "GA", "LM",
                         "DLMT", "RQ", "GPR")) %>%
    mutate(method =  factor(method,
                            levels = c("RS", "LHS", "GS", "GSR", "GA", "LM",
                                       "RQ", "DLMT", "GPR"),
                            labels = c("Random Sampling (RS)",
                                       "Latin Hypercube Sampling (LHS)",
                                       "Greedy Search (GS)",
                                       "Greedy Search w. Restart (GSR)",
                                       "Genetic Algorithm (GA)",
                                       "Linear Model (LM)",
                                       "Quantile Regression (QR)",
                                       "D-Opt., Linear Model w. Transform (DLMT)",
                                       "Gaussian Process Regression w. EI (GPR)")))

summaries = df_all_methods %>%
    group_by(method) %>%
    summarise(method = method,
              mean_slowdown = mean(slowdown),
              min_slowdown = min(slowdown),
              max_slowdown = max(slowdown),
              mean_budget = mean(point_number),
              max_budget = max(point_number),
              .groups = "keep") %>%
    distinct() %>%
    ungroup()

cap <- paste("Slowdown and budget used by 7 optimization methods on",
             "the Laplacian Kernel, using a budget of 125 points with 1000 repetitions")

x <- xtable(summaries,
            caption = cap,
            digits = 2,
            label = "tab:gpu_laplacian_compare_budget")

align(x) <- xalign(x)
display(x) <- display(x)

header = paste(paste("Method", "Mean", "Min.", "Max.", "Mean", "Max.",
               sep = " & "), "\\\\ \n \\midrule \n")

super_header = paste("\\toprule \n",
                     "& \\multicolumn{3}{c}{Slowdown} & \\multicolumn{2}{c}{Budget}",
                     "\\\\ \n")

bottom = "\\bottomrule\n"

print(x,
      size = "\\small",
      booktabs = TRUE,
      include.rownames = FALSE,
      include.colnames = FALSE,
      hline.after = NULL,
      add.to.row = list(pos = as.list(c(-1,
                                        0,
                                        7,
                                        nrow(summaries))),
                        command = c(super_header,
                                    header,
                                    "\\rowcolor{red!25}",
                                    bottom)),
      math.style.exponents = TRUE,
      table.placement = "b",
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+begin_export latex
% latex table generated in R 4.0.4 by xtable 1.8-4 package
% Mon Mar 15 17:13:01 2021
\begin{table}[b]
\centering
\caption{Slowdown and budget used by 7 optimization methods on the Laplacian Kernel, using a budget of 125 points with 1000 repetitions}
\label{tab:gpu_laplacian_compare_budget}
\begingroup\small
\begin{tabular}{lrrrrr}
  \toprule
 & \multicolumn{3}{c}{Slowdown} & \multicolumn{2}{c}{Budget} \\
  Method & Mean & Min. & Max. & Mean & Max. \\
 \midrule
Random Sampling (RS) & 1.10 & 1.00 & 1.39 & 120.00 & 120 \\
  Latin Hypercube Sampling (LHS) & 1.17 & 1.00 & 1.52 & 98.92 & 125 \\
  Greedy Search (GS) & 6.46 & 1.00 & 124.76 & 22.17 & 106 \\
  Greedy Search w. Restart (GSR) & 1.23 & 1.00 & 3.16 & 120.00 & 120 \\
  Genetic Algorithm (GA) & 1.12 & 1.00 & 1.65 & 120.00 & 120 \\
  Linear Model (LM) & 1.02 & 1.01 & 3.77 & 119.00 & 119 \\
  Quantile Regression (QR) & 1.02 & 1.01 & 2.06 & 119.00 & 119 \\
   \rowcolor{red!25}D-Opt., Linear Model w. Transform (DLMT) & 1.01 & 1.01 & 1.01 & 54.84 &  56 \\
  Gaussian Process Regression w. EI (GPR) & 1.04 & 1.01 & 1.27 & 71.45 & 120 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+end_export

We  implemented a  simple  approach for  the prediction  step  in this  problem,
choosing the best value of our fitted  models on the complete set of valid level
combinations. This  was possible for  this problem since all  valid combinations
were known. For problems were the search  space is too large to be generated, we
would have to either adapt this step and run the prediction on a sample.

This  kernel  provided  ideal  conditions  for using  our  approach,  where  the
performance model is approximately known and  the complete valid search space is
small enough to be  used for prediction. The global optimum  also appears to not
be isolated in a  region of points with bad performance,  since our approach was
able  to exploit  search space  geometry.  In  Chapter\nbsp{}\ref{chap:spapt} we
present a more comprehensive performance evaluation  of our approach in a larger
benchmark suite.

*** Summary
:PROPERTIES:
:CUSTOM_ID: sec:laplacian-summary
:END:

This chapter  described a transparent  and parsimonious approach  to autotuning,
based on  D-Optimal Designs, ANOVA tests,  and Linear Regression. We  dealt with
the heteroscedasticity that could violate modeling assumptions by using standard
detection  tests   and  transformation   procedures.   This   approach  produced
optimizations  consistently  within  1%  of  the global  optimum  of  an  OpenCL
Laplacian kernel run in an /NVIDIA/  GPU, using half of the allotted measurement
budget for optimization.

We  demonstrated  how  the  choices  made  during  optimization  are  completely
transparent, in  such a  way that  it is straightforward  to envision  how users
could interfere  with and learn from  the optimization process by,  for example,
selecting  specific factors  that should  be treated  differently, and  changing
initial modeling assumptions based on the results produced.

The development,  testing, and tweaking of  this approach relied heavily  on the
availability of a completely evaluated  search space, with known global optimum.
Having access to this sort of search space is not possible in real applications,
as we will see  in the next two chapters, but it  is also unfortunately uncommon
in the autotuning literature.  We hope the studies presented in this thesis, and
the  one in  this chapter  in particular,  help arguing  for and  justifying the
elaboration  and  evaluation  of well-defined  and  completely-evaluated  search
spaces for autotuning,  composed of high-interest computing  kernels, that could
be  used  for  the  development  and  comparison  of  optimization  methods  for
autotuning.

** Parameters for SPAPT Kernels and Orio
:PROPERTIES:
:CUSTOM_ID: chap:spapt
:END:
This  chapter presents  an application  of Optimal  Design and  Gaussian Process
Regression to  the optimization of source-to-source  transformation kernels from
the SPAPT\nbsp{}\cite{balaprakash2012spapt} benchmark suite.

The  chapter is  organized as  follows.  Section\nbsp{}[[Choosing  an Experimental
Design Method  for Autotuning]] discusses  our choice of using  D-Optimal designs,
considering  its applicability  to  autotuning among  other Experimental  Design
methods.  Section\nbsp{}[[The  SPAPT Benchmark Suite]]  presents the CPU  kernels to
which we applied our  approach, and Section\nbsp{}[[Performance Improvements using
Optimal   Design]]   discusses   the   results.   The   work   presented   up   to
Section\nbsp{}[[Performance  Improvements using  Optimal Design]]  was published  at
CCGrid\nbsp{}\cite{bruel2019autotuning} in 2019.  The studies we performed later
were   not    yet   published   at    the   writing   of   this    thesis.    In
Section\nbsp{}[[Identifying Significant  Factors for the /bicg/  Kernel]] we explore
the  optimization of  one of  the target  kernels and  identify the  factors and
levels that  where responsible  for the  observed performance  improvements.  In
Section\nbsp{}[[Autotuning  SPAPT kernels  with  Gaussian  Process Regression  and
Expected Improvement]] we  discuss the application of  Gaussian Process Regression
with     Expected    Improvement     to    the     same    kernel.      Finally,
Section\nbsp{}\ref{sec:dlmt-summary} summarizes the chapter and discusses future
work.

*** Choosing an Experimental Design Method for Autotuning
Our application of the ED methodology  requires support for factors of different
types and  numbers of levels, such  as binary flags, integer  and floating point
numerical values and  enumerations of categorical values.  We  also need designs
that minimize the number of experiments needed for identifying the most relevant
factors of a  problem, since at this  moment we are not interested  in a precise
analytical   model.   The   design  construction   techniques  that   fit  these
requirements  are limited.  The first  ED  approach we  studied was  /screening/
which, despite being extremely parsimonious, does not have enough flexibility to
explore the search spaces which we wished to study.

Despite  being   parsimonious,  screening  designs  are   extremely  restrictive
Algorithms for constructing Optimal Designs can adapt to our requirements, being
able  to mix  factors  of different  types and  optimizing  a starting  designs.
Before settling  on D-Optimal designs, which  we use in this  study, we explored
other    design    construction    techniques     such    as    extensions    to
Plackett-Burman\nbsp{}\cite{plackett1946design} designs  that use  random factor
levels,    the     /contractive    replacement/    technique     presented    by
Addelman-Kempthorne\nbsp{}\cite{addelman1961some},  and the  /direct generation/
algorithm  by  Grömping   and  Fontana\nbsp{}\cite{ulrike2018algorithm}.   These
techniques have strong requirements on design size and level mixing, so we opted
for a more  flexible technique that would enable exploring  a more comprehensive
class of autotuning problems.

The modified screening method provides a  strategy to use factors with more than
two  levels by,  instead of  encoding fixed  high and  low levels,  sampling two
levels uniformly, each time the factor  is included in a Plackett-Burman design.
This approach  has the advantage  of a small design  size with good  main effect
estimation capability, but  it is still not capable  of estimating interactions.

The  /contractive replacement/  method starts  with a  large 2-level  design and
generates mixed-level designs by re-encoding and replacing pairs of columns with
a  new column  for a  multi-level  factor.  The  contractive replacement  method
presented by Addelman-Kempthorne\nbsp{}\cite{addelman1961some}  is a strategy of
this kind.   In addition to  small design  size and good  estimation capability,
their method maintains the orthogonality of starting designs, although it places
strong  requirements  on initial  designs,  such  that only  orthogonal  2-level
matrices can be contracted with their method.

The    /direct    generation/    algorithm   introduced    by    Grömping    and
Fontana\nbsp\cite{ulrike2018algorithm}   enables   the  direct   generation   of
multi-level     designs    that     minimize     the    /Generalized     Minimum
Aberration/\nbsp{}\cite{deng1999generalized}  optimality  criterion  by  solving
mixed  integer  problems.   We  did  not  pursue  this  method  because  of  the
limitations it  imposes on the  size and  the shape of  the designs that  can be
generated, and also because it relied on proprietary MIP solvers that we did not
have access to.

Weighting flexibility,  effectiveness, parsimony, and the  cost and availability
of algorithmic  construction, we  picked /D-Optimal  Designs/ among  the methods
that  fulfilled the  requirements for  application to  autotuning problems.   In
particular,            we             used            the            KL-exchange
algorithm\nbsp{}\cite{atkinson1989construction},     which    enables     mixing
categorical and  numerical factors  in the same  design, while  biasing sampling
according  to the  performance  model  we wish  to  explore.   This enables  the
exploitation of global  search space structures, if we use  the right model.  We
can  safely optimize  for  the  /D/ optimality  criterion  among other  criteria
without loosing quality of designs\nbsp{}\cite{kiefer1974general}.

*** The SPAPT Benchmark Suite
The    /Search   Problems    in    Automatic    Performance   Tuning/    (SPAPT)
\nbsp{}\cite{balaprakash2012spapt}  benchmark  suite provides  parametrized  CPU
kernels    from    different    HPC    domains.    The    kernels    shown    in
Table\nbsp{}[[tab:spapt_apps]]  are  implemented  using   the  code  annotation  and
transformation   tools   provided  by   Orio\nbsp{}\cite{hartono2009annotation}.
Search space  sizes are  larger than  in the  Laplacian Kernel  example.  Kernel
factors are either integers in an  interval, such as loop unrolling and register
tiling amounts, or binary flags that control parallelization and vectorization.

We used the Random Sampling (RS) implementation available in Orio and integrated
an implementation  of our approach  (DLMT) to the  system. We omitted  the other
Orio      algorithms      because       other      studies      using      SPAPT
kernels\nbsp{}\cite{balaprakash2011can,balaprakash2012experimental}  showed that
their performance is similar to RS regarding budget usage. The global minima are
not known  for any of  the problems,  and search spaces  are too large  to allow
complete measurements.

#+ATTR_LATEX: :booktabs t :align llll :font \scriptsize :float t :placement [t]
#+NAME: tab:spapt_apps
#+CAPTION: Kernels from the SPAPT benchmark used in this evaluation
|---------------+---------------------------------+---------+----------------------|
| Kernel        | Operation                       | Factors | Size                 |
|---------------+---------------------------------+---------+----------------------|
| =atax=        | Matrix transp. & vector mult.   |      18 | $2.6 \times 10^{16}$ |
| =dgemv3=      | Scalar, vector & matrix mult.   |      49 | $3.8 \times 10^{36}$ |
| =gemver=      | Vector mult. & matrix add.      |      24 | $2.6 \times 10^{22}$ |
| =gesummv=     | Scalar, vector, & matrix mult.  |      11 | $5.3 \times 10^{9}$  |
| =hessian=     | Hessian computation             |       9 | $3.7 \times 10^{7}$  |
| =mm=          | Matrix multiplication           |      13 | $1.2 \times 10^{12}$ |
| =mvt=         | Matrix vector product & transp. |      12 | $1.1 \times 10^{9}$  |
| =tensor=      | Tensor matrix mult.             |      20 | $1.2 \times 10^{19}$ |
| =trmm=        | Triangular matrix operations    |      25 | $3.7 \times 10^{23}$ |
| =bicg=        | Subkernel of BiCGStab           |      13 | $3.2 \times 10^{11}$ |
| =lu=          | LU decomposition                |      14 | $9.6 \times 10^{12}$ |
| =adi=         | Matrix sub., mult., & div.      |      20 | $6.0 \times 10^{15}$ |
| =jacobi=      | 1-D Jacobi computation          |      11 | $5.3 \times 10^{9}$  |
| =seidel=      | Matrix factorization            |      15 | $1.3 \times 10^{14}$ |
| =stencil3d=   | 3-D stencil computation         |      29 | $9.7 \times 10^{27}$ |
| =correlation= | Correlation computation         |      21 | $4.5 \times 10^{17}$ |
|---------------+---------------------------------+---------+----------------------|

*** Performance Improvements using Optimal Design
We used the performance of each SPAPT kernel compiled with the /gcc/ /-O3/ flag,
with  no code  transformations, as  a  /baseline/ for  computing the  /speedups/
achieved  by each  strategy.  We  performed 10  autotuning repetitions  for each
kernel using RS and DLMT, using a budget of /at most/ 400 measurements. DLMT was
allowed    to    perform    only    4     of    the    iterations    shown    in
Figure\nbsp{}[[fig:doe_anova_strategy]].     Experiments   were    performed   using
Grid5000\nbsp{}\cite{balouek2013adding},  on /Debian  Jessie/,  using an  /Intel
Xeon E5-2630v3/ CPU and /gcc/ version /6.3.0/.

The time to measure  each kernel varied from a few seconds to  up to 20 minutes.
In  testing, some  transformations  caused  the compiler  to  enter an  internal
optimization process that did  not stop for over 12 hours. We  did not study why
these cases  delayed for  so long,  and implemented an  execution timeout  of 20
minutes, considering cases  that took longer than that to  compile to be runtime
failures.

We automated  factor elimination based  on ANOVA  tests so that  a comprehensive
evaluation  could be  performed.  We  also did  not  tailor initial  performance
models, which were  the same for all  kernels. Initial models had  a linear term
for each  factor with  two or more  levels, plus quadratic  and cubic  terms for
factors with sufficient levels. Although automation and identical initial models
might have limited the improvements at each step of our application, our results
show that it still succeeded in decreasing the budget needed to find significant
speedups for some kernels.

Figure\nbsp{}[[fig:iteration_best_comparison]] presents the  /speedup/ found by each
run of RS and DLMT, plotted against the algorithm /iteration/ where that speedup
was found. We  divided the kernels into  3 groups according to  the results. The
group where  no algorithm found  any speedups contains  3 kernels and  is marked
with ``[0]'' and /blue/ headers.  The  group where both algorithms found similar
speedups, in similar  iterations, contains 6 kernels and is  marked with ``[=]''
and  /orange/ headers.   The group  where DLMT  found similar  speedups using  a
significantly  smaller budget  than RS  contains 8  kernels and  is marked  with
``[+]'' and /green/  headers.  Ellipses delimit an estimate of  where 95% of the
underlying distribution  lies, and a dashed  line marks the /-03/  baseline.  In
comparison to  RS, our  approach significantly decreased  the average  number of
iterations needed to find speedups for the 8 kernels in the green group.

#+NAME: fig:iteration_best_comparison
#+CAPTION: Cost of best points found on each run, and the iteration where
#+CAPTION: they were found. RS and DLMT found no speedups with similar budgets
#+CAPTION: for kernels marked with ``[0]'' and /blue/ headers, and similar speedups with
#+CAPTION: similar budgets for kernels marked with ``[=]'' and /orange/ headers. DLMT found similar
#+CAPTION: speedups using smaller budgets for kernels marked with ``[+]'' /green/ headers.
#+CAPTION: Ellipses delimit an estimate of where 95% of the underlying distribution lies
#+ATTR_LATEX: :placement [t] :width \textwidth
[[./img/ccgrid19/iteration_best_comparison.pdf]]

#+NAME: fig:split_histograms
#+CAPTION: Histograms of explored search spaces, showing the real count of measured configurations.
#+CAPTION: Kernels are grouped in the same way as in Figure\nbsp{}[[fig:iteration_best_comparison]].
#+CAPTION: DLMT spent fewer measurements than RS in configurations with smaller speedups or with slowdowns,
#+CAPTION: even for kernels in the orange group. DLMT also spent more time exploring configurations with
#+CAPTION: larger speedups
#+ATTR_LATEX: :placement [t] :width \textwidth
[[./img/ccgrid19/split_histograms.pdf]]

Figure\nbsp{}[[fig:split_histograms]] shows  the search space  exploration performed
by    RS    and    DLMT.     It     uses    the    same    color    groups    as
Figure\nbsp{}[[fig:iteration_best_comparison]],  and shows  the distribution  of the
speedups that were  found during all repetitions of  the experiments.  Histogram
areas corresponding to  DLMT are usually smaller because it  always stopped at 4
iterations, while  RS always  performed 400  measurements. This  is particularly
visible in  /lu/, /mvt/,  and /jacobi/.   We also observe  that the  quantity of
configurations with high  speedups found by DLMT is higher,  even for kernels on
the  orange group.   This  is  noticeable in  /gemver/,  /bicgkernel/, /mm/  and
/tensor/, and means that DLMT spent  less of the budget exploring configurations
with small speedups or slowdowns, in comparison with RS.

Analyzing  the significant  performance parameters  identified by  our automated
approach for  every kernel, we  were able to identify  interesting relationships
between  parameters   and  performance.  In  /bicgkernel/,   for  example,  DLTM
identified   a   linear  relationship   for   OpenMP   and  scalar   replacement
optimizations, and  quadratic relationships  between register and  cache tiling,
and loop unrolling.  This is an example of the  transparency in the optimization
process that can be achieved with an ED approach.

Our approach used a generic initial performance model for all kernels, but since
it iteratively eliminates  factors and model terms based on  ANOVA tests, it was
still able to  exploit global search space structures for  kernels in the orange
and green groups. Even in this automated setting, the results with SPAPT kernels
illustrate the ability our approach has to reduce the budget needed to find good
speedups by efficiently exploring search spaces.

*** Identifying Significant Factors for the /bicg/ Kernel
The discussion on  the following sections was  not yet published at  the time of
writing of this  thesis.  We continued to study the  application of Experimental
Design methods to the optimization of SPAPT kernels, modifying the original DLMT
algorithm  by removing  cubic terms  to simplify  the target  performance model,
fixing binary parameters to look  for other significant effects, leveraging data
collected in  previous designs, running  more optimization steps,  and assessing
the performance  of quantile regression as  an alternative to the  linear model.
In  this study  we  sought to  expose  and  analyze the  inner  workings of  our
approach.

In a non-automated setting, it becomes  more clear how users could interfere and
guide search  space restriction  and exploration, by  intervening at  each ANOVA
step, for example.  We started with the /bicg/ SPAPT kernel because our approach
achieved  equivalent solutions  using a  smaller budget  than a  random sampling
approach.

Figure\nbsp{}[[fig:best-all]]  shows   the  execution  times  of   the  best  kernel
configurations, and the corresponding iterations  where each of these points was
found. Results are  shown for Random Sampling, for our  DLMT implementation from
the CCGRID paper, and for four  variations that we later explored, attempting to
interpret and improve  the optimization process.  The /y/-axis on  the top panel
shows the  execution time  of the  best points  found in  10 distinct  runs, and
lighter colors mark points found  with fewer iterations. Similarly, the /y/-axis
on the bottom  panel shows the iterations  where points were found  for the same
data, with lighter colors marking points with lower execution times.

#+CAPTION: Summary of our DLMT results, compared to uniform Random Sampling (RS),
#+CAPTION: showing the configurations with smallest speedups, found in 10 independent
#+CAPTION: runs. The top panel compares execution time, with color-encoded iterations,
#+CAPTION: and the bottom panel compares iterations where configurations were found,
#+CAPTION: with color-encoded execution times.
#+LABEL: fig:best-all
#+ATTR_LATEX: :width 0.7\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/iterations_all.pdf]]

The next sections  discuss incremental modifications to  the algorithm published
at CCGRID, which were performed to help identify significant factors and attempt
to find better speedups. Although we  did not significantly improve the speedups
found  by our  method, we  were able  to present  a deeper  look into  the inner
workings of  this white-box autotuning  approach.  The next section  describe in
detail the two incremental modifications below:

\paragraph{Removing Cubic Terms} We  initially used third-degree polynomials for
the performance models  of all kernels.  Cubic terms were  never eliminated when
following significance  levels reported by  ANOVA, so  we decided to  drop cubic
terms and  fit a quadratic polynomial,  which reduced the necessary  design size
and produced equivalent results.

\paragraph{Reusing Data}  In our  initial approach, we  performed ANOVA  in each
iteration  using  only the  data  from  the  subspaces  to which  we  restricted
exploration.  This can increase flexibility because  the models are free to vary
outside the restricted regions, but this also wastes experimental data.  In this
modification  we   reused  all   measurements  performed   during  optimization,
leveraging all  measurements regardless of  their position in the  search space,
which increases the benefits of multiple runs.

The two other modifications shown in Figure\nbsp{}[[fig:best-all]] are:

\paragraph{Quantile Regression} We  identified a substantial amount  of noise on
the  performance measurements  of non-binary  parameters. Models  accounting for
factor interactions could  help, since there is a good  chance that interactions
are  responsible for  the  noise. In  this  study, we  decided  to try  Quantile
Regression, which is a modification of Linear Regression where it is possible to
weight points based on  the quantiles they belong to, allowing  the model to fit
to a different quantile, opposition to fitting to the mean. We explored multiple
quantiles, and Figure\nbsp{}[[fig:best-all]] shows results using the 5% quantile.

\paragraph{Running  more Steps}  This is  less of  a modification,  and involved
simply running the original approach for twice the number of steps.

**** Removing Cubic Terms
The  data  in the  figures  shown  next  were obtained  with  a  new set  of  10
repetitions      of       the      /bicg/      kernel       experiment      from
Figures\nbsp{}\ref{fig:iteration_best_comparison}                            and
\ref{fig:split_histograms}, but using  a performance model with  only linear and
quadratic terms.  The results are similar to the ones from the CCGRID paper, but
our  approach  found slightly  better  configurations  slightly faster.   Random
sampling also found  a much better configuration much faster  than before in one
experiment.   Figure\nbsp{}[[fig:eliminated-terms-no-cubic]]  shows  the  /count  of
model terms/ that were eliminated at  each color-coded DLMT step.  As before, we
see that  /OMP/ and /SCR/  were the most  eliminated factors, especially  on the
first step.

#+CAPTION: Count of the model terms eliminated in each of the 4 steps of 10
#+CAPTION: independent runs, without  cubic terms
#+LABEL: fig:eliminated-terms-no-cubic
#+ATTR_LATEX: :width 0.6\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/eliminated_terms_no_cubic.pdf]]

Figure\nbsp{}[[fig:explored-space-no-cubic]] shows the  factors that were identified
as significant  by ANOVA, with  significance threshold  of $0.01$, at  each DLMT
step.  Identifying a factor removes  the corresponding parameter from the model,
regardless of the  model term that was identified.  Figures  are grouped by each
of the 10 experiments.  Figure headers  identify each run, and correspond to the
names of the Grid5000 machines where experiments were run.

It  is  interesting that  only  /parasilo-19/,  /parasilo-20/, and  /parasilo-9/
eliminated any  factor other than  /OMP/ and /SCR/ on  the first step  where any
factor was eliminated, and  also that those runs fixed the  factor /RT_I/ to the
same value.   We can also see  that /OMP/ seems  to be the parameter  behind the
most extreme  changes in the execution  time of tested configurations.   This is
specially  clear   at  /parasilo-13/  and  /parasilo-10/,   where  the  explored
configurations have  relatively high  execution time until  after step  3, where
/OMP/ is  fixed.  A slight worsening  of execution times, an  increase, that is,
can be  seen at the  fourth step at /parasilo-11/,  after /RT_I/ was  fixed. The
minimum execution time seems to be higher than in the third step.

#+CAPTION: Measured execution time of all points in the designs constructed at
#+CAPTION: each step, where panel headers mark the hostname of the machines
#+CAPTION: used in each of the 10 separate experiments
#+LABEL: fig:explored-space-no-cubic
#+ATTR_LATEX: :width 0.9\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/explored_space_no_cubic.pdf]]

#+CAPTION: Fitting linear models and quadratic quantile regression,
#+CAPTION: with $\tau = 0.05$, $0.25$, and $0.5$,
#+CAPTION: to separate factors for 10 uniform random sampling runs
#+LABEL: fig:models-no-cubic
#+ATTR_LATEX: :width 0.9\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/models_no_cubic.pdf]]

#+CAPTION: Same as above, for 10 uniform sampling runs with
#+CAPTION: \textit{OMP} fixed to \textit{on}
#+LABEL: fig:models-fixed-omp-no-cubic
#+ATTR_LATEX: :width 0.9\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/models_fixed_omp_no_cubic.pdf]]

#+latex: \clearpage

Figures\nbsp{}[[fig:models-no-cubic]] and  [[fig:models-fixed-omp-no-cubic]] contain all
points  measured in  10  uniform  random sampling  runs.   Each  panel the  same
execution time data in the /y/-axis,  viewed from the perspective of the changes
in a  single factor,  shown in  the /x/-axis.   The blue  lines in  both figures
represents  a  linear  model  fit,  using only  linear  terms  for  each  factor
separately.   The  green  lines  denote three  quantile  regression  fits  using
quadratic model terms for  each factor separately. We used the  5%, 25%, and 50%
quantiles. Using the  50% quantile is equivalent to linear  regression. Red dots
mark the values of each factor in the point with the best performance across the
10 runs.

The  /OMP/  parameter controls  whether  OpenMP  is  used to  run  multithreaded
execution, and  has clearly the  largest impact when  its level is  $1.0$, which
encodes  the binary  level  corresponding to  turning  parallelization on.   All
measurements larger  than $1.4$ seconds  happened when the  parallelization flag
was  off, as  can be  seen on  Figure\nbsp{}[[fig:models-fixed-omp-no-cubic]], which
shows  all data  collected in  10 uniform  random sampling  runs with  the /OMP/
factor  fixed  on  the  /on/  level.  The /SCR/  parameter  also  has  a  strong
effect. This parameter  controls /scalar replacement/, which  consists of moving
array references to the stack, preferably into registers.

Fixing /OMP/  to /on/  makes the  effects of other  factors more  pronounced and
easier to detect  using ANOVA.  Most of  the best factor levels  in this example
were near the limits  of the search space. The /RT_I/  factor controls tiling of
one of the loops  in /bicg/, and its best level was  exceptionally at the middle
of the range.

We  can  also  verify  that  the optimal  design  approach  is  restricting  the
exploration  of  the  search  space   to  regions  with  faster  configurations.
Figure\nbsp{}[[fig:predictions-no-cubic]] compares the  performance predicted by the
fitted model,  represented by the green  line, with the performance  of the best
point found at each design, represented by the blue line. The red line marks the
best point found so far.

#+CAPTION: Performance of best points, step-by-step, by experiment
#+LABEL: fig:predictions-no-cubic
#+ATTR_LATEX: :width 0.99\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/predictions_no_cubic.pdf]]

It is  only at the  fourth step  of /parasilo-17/ that  the point with  the best
predicted performance  was better  than the  best point on  the design  for that
step, while also being better than the best point found so far. Although we seem
to be  effectively restricting the search  space with our exploration,  which is
evidenced  by the  improvement that  occurs as  steps progress  and by  the best
points being found inside designs, the  models fit using experiment data are not
able to improve the current best on the majority of cases. This is an indication
that the  model is not capable  of describing the relationships  between factors
and  response,  perhaps   because  of  the  presence   of  interactions  between
parameters.

**** Reusing Data from All Steps
Significant  changes were  performed  on the  initial  DLMT implementation.   We
decided  that there  was no  good reason  to not  reuse the  data obtained  from
evaluating designs  at each step,  and the various  samples of the  search space
taken  at different  points. Now,  all  evaluated experiments  compose a  single
growing design used by /aov/ to identify  the best factors, and all samples from
the  search space  compose a  single  data set  used by  /optFederov/ to  select
experiments.   The data  set  is pruned  in  both /aov/  and  /lm/ analyses,  to
guarantee only  experiments with the correct  levels of fixed factors  are used.
This  is crucial  for  both analyses,  since having  different  fixed values  of
factors that are  not in the model  would imply that we  would have inexplicable
variance in the data set.

Using all experimental  data on /aov/ is interesting because  it is always worth
it to  consider additional information on  the factors that are  currently being
studied.   On one  hand,  it might  not  allow for  enough  flexibility when  we
consider regression only on a  small restricted subspace, because points outside
the  subspace  would impact  regression,  and  we  would  be interested  in  the
significance of the factor inside the subspace at the moment. On the other hand,
using all data available makes sense because we are also interested in exploring
any  global structures  of  the  search space,  and  keeping  points from  other
subspaces would increase the chance of ``catching'' the significance of a factor
globally.

Using all sampled  space, across all steps, as a  candidate for /optFederov/ has
no  downsides,  provided we  prune  the  search  space  to account  for  current
constraints on factor  levels fixed on previous steps.  We  increase the size of
the available set of  configurations that can compose a new  design each time we
sample  a new  subspace. This  would hopefully  improve the  quality of  designs
produced as we progress.

***** Extra Figures                                            :noexport:
#+CAPTION: Eliminated terms
#+LABEL: fig:eliminated-terms-reuse
#+ATTR_LATEX: :width 0.7\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/eliminated_terms_reuse.pdf]]

#+CAPTION: Execution time of Searched Space, step-by-step, by experiment
#+LABEL: fig:explored-space-reuse
#+ATTR_LATEX: :width 0.9\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/explored_space_reuse.pdf]]

#+CAPTION: Fitting \texttt{lm} linear and cubic models to separate factors for the RS runs
#+LABEL: fig:models-reuse
#+ATTR_LATEX: :width 0.9\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/models_reuse.pdf]]

#+CAPTION: Fitting \texttt{lm} linear and cubic models to separate factors for the RS runs, with \texttt{OMP} fixed to \texttt{true}
#+LABEL: fig:models-fixed-omp-reuse
#+ATTR_LATEX: :width 0.9\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/models_fixed_omp_reuse.pdf]]

#+CAPTION: Performance of best points, step-by-step, by experiment
#+LABEL: fig:predictions-reuse
#+ATTR_LATEX: :width 0.7\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/predictions_reuse.pdf]]


**** Summary: Tuning SPAPT Kernels with Optimal Design
We could not significantly improve upon the speedups found by the uniform random
sampling  strategy, although  we did  find good  speedups using  a significantly
smaller budget.  This  happens mainly because the DLMT  approach quickly detects
the large effect  of the /OMP/ binary parameter. Subsequent  parameters that are
detected  as  significant  have  much   smaller  effects,  so  consequently  the
performance does  not improve by  much.

We  can conclude  that  the  SPAPT search  space  structures  defined by  kernel
parameters exposed  and modified  by Orio  cannot be  fully exploited  by models
based  on  second   or  third  degree  polynomials,  even   with  well  designed
experiments.  We could  not determine whether kernel  configurations with better
performance than the  ones we found exist in these  search spaces. Additionally,
the strongly biased  models we used could be responsible  for not finding better
configurations. We  attempted to  find better configurations  using a  much more
flexible   approach,  based   on  Gaussian   Process  Regression   and  Expected
Improvement, which  we discuss  next. We  also compared  the performance  of the
/dgemv/ SPAPT kernel with the peak theoretical performance for the /Xeon 2630v3/
processor, indicating that better kernel configurations might exist.

*** Autotuning SPAPT kernels with Gaussian Process Regression and Expected Improvement
We used  Gaussian Process  Regression with  an Expected  Improvement acquisition
function, denoted  GPR, to try  and find /bicg/ kernel  configurations improving
upon  those  found  by  the  uniform  Random  Sampling  (RS)  method.  We  found
statistically but not practically significant improvements upon RS.

#+CAPTION: Best /bicg/ configurations and iterations that found them, for Gaussian Process
#+CAPTION: Regression with Expected Improvement acquisition function (GPR), and uniform
#+CAPTION: Random Sampling (RS), with a budget of 400 measurements.
#+LABEL: fig:gp-rs-comparison
#+ATTR_LATEX: :width 0.65\textwidth :placement [h]
#+RESULTS:
[[file:./img/journal/gp_rs_comparison.pdf]]

We     used      two     /R/      packages     to     construct      our     GPR
algorithm\nbsp{}\cite{roustant2012dicekriging}.    We  used   the  /DiceKriging/
package to fit  Gaussian Processes using /Matérn/ covariance  functions, and the
/DiceOptim/ package  to compute  the Expected Improvement  metric at  each step.
Figure\nbsp{}[[fig:gp-rs-comparison]] compares  the performance  of the  best points
found by GPR  and RS, across 20  independent tuning runs. The  figure also shows
the  iteration finding  the  best point  at  each run.   Dashed  lines mark  the
estimate  of  the  mean  execution  time  of  the  best  points  across  the  20
experiments, and the  gray bands show the 95% confidence  intervals for the mean
estimate.

The GPR  method found statistically  significant better configurations  than RS,
but the practical difference is less  than $0.1$ seconds. Since Gaussian Process
Regression is a much more flexible  method than our previous Experimental Design
approach, not finding expressively better configurations hints that it might not
be possible to improve this kernel much more, in comparison to RS.

Figures\nbsp{}[[fig:bicgkernel-gp-pareto]]  and  [[fig:bicgkernel-rs-pareto]]  show  the
progression of  all configurations evaluated during  each of the 20  runs of GPR
and RS,  respectively.  We can  see that after an  initial Sobol sample  the GPR
method  quickly finds  regions with  better  configurations. The  RS method,  as
expected, continues to randomly explore the search space.

#+latex: \clearpage

#+CAPTION: Execution time of points measured by GPR along iterations, with pareto border in red
#+LABEL: fig:bicgkernel-gp-pareto
#+ATTR_LATEX: :width 0.85\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/bicgkernel_gaussian_pareto.pdf]]

#+CAPTION: Execution time of points measured by RS along iterations
#+LABEL: fig:bicgkernel-rs-pareto
#+ATTR_LATEX: :width 0.85\textwidth :placement [b]
#+RESULTS:
[[file:./img/journal/bicgkernel_rs_pareto.pdf]]

#+latex: \clearpage

**** Peak Performance for the /DGEMV/ kernel
#+CAPTION: Best /dgemv3/ configurations and iterations that found them, for Gaussian Process
#+CAPTION: Regression with Expected Improvement acquisition function (GPR), and uniform
#+CAPTION: Random Sampling (RS), with a budget of 400 measurements.
#+LABEL: fig:gp-rs-dgemv-comparison
#+ATTR_LATEX: :width 0.65\textwidth :placement [t]
#+RESULTS:
[[file:./img/journal/gp_rs_dgemv_comparison.pdf]]

We also performed GPR experiments with the /dgemv3/ SPAPT kernel which, since we
can  compute its  theoretical  performance, allowed  determining  that the  best
kernel configuration  we have  found is  still around 20  times slower  than the
theoretical peak, despite being around two times faster than the /-O3/ flag.  We
did not perform  a detailed analysis of  the SPAPT kernel, which  could find out
whether more performance can be obtained from this kernel implementation.

#+CAPTION: Empirical Roofline graph for the /Xeon E5-2630v3/
#+CAPTION: processor\nbsp{}\cite{intel2021xeonAPP,intel2021xeon},
#+CAPTION: showing the best point we found during all experiments with the memory-bound /dgemv3/
#+CAPTION: SPAPT kernel
#+LABEL: fig:xeon-generic-roofline-dgemv
#+ATTR_LATEX: :width 0.7\textwidth :placement [h]
#+RESULTS:
[[file:./img/roofline/roofline_generic.pdf]]

#+CAPTION: Empirical Roofline graph for the /Xeon E5-2630v3/
#+CAPTION: processor\nbsp{}\cite{intel2021xeonAPP,intel2021xeon},
#+CAPTION: showing the best point we found during all experiments with the memory-bound /dgemv3/
#+CAPTION: SPAPT kernel
#+LABEL: fig:xeon-native-roofline-dgemv
#+ATTR_LATEX: :width 0.7\textwidth :placement [h]
#+RESULTS:
[[file:./img/roofline/roofline_native.pdf]]

Figure\nbsp{}[[fig:gp-rs-dgemv-comparison]]  shows  the best  kernel  configurations
found  across 20  repetitions of  the GPR  and RS  methods. GPR  could not  find
statistically or  practically significant  speedups in relation  to RS  for this
kernel,  although  one   specific  GPR  run  found   an  outlier  configuration.
Figures\nbsp{}[[fig:xeon-generic-roofline-dgemv]] and [[fig:xeon-native-roofline-dgemv]]
show  the empirical  Roofline\nbsp{}\cite{williams2009roofline}  graphs for  the
processor targeted by our experiments, which was the same from our CCGRID paper,
using  /GCC/  /-march=generic,native/  options.    The  figure  also  shows  the
performance  that was  achieved  by  the best  configuration  we  found for  the
/dgemv3/        SPAPT       kernel,        the       red        outlier       in
Figure\nbsp{}[[fig:gp-rs-dgemv-comparison]].

***** Theoretical Roofline                                     :noexport:
#+CAPTION: Empirical Roofline graph for the /Xeon E5-2630v3/
#+CAPTION: processor\nbsp{}\cite{intel2021xeonAPP,intel2021xeon},
#+CAPTION: showing the best point we found during all experiments with the memory-bound /dgemv3/
#+CAPTION: SPAPT kernel
#+LABEL: fig:xeon-generic-roofline-dgemv
#+ATTR_LATEX: :width 0.65\textwidth :placement [h]
#+RESULTS:
[[file:./img/journal/theoretical_roofline_xeonE52630v3.pdf]]


*** Summary
:PROPERTIES:
:CUSTOM_ID: sec:dlmt-summary
:END:

We have implemented an Experimental Design approach to autotuning based on ANOVA
and D-Optimal  Designs. We have applied  this approach to a  set of configurable
kernels  from  the  SPAPT  benchmark  suite and,  although  our  approach  found
significant  speedups in  relation  an  /-O3/ compiler  baseline,  we could  not
significantly  improve upon  the results  found  by an  uniform Random  Sampling
algorithm using  the same  exploratory budget. Other  experiments with  the same
kernels         also         showed          than         uniform         random
sampling\nbsp{}\cite{balaprakash2011can,balaprakash2012experimental}    performs
well, especially when budgets are short.

We could identify and eliminate significant  factors using our approach, but the
largest  effect was  consistently that  of the  binary flags,  especially /OMP/,
which  controlled parallelization.   We  could  consistently detect  significant
albeit small effects for a few other numerical parameters.

The  polynomial  models  coupled  with  optimal  design  construction  have  the
potential  for low  prediction variance,  but tend  to have  strong biases.   We
explored  a Gaussian  Process Regression  approach which  is much  more complex,
implying in high variance, but has  small bias. We found small improvements upon
Random  Sampling  with this  new  approach,  but it  is  still  unclear if  Orio
configurations are  capable of  further improving  the kernel  configurations we
already found. Analyzing the Roofline theoretical model for our target CPU hints
that tweaks to the kernel implementation  might be needed to improve the current
performance.

A possible  future direction  for exploring the  potential of  this Experimental
Design  approach could  involve developing  a  lower level  set of  configurable
kernels, where performance measurements are always tied to a theoretical peak or
to     a      known     global     optimum.      As      we     discussed     in
Chapter\nbsp{}\ref{chap:laplacian}, knowing the  global optimum helps evaluating
the performance of an autotuning approach.

** Mixed-Precision Quantization for /ResNet50/
:PROPERTIES:
:CUSTOM_ID: chap:quantization
:END:

This  chapter  presents  an  application of  Gaussian  Process  Regression  with
Expected Improvement  acquisition function (GPR)  to bit precision tuning  for a
Convolutional Neural Network (CNN).  We  compare GPR to a Reinforcement Learning
approach which  uses an  actor-critic model (RL)\nbsp{}\cite{wang2019haq},  to a
uniform random sampling algorithm, and  to a low-discrepancy sampler using Sobol
sequences.   The objective  function we  attempted to  optimize was  the network
accuracy                   over                   the                   ImageNet
dataset\nbsp{}\cite{deng2009imagenet,russakovsky2015imagenet},   subject  to   a
limit imposed on  network size. Our GPR approach achieved  comparable results to
the  baseline  RL  approach,  but  the  random  samplers  also  performed  well,
especially the low-discrepancy Sobol sequence.

This work started during a two-month stay at /Hewlett Packard Enterprise/ (HPE),
former  /HP   Labs/,  in   Palo  Alto,  California.    The  problem   of  tuning
mixed-precision quantization for CNNs is  of interest to HPE researchers because
it produces  smaller and  more energy-efficient  networks, which  are especially
suited to embedded and custom-built hardware.   We expanded the initial study in
the last year  of this thesis, still in collaboration  with HPE researchers, but
this work is still unpublished at the time of writing.

The    remainder     of    this    chapter    is     organized    as    follows.
Section\nbsp{}[[Autotuning  Bit Precision  for  Convolutional Neural  Networks]]
describes the  /HAQ/ framework\nbsp{}\cite{wang2019haq} for  mixed-precision bit
quantization, which we used to change  the bitwidth of network layers, and which
implements the  baseline RL  approach.  Section\nbsp{}[[ResNet50  and ImageNet]]
presents  the  /ResNet50/ network  used  in  our  experiments and  the  ImageNet
dataset.   Section\nbsp{}[[Optimization Methods]]  discusses the  implementation
details of  the methods we compared,  and Section\nbsp{}[[Performance Evaluation
and Comparison]] evaluates  their performance with respect  to network accuracy.
Section\nbsp{}[[Measuring  Performance  and  Detecting  Significance  for  GPR]]
presents  a  more  detailed  assessment   of  the  GPR  method  we  implemented.
Section\nbsp{}[[Summary]] summarizes the discussion and concludes the chapter.

*** Autotuning Bit Precision for Convolutional Neural Networks
A Neural Network is composed of interconnected layers of "neurons".  Each neuron
can be interpreted as an agent  that receives /weighted/ inputs from connections
with other neurons, combines them, and  produces an output if the combined input
crosses an /activation/  threshold.  The process of /quantization/  of a network
consists in choosing, according to a  performance metric, the best bit precision
for   data   representing   weights    and   activation   thresholds   in   each
layer. Quantizing  a network  consequently determines the  bit precision  of the
computations  performed   during  training   and  inference.    Quantization  is
increasingly           relevant            in           Neural           Network
research\nbsp{}\cite{wang2019haq,elthakeb2019releq,zhu2016trained,han2015deep,mishra2017wrpn,zhou2016dorefa}
because  it  reduces  energy  and space  requirements,  enabling  deployment  in
embedded, mobile, or custom-engineered hardware.

#+NAME: fig:quantization
#+CAPTION: Optimizing nonuniform weight quantization policies for the
#+CAPTION: \textit{ResNet50} network, keeping total weight size
#+CAPTION: below 10 /MB/, and attempting to keep accuracy.
#+CAPTION: The optimization methods we compared were a Sobol Sampler,
#+CAPTION: Reinforcement Learning\nbsp{}\cite{wang2019haq}, and
#+CAPTION: Gaussian Process Regression with Expected Improvement
#+ATTR_LATEX: :placement [t] :width .99\textwidth
[[./img/neural_net_autotuning/quantization_autotuning.pdf]]

Figure\nbsp{}[[fig:quantization]]  shows the  bit  precision  tuning experiments  we
present in  this chapter.   We used different  methods to  optimize quantization
policies for  the layer  groups of  the /PyTorch/\nbsp{}\cite{paszke2019pytorch}
implementation of a /Residual  Network/\nbsp{}\cite{he2016deep} with 50 residual
blocks, called /ResNet50/.  We used the /HAQ/ framework\nbsp{}\cite{wang2019haq}
to quantize  each layer  with a  bitwidth in the  $[1,8]$ interval,  producing a
smaller  network  that  was  retrained   on  /NVIDIA/  GPUs.   We  ensured  that
quantization  policies respected  size  constraints, keeping  total weight  size
under 10 /MB/, and  allowing the comparison of our results  to a reproduction of
the approach of the original paper\nbsp{}\cite{wang2019haq}.

**** Further Applications of Autotuning to Neural Networks
Autotuning  methods can  be  applied to  optimize  networks targeting  different
stages and structures.  Online Gradient  Descent coupled with backpropagation is
a         well          established         approach          to         network
training\nbsp{}\cite{wilson2003general,lecun2012efficient},  but fine  tuning of
the network architecture  and its parameters is also a  promising target for the
application of optimization methods.

A recent survey and  taxonomy by Talbi\nbsp{}\cite{talbi2021automated} discusses
work on /Neural Architecture Search/ (NAS), and /Hyperparameter Tuning/.  Neural
Architecture  Search  consists  of  using   optimization  methods  to  find  the
modifications of  the layers  and connections  of a network  that best  adapt to
resource constraints  or hardware architecture features.   Hyperparameter Tuning
refers to  the optimization  of any  network parameters that  have an  impact on
performance without necessarily requiring structure modification.

*** ResNet50 and ImageNet
**** ResNet50 Architecture
Residual    Networks,    or   /ResNets/,    were    introduced    by   He    /et
al./\nbsp{}\cite{he2016deep}, and  the intuition behind  the idea is to  add the
original layer  input, or /residual/,  to the output  of a stack  of convolution
layers. Keeping this residual component in  a layer output enables better global
optimization of  the network. Figure\nbsp{}[[fig:resnet50]] shows  the /PyTorch/
implementation of a 50-block Residual Network, called /ResNet50/.

We generated this visualization of /ResNet50/'s structure with the /HiddenLayer/
library\nbsp{}\cite{hiddenlayer2021visualization},    adding   the    underlying
structure  of /Bottleneck  Residual Blocks/  in  panel /(a)/,  and of  /Residual
Blocks/ in  panel /(b)/. Bottleneck  blocks serve  the same purpose  as Residual
blocks, adding  an input component  to layer output,  but are favored  in larger
networks because they are computationally less expensive.

#+latex: \clearpage

#+NAME: fig:resnet50
#+CAPTION: \textit{ResNet50} architecture, implemented in \textit{pytorch}
#+ATTR_LATEX: :placement [t] :width .99\textwidth
[[./img/neural_net_autotuning/resnet50_architecture.pdf]]

#+latex: \clearpage

**** The ImageNet Dataset

#+NAME: fig:imagenet
#+CAPTION: Images sampled from some of the categories in \textit{ImageNet},
#+CAPTION: adapted from Deng /et al./\nbsp{}\cite{deng2009imagenet}
#+ATTR_LATEX: :placement [t] :width .99\textwidth
[[./img/neural_net_autotuning/imagenet_small.pdf]]

In this  study we were interested  in maintaining the accuracy  of /ResNet50/ on
the /ImageNet/ dataset  as we decreased the  bit precision for a  set of network
layers.   /ImageNet/\nbsp{}\cite{deng2009imagenet} is  a massive  image dataset,
with   $3.2$    million   labeled    and   semantically    categorized   images.
Figure\nbsp{}[[fig:imagenet]] shows a  sample of the dataset.   /ImageNet/ is widely
used to validate works on image recognition, and the dataset version we obtained
for the work presented in this chapter  came from a series of visual recognition
challenges\nbsp{}\cite{russakovsky2015imagenet}.

In  each  repetition  the  experiments  discussed  in  Section\nbsp{}[[Performance
Evaluation  and Comparison]]  we used  distinct  subset of  30 thousand  uniformly
sampled images, of which 20 thousand were  used for training and 10 thousand for
validation.

*** Search Space and Objective Function
The search space  we explored consisted of 54 convolutional  layers, that is, 54
component  layers  of  the  50  residual  blocks  in  the  /PyTorch/  /ResNet50/
implementation. We optimized only the bit  precision for weights, since the code
that quantized activation precision was only  released by the authors after this
study was completed.

Choosing between a bit precision in the $[1, 8]$ interval for the 54 quantizable
layers generates  a search space  with $8^{54} \approx  10^{49}$ configurations.
This search  space is relatively large,  although not the largest  we studied in
this thesis.  Completely evaluating it  is unfeasible, considering we would have
to retrain and validate the quantized network on our 30 thousand image sample of
/ImageNet/, which  took tens  of minutes  for each  quantization on  an /NVIDIA/
/K80/ GPU.

The  accuracy of  a  network is  typically  measured by  its  /Top1/ and  /Top5/
accuracy  measures.  Both  metrics measure  the  ratio between  right and  wrong
network predictions in the testing dataset, but the /Top1/ accuracy considers as
a prediction  only the class with  the highest predicted probability,  while the
/Top5/  metric  considers  the  classes  with the  top  five  highest  predicted
probabilities. For example, if a network predicts a picture of a Chihuahua is in
the  "Muffin"  class with  85%  probability  and in  the  "Dog"  class with  80%
probability, this  would be considered  a miss in /Top1/  accuracy but a  hit in
/Top5/.

In this  work we were  interested in minimizing  the distance between  the 8-bit
uniformly quantized /Top5/ accuracy and the /Top5/ for a quantized network whose
total  weight size  was under  10 /MB/.   We targeted  /Top5/ because,  from the
perspective of the HPE researchers we collaborated with, it was more interesting
that the network was made to fit on the smallest possible device, perhaps at the
cost of some /Top1/ accuracy, because then it would be cheap to run a larger set
of predictions.  In  the end, suggesting the two metrics  are linked, optimizing
for /Top5/ produced /Top1/ results comparable  to the ones found by the original
paper\nbsp{}\cite{wang2019haq} and  by our replication  of their results  in our
settings.

We also performed experiments that added  the total weight size to the objective
function and dropped  the 10 /MB/ restriction,  but we found it  hard to compare
the results  we produced with  the original paper,  so the main  discussion that
follows  will regard  the  performance  of optimization  methods  on the  /Top5/
distance minimization.

*** Optimization Methods
This section describes  the optimization methods we applied  to the quantization
policy optimization  for /ResNet50/. The performance  comparison between methods
is presented in the next section.

**** Random and Sobol Sampling
Although practically  impossible, in  order to  properly explore  the /ResNet50/
quantization search  space it would  be ideal know  the global optimum.   In the
absence  of this  knowledge, it  is  useful to  compare the  performance of  the
optimization methods we wish to use  to simple uniform random sampling. Since in
high dimensional  spaces almost all  uniformly sampled  points would lie  in the
extremes of the search space, we also  compare the performance of methods with a
sampling  algorithm   using  Sobol  sequences,  that   produce  low-discrepancy,
space-filling, samples.

**** Gaussian Process Regression with Expected Improvement
We used the same two /R/ packages we used in Chapters\nbsp{}\ref{chap:spapt} and
\ref{chap:laplacian}           to           implement          the           GPR
algorithm\nbsp{}\cite{roustant2012dicekriging}.    The   /DiceKriging/   package
provided  Gaussian  Processes  Regression  fits using  the  /Matérn/  covariance
kernel, and the  /DiceOptim/ package provided functions to  compute the Expected
Improvement (EI) metric  at each step, as  described in Section\nbsp{}[[Expected
Improvement  for  Gaussian  Process  Regression]].  Additionally,  we  used  the
/future.apply/ package\nbsp{}\cite{bengtsson2020unifying} to run EI computations
in parallel.

***** Respecting the Size Constraint with Sobol Samples
We  used  the implementation  of  the  Sobol sequence  low-discrepancy  sampling
algorithm from  the /randtoolbox/\nbsp{}\cite{dutang2020randtoolbox}/R/ package.
Low-discrepancy   sampling  and   space-filling   designs   were  discussed   in
Section\nbsp{}\ref{sec:filling}.   To  ensure  all samples  respected  the  size
constraint,  we  progressively  doubled  the  sample  size  until  enough  valid
configurations were produced.

***** Pseudocode for the GPR Method
Figure\nbsp{}\ref{alg:gpr} shows the pseudocode for our adaptation of GPR to the
bit  precision   tuning  problem,  including  the   constrained  Sobol  sampling
algorithm.  The  algorithm starts  with a  smaller valid  Sobol sample  which is
completely evaluated and used to fit a Gaussian Process.  This model fit enables
computing the  Expected Improvement for  a much  larger valid Sobol  sample.

We  pick the  three configurations  with  the highest  EI from  this sample  and
compute  additionally   the  EI   for  a   sampled  neighborhood   around  these
configurations.  The  neighborhood is  also generated  using the  Sobol sampling
algorithm.

We then  measure the unique configuration  with the highest EI  over this larger
sample, augmented  with a targeted neighborhood,  and keep a backlog  of the 200
other  configurations  with  the highest  EI  in  the  sample.   The EI  of  the
configurations in the backlog is constantly reevaluated, and the backlog is used
if  the  highest EI  in  the  current  sample is  smaller  than  the EI  of  any
configuration in the backlog.

#+begin_export latex
\begin{figure}
  \begin{minipage}{\textwidth}
    \begin{algorithm}[H]
      \small
      \begin{algorithmic}[1]
        \Function{Filter\_Sobol}{$dimension,\,target\_size,\,max\_weight$}
        \State $sample\_size \gets 10^6$
        \State $samples \gets \emptyset$
        \Statex
        \While{$\textsc{Size}(samples) \leq target\_size$}
        \State $samples \gets \textsc{Sobol}(dimension, \,sample\_size)$
        \State $samples \gets \textsc{Filter}(samples, \,\leq max\_weight)$
        \State $sample\_size \gets 2 * sample\_size$
        \EndWhile
        \Statex
        \State \textbf{return} $samples$
        \EndFunction
        \Statex
        \Function{GPR}{$budget, \,dimension, \,starting\_size, \,sample\_size, \,max\_weight$}
        \State $starting\_sample \gets \textsc{Filter\_Sobol}(dimension, \,starting\_size, \,max\_weight)$
        \State $training\_set \gets \textsc{Measure}(starting\_sample)$
        \State $measurements \gets \textsc{Size}(training\_set)$
        \State $EI\_backlog \gets \emptyset$
        \Statex
        \While{$measurements \leq budget$}
        \State $model \gets \textsc{FitGP}(training\_set)$
        \Statex
        \State $new\_sample \gets \textsc{Filter\_Sobol}(dimension, \,sample\_size, \,max\_weight)$
        \State $candidates \gets \textsc{ComputeEI}(model, \,new\_sample)$
        \State $neighbors \gets \textsc{ComputeEI}(model, \,\textsc{Neighborhood}(candidates))$
        \Statex
        \State $new\_measurement \gets \textsc{MaxEI}(candidates \cup neighbors \cup EI\_backlog)$
        \State $EI\_backlog \gets (EI\_backlog \cup candidates \cup neighbors) \setminus new\_measurement$
        \Statex
        \State $training\_set \gets training\_set \cup \textsc{Measure}(new\_measurement)$
        \State $measurements \gets \textsc{Size}(training\_set)$
        \EndWhile
        \Statex
        \State \textbf{return} $\textsc{MaxTop5}(traning\_set)$
        \EndFunction
        \Statex
      \end{algorithmic}
    \end{algorithm}
  \end{minipage}
  \caption[Pseudocode  implementations   of  the   GPR  method   using  Expected
    Improvement   (EI),   and   of   the   constrained   sampler   using   Sobol
    sequences]{Pseudocode  implementation  of  the  GPR  method  using  Expected
    Improvement (EI), and  of the constrained sampler using  Sobol sequences. In
    our experiments,  variables were $budget  \gets 245$, $dimension  \gets 54$,
    $starting\_size \gets 64$, $sample\_size \gets 3240$, and $max\_weight \gets
    10$ \textit{MB}}
  \label{alg:gpr}
\end{figure}
#+end_export

**** Reinforcement Learning: The Baseline Method
We reused  the Reinforcement Learning  (RL) /Python/ implementation  provided by
the authors of the /HAQ/ paper\nbsp{}\cite{wang2019haq}.  Reinforcement Learning
using  the /actor-critic/  model  was  discussed in  Section\nbsp{}[[Reinforcement
Learning]],  and  the implementation  in  /HAQ/  employed additionally  the  /Deep
Deterministic     Policy     Gradient/     method     from     Lillicrap     /et
al./\nbsp{}\cite{lillicrap2015continuous}.  The cumulative regret to minimize is
the  distance  of  the  quantized   accuracy  from  the  initial  8-bit  uniform
quantization, across all tested quantization policies.

***** Respecting the Size Constraint
To respect  the constraint imposed on  the sum of network  weights, the original
paper  implementation used  a /round-robin/  algorithms to  decrease the  weight
bitwidth for each  layer sequentially, starting with the first,  until the total
sum crossed  the 10  /MB/ threshold.   After analyzing our  results with  RL and
other methods, our hypothesis for this  implementation choice is that it encodes
the  implicit knowledge  that the  bit precision  of the  initial layers  of the
/PyTorch/ /ResNet50/ implementation has a smaller impact on accuracy.

*** Performance Evaluation and Comparison
This section presents our evaluation of the performance of our implementation of
Gaussian Process Regression with Expected  Improvement (GPR) and of the baseline
Reinforcement  Learning  (RL)\nbsp{}\cite{wang2019haq}  on the  optimization  of
quantization  policies for  /ResNet50/,  targeting the  /ImageNet/ dataset.   We
compare the performance of GPR and RL  with a uniform random sampling method and
with a low-discrepancy space-filling sampler using Sobol sequences.

The simple samplers performed interestingly well in comparison with more complex
approaches, a pattern we have already  observed in many of the studies presented
in this thesis.  This  time, the more complex methods were  able to improve upon
the  results  of  the  simpler  approaches.  Under  the  same  budget,  our  GPR
implementation  achieved  accuracy   statistically  indistinguishable  from  the
original RL implementation.  In comparison with  the RL algorithm, we argue that
our approach produces more consistent quantization policies between repetitions,
and  that it  is more  extensible  and perhaps  more  robust to  changes in  the
objective function.

**** Optimizing /Top5/ for Constrained Total Weight
#+NAME: fig:comparison_means
#+CAPTION: 10 repetitions of the 4 methods we compared,
#+CAPTION: using a budget of 245 measurements,
#+CAPTION: with 95% confidence intervals for estimates of the mean
#+ATTR_LATEX: :placement [t] :width \textwidth
[[./img/neural_net_autotuning/comparison_means.pdf]]

Figure\nbsp{}[[fig:comparison_means]] compares the  best quantization policies found
by each  optimization method across  10 repetitions,  with a fixed  budget.  The
total  /ResNet50/ weight  size was  constrained to  be no  larger than  10 /MB/.
Comparing  the 95%  confidence  intervals for  the estimates  of  the mean  best
configuration for each method  we see that GPR and RL  have similar results, but
that the  Sobol sampler produces policies with good accuracy.
Figure\nbsp{}[[fig:comparison_best_gpr_rl]] shows  the accuracy metrics of  the best
configurations found at each iteration, for the 10 repetitions of GPR and RL.

#+NAME: fig:comparison_best_gpr_rl
#+CAPTION: Best quantization policy found by each method
#+CAPTION: during optimization, across
#+CAPTION: 10 repetitions
#+ATTR_LATEX: :placement [t] :width \textwidth
[[./img/neural_net_autotuning/gpr_rl_smooth.pdf]]

Figures\nbsp{}[[fig:rl_results]] and [[fig:gpr_results]] present  four columns with more
detailed looks  into the quantization  policies explored by  GPR and RL  and the
corresponding network configuration, in the  first column, total weight size, in
the  second column,  and /Top5/  and  /Top1/ accuracy  metrics in  the last  two
columns.

The  first  columns of  both  Figures  show  stacked  /ResNet50/ layers  in  the
/y/-axis, in similar fashion to  what is presented in Figure\nbsp{}[[fig:resnet50]].
The /x/-axis, as in all columns, shows each of the 245 measurements. Each square
represents a  layer at  a given  time, and  is colored  with a  gradient between
\textcolor{BrickRed}{red} for 1-bit  and \textcolor{OliveGreen}{green} for 8-bit
quantization. Initial blocks  of measurements for all experiments  consist of 64
/warm-up/  measurements, generating  visually distinct  starting regions  in all
images.

The three horizontal dashed lines in the second column, from top to bottom, mark
the size of  /ResNet50/ with all layers quantized  with 8, 2, and 1  bit for all
layers. Likewise for the third and fourth columns, the horizontal line marks the
accuracy of the 8-bit quantized network.  The vertical lines in all columns mark
the iteration  that found the point  with the highest /Top5/  accuracy, for each
repetition.

Figure\nbsp{}[[fig:rl_results]]  shows  that the  policies  arrived  at by  RL  have
different quantization distributions.  Experiments /3/,  /5/, /6/, and /10/ have
clearly defined red bands in similar  regions, corresponding to layers that were
quantized with fewer bits.  Fainter bands in the same places can  be seen in the
other experiments,  but we can  identify other features  as well, such  as green
bands  or more  uniform orange  policies.  The  original RL  implementation used
random sampling and  round-robin decrease of weights to obtain  the warm-up set,
while our  GPR implementation used  Sobol sequences and filtering,  resulting in
different-looking starting stages.

Figure\nbsp{}[[fig:gpr_results]] shows clear  red bands in the same  top layers, and
less variable  policy choices than  RL across  experiments. We see  regions that
abruptly change  from darker to light  greens in experiments /2/,  /4/, /9/, and
/10/,  which could  be due  to the  algorithm using  a configuration  on the  EI
backlog and picking a new region of the search space to explore.

#+latex: \clearpage

#+NAME: fig:rl_results
#+CAPTION: Results with the baseline Reinforcement Learning method
#+ATTR_LATEX: :placement [t] :width \textwidth
[[./img/neural_net_autotuning/rl_original.png]]

#+latex: \clearpage

#+NAME: fig:gpr_results
#+CAPTION: Results with Gaussian Process Regression
#+ATTR_LATEX: :placement [t] :width \textwidth
[[./img/neural_net_autotuning/gpr_restricted.png]]

#+latex: \clearpage

***** Comparing Policy Variability on Different Samples
#+NAME: fig:variances
#+CAPTION: Running optimized quantization policies with new
#+CAPTION: /ImageNet/ samples, with 95% confidence intervals
#+CAPTION: for the mean estimate. The baseline is the uniform 8-bit
#+CAPTION: /ResNet50/ quantization
#+ATTR_LATEX: :placement [t] :width 0.8\textwidth
[[./img/neural_net_autotuning/variances.pdf]]

Figure\nbsp{}[[fig:variances]]  shows the  result of  running some  of the  policies
produced by GPR, and  all produced by RL, in new  random /ImageNet/ samples.  We
wanted to check if the ``homogeneous'' RL experiments, that is, the ones without
clear red bands, had a different  behavior when pitted against new datasets.  We
did not see any difference between RL experiments, but we were surprised to find
that  RL experiments  were consistently  better, and  outside of  the confidence
intervals,  for   new  datasets,   although  differences  are   not  practically
significant.  Experiment numbering is the same from Figures\nbsp{}[[fig:rl_results]]
and [[fig:gpr_results]].

*** Measuring Performance and Detecting Significance for GPR
In this  section we analyze the  performance of our GPR  implementation, discuss
our attempts to detect the significance of the quantization impact of each layer
on /Top5/ accuracy,  and present perspectives on future explorations  of GPR for
CNN quantization.

**** Performance of the GPR Method
Figure\nbsp{}[[fig:gpr_performance]] shows the performance  of six executions of the
GPR  algorithm described  in  Figure\nbsp{}\ref{alg:gpr}, broken  down by  step.
Most steps of  the algorithm have fixed computational cost,  since Sobol samples
and neighborhoods have fixed sizes  across the optimization process, but fitting
the model  takes more  time as the  size of training  data grows.   The accuracy
measurement time  for a quantization policy  on an /ImageNet/ sample  depends on
the policy. We did not explore the reasons why this happens, but we suspect that
training a network using operations with less precision is faster.

#+NAME: fig:gpr_performance
#+CAPTION: Execution time of the different steps of our GPR
#+CAPTION: implementation. The initial sample measurement is not shown,
#+CAPTION: and the /y/-axis is logarithmic
#+ATTR_LATEX: :placement [t] :width .99\textwidth
[[./img/neural_net_autotuning/timing_resnet50.pdf]]

**** Detecting Significance with Sobol Indices
We  used  the  /sensitivity/  /R/  package\nbsp{}\cite{iooss2021sensitivity}  to
compute Sobol indices,  using Gaussian Process surrogate models,  for the impact
on /Top5/ of quantizing  each /ResNet50/ layer.  Figure\nbsp{}[[fig:sobol_resnet50]]
shows the  indices computed for the  ensemble of Sobol samples  we collected, in
the left  panel, and  for the  Sobol samples plus  the search  space exploration
performed by GPR, in the right panel.

It is expected that  the first and last layers have the  most impact, since they
determine the  precision of all computations  that follow, and the  precision of
the final  output, respectively. The confidence  intervals for the means  of the
other indices in the Sobol sample all cross  zero but we know that we can impact
accuracy by  changing them, so  the interpretation  is unclear. Likewise,  it is
unclear how to interpret  that all indices seem to have  an equally large impact
when the explored search spaces are added on the right panel, since we see clear
bands of strongly quantized layers on the GPR results.

#+NAME: fig:sobol_resnet50
#+CAPTION: Sobol indices computed for the impact on /Top5/ accuracy
#+CAPTION: of the quantization of each /ResNet50/ layer using only Sobol
#+CAPTION: samples, in the left panel, and Sobol samples plus the
#+CAPTION: search spaces explored by GPR, in the right panel
#+ATTR_LATEX: :placement [h] :width \textwidth
[[./img/neural_net_autotuning/sensitivity.pdf]]

**** Perspectives for Further Exploration
***** Running GPR with Data from Past Experiments
An  interesting feature  of  Gaussian Process  Regression is  that,  due to  its
stability, we  can leverage past executions  by keeping only a  /.csv/ file with
the previous measurements.  It is more expensive to relaunch GPR with each added
past exploration,  since we need to  fit a model for  more data, but the  way in
which new experiments are picked does not change.

The  top row  of Figure\nbsp{}[[fig:gpr_ensemble]]  shows  a GPR  instance that  was
launched with the ensemble  of data from all past GPR  experiments.  On the last
two columns we can  see each peak in accuracy, corresponding to  the end of each
experiment from Figure\nbsp{}[[fig:gpr_results]]. The bottom  row shows the same GPR
instance, but  focuses only on  the experiments  picked after fitting  the model
with the data ensemble.

We see that all newly picked configurations have around 95% /Top5/ accuracy, and
that accuracy seems  to continue to increase. A long-running  optimizer could be
continuously  fed  with   new  experiments  as  tests  are   made,  even  during
prototyping, and  could be used  to produce  configurations similar to  the best
known  ones, which  could help  understanding  the impact  of each  controllable
parameter.

#+NAME: fig:gpr_ensemble
#+CAPTION: Running GPR with the gathered ensemble of GPR experiments.
#+CAPTION: The bottom row shows only the new measurements performed after
#+CAPTION: fitting a Gaussian Process with the ensemble of experiments, and the top
#+CAPTION: row shows all the data that were used
#+ATTR_LATEX: :placement [h] :width \textwidth
[[./img/neural_net_autotuning/gpr_ensemble.png]]

***** Optimizing Accuracy and Weight
We ran preliminary experiments with adding  the total weight size as a component
of  the performance  metric for  GPR and  RL, but  this made  it unclear  how to
compare the  GPR results to the  original RL implementation, because  to add the
new  metric we  removed the  round-robin weight  decrease used  in the  original
paper.

Figures\nbsp{}[[fig:gpr_multi_results]]  and [[fig:rl_multi_results]]  show the  results
for GPR and RL, respectively. We see that this expanded problem is a much harder
task for both methods, but RL is especially impacted by it. This might be a good
reason for  the original paper's usage  of the round-robin procedure  for weight
decrease. Experiments /2/, /3/, and /4/  with the RL method show quirky features
that  we  are  not able  to  explain,  but  we  hypothesize that  some  emerging
restarting strategy has  taken place, especially for  experiment /4/. Experiment
/2/ is the only one that seems to behave as expected.

The  experiments with  GPR  were much  more  stable.   We show  two  of them  in
Figure\nbsp{}[[fig:gpr_multi_results]], and we  can see red bands  that resemble the
ones from Figure\nbsp{}[[fig:gpr_results]]. Although  this modified version does not
achieve  the  same /Top5/  accuracy,  it  does find  interesting  configurations
producing quantization policies equivalent to using  less than 2 bits per layer,
without  letting /Top5/  go  below  80%. This  hints  that  policies exist  that
generate much smaller networks with a manageable compromise in accuracy.

#+latex: \clearpage

#+NAME: fig:rl_multi_results
#+CAPTION: Optimizing accuracy and weight with
#+CAPTION: the modified baseline Reinforcement Learning method
#+ATTR_LATEX: :placement [t] :width \textwidth
[[./img/neural_net_autotuning/rl_top5_size.png]]

#+NAME: fig:gpr_multi_results
#+CAPTION: Optimizing accuracy and weight with
#+CAPTION: Gaussian Process Regression with EI.
#+ATTR_LATEX: :placement [b] :width \textwidth
[[./img/neural_net_autotuning/gpr_top5_size.png]]

#+latex: \clearpage

*** Summary
:PROPERTIES:
:CUSTOM_ID: sec:gprnn-summary
:END:

This  chapter  presented an  application  of  Gaussian Process  Regression  with
Expected  Improvement to  the problem  of finding  the smallest  mixed-precision
quantization policies for the layers of a neural network, subject to keeping the
accuracy comparable to the 8-bit per layer network version, and the total weight
size below 10 /MB/.

We compared our GPR approach with  a baseline Reinforcement Learning method, and
with  uniform  and  space-filling  samplers.   The  sampling  methods  performed
relatively well, but worse than  the more complex approaches. Our implementation
produced  policies  with  comparable  accuracy  to  the  Reinforcement  Learning
approach, with the  advantage of more consistent explorations.

Although our  attempts to estimate  Sobol indices were inconclusive,  we believe
that the  choices made  by the GPR  method are overall  easier to  interpret and
understand, in comparison with the results produced by the RL.

The  GPR method  was  also  capable of  leveraging  past  executions to  produce
policies with similar accuracy. Additionally, we  argue that GPR was more robust
to the addition  of the total weight size to  the objective function. Accurately
estimating  Sobol  indices,  performing multi-objective  GPR  optimization,  and
learning more about high-performing configurations  using past data are possibly
interesting paths that we could point to during this chapter, but that we do not
explore in this thesis.

* Conclusion
** Conclusion
* References                                                         :ignore:
#+begin_export latex
\bibliographystyle{IEEEtran}
\bibliography{bibliography/references}
#+end_export
* Lists of Figures, Tables                                           :ignore:
#+TOC: tables
#+latex: \listoffigures{}
#+TOC: figures
