#+STARTUP: overview indent inlineimages logdrawer
#+TITLE: Towards Transparent and Parsimonious
#+TITLE: Methods for Automatic Performance Tuning
#+AUTHOR:      Pedro Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: ExportableReports(E)
#+TAGS: FAPESP(f)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:4 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+begin_src emacs-lisp :exports none :eval no-export
;; Pour configurer les subdivisions de la classe book (indiquer : #+LaTeX_CLASS: book)
(with-eval-after-load "ox-latex"
(add-to-list 'org-latex-classes
             '("book"
               "\\documentclass{book}"
               ("\\part{%s}" . "\\part*{%s}")
               ("\\chapter{%s}" . "\\chapter*{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}"))))
#+end_src

#+RESULTS:
| book    | \documentclass{book}                 | (\part{%s} . \part*{%s})       | (\chapter{%s} . \chapter*{%s})       | (\section{%s} . \section*{%s})             | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) |
| beamer  | \documentclass[presentation]{beamer} | (\section{%s} . \section*{%s}) | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) |                                      |                                            |
| article | \documentclass{article}              |                                |                                      |                                            |                                      |                                            |

#+LATEX_CLASS: book
#+LATEX_CLASS_OPTIONS: [11pt,oneside,a4paper]
#+DRAWERS: latex_headers

:latex_headers:
#+LATEX_HEADER: \usepackage[a4paper]{geometry}
#+LATEX_HEADER: \geometry{
#+LATEX_HEADER:   %top=32mm,
#+LATEX_HEADER:   %bottom=28mm,
#+LATEX_HEADER:   %left=24mm,
#+LATEX_HEADER:   %right=34mm,
#+LATEX_HEADER:   textwidth=152mm, % 210-24-34
#+LATEX_HEADER:   textheight=237mm, % 297-32-28
#+LATEX_HEADER:   vmarginratio=8:7, % 32:28
#+LATEX_HEADER:   hmarginratio=12:17, % 24:34
#+LATEX_HEADER:   % Com geometry, esta medida não é tão relevante; basta garantir que ela
#+LATEX_HEADER:   % seja menor que "top" e que o texto do cabeçalho caiba nela.
#+LATEX_HEADER:   headheight=25.4mm,
#+LATEX_HEADER:   % distância entre o início do texto principal e a base do cabeçalho;
#+LATEX_HEADER:   % ou seja, o cabeçalho "invade" a margem superior nessa medida. Essa
#+LATEX_HEADER:   % é a medida que determina a posição do cabeçalho
#+LATEX_HEADER:   headsep=11mm,
#+LATEX_HEADER:   footskip=10mm,
#+LATEX_HEADER:   marginpar=20mm,
#+LATEX_HEADER:   marginparsep=5mm,
#+LATEX_HEADER: }
#+LATEX_HEADER: \widowpenalty=10000
#+LATEX_HEADER: \clubpenalty=10000
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX:HEADER: \usepackage[inline]{enumitem}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amssymb,amsthm}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{newpxtext}
#+LATEX_HEADER: \usepackage{newpxmath}
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \usepackage{titling}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-qtree}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstdefinelanguage{Julia}%
#+LATEX_HEADER:   {morekeywords={abstract,struct,break,case,catch,const,continue,do,else,elseif,%
#+LATEX_HEADER:       end,export,false,for,function,immutable,mutable,using,import,importall,if,in,%
#+LATEX_HEADER:       macro,module,quote,return,switch,true,try,catch,type,typealias,%
#+LATEX_HEADER:       while,<:,+,-,::,/},%
#+LATEX_HEADER:    sensitive=true,%
#+LATEX_HEADER:    alsoother={$},%
#+LATEX_HEADER:    morecomment=[l]\#,%
#+LATEX_HEADER:    morecomment=[n]{\#=}{=\#},%
#+LATEX_HEADER:    morestring=[s]{"}{"},%
#+LATEX_HEADER:    morestring=[m]{'}{'},%
#+LATEX_HEADER: }[keywords,comments,strings]%
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\tiny,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=b,
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   numbers=left,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=1,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \setlength{\parskip}{0.5em}
#+LATEX_HEADER: \usepackage[pagestyles,raggedright]{titlesec}
#+LATEX_HEADER: \titleformat{\chapter}[display]{\normalfont\bfseries}{}{0pt}{\Huge}
# #+LATEX_HEADER: \newpagestyle{mystyle}
# #+LATEX_HEADER: {\sethead[\thepage][][\chaptertitle]{}{}{\thepage}}
# #+LATEX_HEADER: \pagestyle{mystyle}
:end:

* Thesis Drafts                                                    :noexport:
** Outlining the draft with Arnaud
*** Introduction
General presentation of the manuscript
*** Context
**** Observation: Historical Trends in Hardware Design
**** Consequences for compilers and application developers: Generating optimized code has become increasingly difficult
**** Autotuning approaches and difficulties
- Describe existing approaches (opentuner, etc.) and provide examples
  of autotuning problems with their dimension

All this leads you to explain the current state of the technology and
what is possibly "wrong" with current approaches.
- Huge dimension and unclear geometry
- Many black/box search heuristics whose effectiveness is difficult to
  evaluate/interpret
*** Optimization (in Autotuning context)
**** Optimization Methods
***** Methods Based on Derivatives (Local Descent)
****** Gradient based
- Classical hypothesis: convex
- convergence difficulties
- path of GD on 3 booth versions?
- mention very high cost of estimating derivatives
- make hypotheses more clear and explicit
  - explain why hypothesis are restrictive for autotuning problems
****** Gradient + Hessian based
- Taylor expansion: Information about derivatives at a point --> (approximation)
  information about the function around a point
- hessian definition
- fast convergence
  - Example: converging in 1 step for the booth function
****** This requires too strong hypothesis, hence the needs for more "general" methods
- high cost of estimating derivatives
***** Stochastic Methods (Derivative-Free)
****** Single-State
- compute neighborhood of x --> perturb x
  - random walk
  - greedy random walk
  - best random walk
- probability distribution for acceptance
  - bio-inspired simulated annealing
****** Population-Based methods
- GAs
- PSO
- Ant colony?
***** Mini-conclusion
- Most of these methods are not parsimonious. Require many estimates
  of f, of \nabla f and even sometimes of \Delta f! Stochastic methods are even
  worse as they need to explore whereas descent based methods head to
  the optimum more directly.
**** Learning: Building a Surrogate
***** Statistical Learning: Linear Regression
- the model
- how to fit
- "Model quality" +how to check whether the model is correct or not ?+
- how to interpret the significance (LM-CI, ANOVA). May hint to good
  values for optimization

Limitation: simple model with shape constraits, cannot "fit"
everything
***** Gaussian Process
- the model
- how to fit
- provides mean estimates with confidence estimation
- how to interpret the significance (sobol indices) but quite costly
***** Mini-Conclusion
Two big classes of models. Generality vs. interpretability. Yet
everything we mentioned assumes X is given, sampled from
observations. In our contects, We can choose which X to test, either
to test the model, or to improve its quality, or to find a "good"
value in our space $\mathcal{X}$.
**** Design of Experiments
***** 2-level factorial designs
***** Screening
Super efficient but very limited
***** Optimal designs
A "flexible" screening: allows to include non-linear terms,
interactions, etc. if needed.

Awesome but if a parameter was not included in the model or if the
model is too simple (e.g. only comprised a linear term where a
quadratic one would have been needed, or an important interaction was
not included), we won't be able to detect it (lack of fit).
***** Space-filling Designs
Not very efficient for parameter estimate but good to evaluate the
lack of fit.

Also good for variance minimization in GP.
***** Mini-conclusion
- Designs to obtain good-quality parameter estimates
  - Screening and D-opt for LM
  - SFD for GP
- Designs to test the model quality (lack of fit)
  - SFD fo LM

If model based, parameter significance and estimation can be used to
reduce dimension and guide the optimization. With this DoE approach,
we have a clear separation between the sampling phase and the
interpretation phase. But what if no parameter really appears
significant anymore ?
**** Online Learning: the exploration / exploitation trade-off
***** Bandits: simple (discrete choice, optimize regret = \sum_t R_t), USB
***** EI for GP (continuous choice, optimize EI = \max_t R_t)
Also mention GP-UCB and contrast with EGO. There are also variants for
Linear Model (LinUCB).
***** Mini-conclusion
These methods seamlessly mix exploration and exploitation but the
overal objective function is generally the regret, which makes sense
for a self-optimizing system (e.g. facebook) but not in an autotuning
context (where EI is more meaningful).
**** Summary and Proposal
- use glassbox (DoE based) approach to perform the optimization,
  always try to interpret the results
- 2 big methods based on different exploration/exploitation strategy:
  1. Evaluate parameter significance and reduce dimension (two phases,
     iterative)
  2. Expected Improvement (first a general exploration phase with a
     SFD, then seamlessly mix exploration and exploitation)
  Possibly combinations of both approaches could be used back and
  forth depending on the specific information we learn on the use
  case.
- In this thesis, we evaluate these DoE-based approaches for several
  autotuning use cases and try to compare them with approaches that
  had beed previously proposed fot these use cases.
*** Evaluation
**** Reproducible Research Methodology
- Tools and such
- Explain difficulty of finding a needle in a haystack:
  - how to know whether we found the optimal value ?
  - how to know how far we are from the optimal value ?
  - how to know whether there is anything to find ?
  - how to know whether the geometry hypothesis we make are sound in
    an unknown space ?
  - ...
**** Use Case 1: GPU compiling flags
- mix binaire/numérique, opentuner (multi-armed to select the right
  stochastic descent algorithm)
- Tried to use clustering to identify significant parameters
- On the interest of using a Screening design.
  - Once the significant parameters are, they can easily be
    fixed. Are there the same as the ones found by opentuner.
**** Use Case 2: Kernel GPU Steven
- Base strategies (RS, GAs, Local Descent, ...)
- D-opt based approach \to excellent results
- GP-EI based approach (TODO)
**** Use Case 3: SPAPT
- Base strategies (RS because equivalent to other classical strategies
  s.a. GAs and others)
- D-opt based approach \to not really impressive compared to RS but
  maybe there is nothing to find.
  - Ability to interpret = unclear. Nothing to see or model too simple ?
**** Use Case 4: FPGA
- 100 of numerical parameters
- several metrics to optimize \to weighted combination
- OpenTuner with heuristic and bandits gave "good" results
  (improvement over the default one)
- +DoE+ because no more access to the code. Ideally GP-EI. But we could
  see that the OpenTuner exploration makes it very hard to interprete
  the geometry.
**** Use Case 5: Bit Quantization in Neural Nets
- 54 discrete numerical parameters
- several objective functions
- RL
- GP-EI
*** Conclusion and Perspectives
** Structure Drafts
1. Introduction
   1. Autotuning
      1. Algorithm Selection Problem?
   2. Overview of Autotuning Methods (taxonomy/decision tree)
   3. Search Heuristics
      - Introduction
      - OpenTuner
      - Autotuning GPU compiler parameters
      - Autotuning High Level Synthesis for FPGAs
   4. Statistical Learning
      - Parametric, nonparametric
   5. Related Work
      - Literature Review
2. Design of Experiments
   1. Introduction
      1. Linear Regression
   2. Screening
      1. Main effects
      2. Example with CUDA flags
   3. Factorial Designs
      1. Example?
   4. Optimal Design
      1. Properties of the BLUE, Information Matrix
      2. Variance-optimizing criteria
      3. Example on Laplacian GPU
   5. Autotuning SPAPT Kernels
      - Mixing factor types
      - Sampling with Constraints
      - Heteroscedasticity
3. Gaussian Process Regression
   1. Introduction
      1. Bayesian Linear Model (Rasmussen's Book)
      2. EGO
   2. Revisiting SPAPT kernels
   3. Quantization for Deep Neural Networks
4. Conclusion
   - Expressing structure with kernels? (Duvenaud's thesis)
   - Performance of the Federov Algorithm for D-Optimal design construction?
*** Structure Draft
- Course on performance optimization for HPC, and why it's hard
- Difficulty to optimize programs comes from complexity in:
  - Computer architecture
    - Pursuit of doubling performance, fitting more transistors,
      (Moore's Law), and the end of frequency and power
      scaling (Dennard's),
      mean that we need parallel architectures, which are more complex
  - Software
    - Parallel architectures are harder to program efficiently

** Underlying Hypotheses of Autotuning Methods
:PROPERTIES:
:EXPORT_FILE_NAME: hipotheses.pdf
:END:
*** Introduction                                                 :noexport:
Given  a program  with $X  \in \mathcal{X}$  configurable parameters,  we want  to
choose the best parameter values according  to a performance metric given by the
function  $f(X)$.   Autotuning methods  attempt  find  the $X_{*}$  that  minimizes
$f(\cdot)$.   Despite  their different  approaches,  autotuning  methods share  some
common hypotheses:

- There is no knowledge about the global optimal configuration
- There could be some problem-specific knowledge to exploit
- Measuring the effects of a choice of parameter values is possible but costly

Each  autotuning method  has  assumptions that  justify  its implementation  and
usage. Some of  these hypotheses are explicit,  such as the ones  that come from
the  linear model.   Others are  implicit,  such as  the ones  that support  the
implementation and the justification of optimization heuristics.
*** Overview of Autotuning Methods
:PROPERTIES:
:EXPORT_TITLE:
:EXPORT_FILE_NAME: tree.pdf
:END:
#+begin_export latex
\begin{sidewaysfigure}[t]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      for tree={%
        anchor = north,
        align = center,
        l sep+=1em
      },
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
        draw,
        [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
          draw,
          color = NavyBlue
          [{Search Heuristics},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{\textbf{Random} \textbf{Sampling}}, draw]
            [{Reachable Optima},
              draw,
              color = BurntOrange
              [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
                draw,
                color = BurntOrange
                [{Strong $corr(f(X),d(X,X_{*}))$?},
                  draw,
                  color = NavyBlue
                  [{More Global},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                    [, phantom]]
                  [{More Local},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{High local optima density?},
                      draw,
                      color = NavyBlue
                      [{Exploit Steepest Descent},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                        [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                        [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                      [{Allows\\exploration},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                        [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                        [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
                [,phantom]]
              [,phantom]]]
          [{Statistical Learning},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{Parametric Learning},
              draw,
              color = BurntOrange
              [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                draw,
                color = BurntOrange
                [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                [, phantom]]
              [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                draw,
                color = BurntOrange
                [, phantom]
                [{Check for model adequacy?},
                  draw,
                  alias = adequacy,
                  color = NavyBlue
                  [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                    draw,
                    alias = interactions,
                    color = NavyBlue,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                      draw
                      [, phantom]
                      [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                        edge = {-stealth, ForestGreen, semithick},
                        edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                        draw,
                        alias = estimate,
                        color = ForestGreen]]
                    [{\textbf{Optimal} \textbf{Design}},
                      draw,
                      alias = optimal,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [{\textbf{Space-filling} \textbf{Designs}},
                    draw,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{Model selection},
                      edge = {-stealth, ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      alias = selection,
                      color = ForestGreen]]]]]
            [{Nonparametric Learning},
              draw,
              color = BurntOrange
              [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
              [{\textbf{Gaussian} \textbf{Process Regression}},
                alias = gaussian,
                draw]
              [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
      \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
      \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
      \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
    \end{forest}
  }
  \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
    denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
    denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
    denote algorithm choices, and \textbf{bold} boxes denote methods.}
\end{sidewaysfigure}
#+end_export

*** Previous Attempts                                            :noexport:
#+begin_export latex
\forestset{linebreaks/.style={for tree={align = center}}}
\begin{sidewaysfigure}
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      linebreaks
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\ $Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$}
        [{Does not construct\\estimate $Y = \hat{f}(\cdot, \theta{}(X))$}
          [{Reachable\\optima}
            [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$}
              [{Strong\\$corr(f(X),d(X,X_{*}))$}
                [{Low local\\optima density}
                  [{\textbf{Greedy}\\\textbf{Search}}, draw]
                  [{Estimate $f^{\prime}(X)$}
                    [{\textbf{Gradient}\\\textbf{Descent}}, draw]]]
                [{Introduce a ``population''\\$\mathbf{X} = (X_1,\dots,X_n)$}
                  [{Combination, mutation,\\within $\mathbf{X}$}
                    [{\textbf{Genetic}\\\textbf{Algorithms}}, draw]]
                  [{\textbf{Ant}\\\textbf{Colony}}, draw]]]
              [{Weaker\\$corr(f(X),d(X,X_{*}))$}
                [{Accept\\worst $f(X)$}
                  [{\textbf{Simulated}\\\textbf{Annealing}}, draw]]
                [{Avoid\\recent $X$}
                  [{\textbf{Tabu}\\\textbf{Search}}, draw]]]]]
          [{\textbf{Random}\\\textbf{Sampling}}, draw]]
        [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$}
          [{Parametric\\Learning}
            [{$\hat{f}(X) \approx f_1(X_1) + \dots + f_k(X_k)$}
              [{\textbf{Independent}\\\textbf{Bandit}}, draw]]
            [{$\hat{f}(X) = \mathcal{B}(logit(\mathcal{M}(X)\theta(X) + \varepsilon))$}
              [{\textbf{Logistic}\\\textbf{Regression}}, draw]]
            [{$\hat{f}(X) = \mathcal{M}(X)\theta(X) + \varepsilon$}
              [{\textbf{Linear}\\\textbf{Regression}}, draw]
              [{Measure\\properties of $X$}
                [{Independance\\of effects}
                  [{\textbf{Screening}}, draw]]
                [{Homoscedasticity of $\varepsilon$}
                  [{\textbf{Optimal}\\\textbf{Design}}, draw]]]]]
          [{Nonparametric\\Learning}
            [{Splitting\\rules on $X$}
              [{\textbf{Decision}\\\textbf{Trees}}, draw]]
            [{$\hat{f} = \mathcal{GP}(X; \mathcal{K})$}
              [{\textbf{Gaussian}\\\textbf{Process Regression}}, draw]]
            [{\textbf{Neural}\\\textbf{Networks}}, draw]
            [{\textbf{Multi-armed}\\\textbf{Bandit (?)}}, draw]]]]
    \end{forest}
  }
  \caption{Some hypothesis of some autotuning methods}
\end{sidewaysfigure}

#+end_export

#+begin_export latex
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\begin{table}[ht]
  \center
  \begin{tabular}{@{}p{0.3\textwidth}p{0.5\textwidth}@{}}
    \toprule
    Method &  Hypotheses \\ \midrule
    Metaheuristics & \tabitem There are similarities between natural fenomena and the target problem \\
    & \tabitem Gradual changes in configurations produce gradual changes in performance \\
    & \tabitem The optimal configuration is ``reachable'', by small changes, from non-optimal configurations  \\
    \addlinespace \\
    Machine Learning & \tabitem As more samples are obtained, decreases in ``out-of-sample error'' imply decreases ``in-sample error'' \\
    & \tabitem \textbf{TODO} What are the classes of models? \\
    \addlinespace \\
    Design of Experiments & \tabitem There is ``exploitable search space structure''\\
    & \tabitem Linear model: Response $\bm{Y}$ is an ``unobservable function'' of parameters $\bm{X}$: \\
    & \hspace{0.15\textwidth} $f(\bm{X}) = \bm{Y} = \bm{X\beta} + \bm{\varepsilon}$ \\
    & \tabitem Optimal Design: Variance of estimator $\hat{\bm{\beta}}$ is proportional to $\bm{X}$: \\
    & \hspace{0.15\textwidth} $\bm{\hat{\beta}} = \left(\bm{X}^{\intercal}\bm{X}\right)^{-1}\bm{X}^{\intercal}\bm{Y}$ \\
    \addlinespace \\
    Gaussian Process Regression & \tabitem Response $\bm{Y}$ is a sample from a multidimensional Gaussian distribution, with mean $m(\bf{X})$ and variance $k(\bm{X}, \bm{X}^{\intercal})$: \\
    & \hspace{0.1\textwidth} $\bm{Y} = f(\bm{X}) \sim \mathcal{N}(m(\bm{X}), k(\bm{X}, \bm{X}^{\intercal}))$ \\
    & \tabitem Predictions $\bm{Y_{*}}$ can be made conditioning distribution to observed data\\ \bottomrule
  \end{tabular}%
\end{table}
#+end_export

#+begin_export latex
\resizebox{!}{\textheight}{%
  \begin{tikzpicture}[rotate = -90]
    \begin{scope}
      \tikzset{every tree node/.style = {align = center}}
      \tikzset{level 1+/.style={level distance = 40pt}}
      \Tree [.\node(n0){Minimize $f: X \mapsto \mathbb{R}$ \\ $f(X) = f^{*}(X) + \varepsilon = m$};
        [.{Does not construct \\ estimate $\hat{f}(X; \theta)$}
          [.{Reachability of \\ optima}
            [.{\textbf{Greedy} \\ \textbf{Search}} ]
            [.{$d(x_i, x_j) \to 0$ $\implies$ \\ $d(f(x_i), f(x_j)) \to 0$}
              [.{Abundance of \\ local optima}
                [.{\textbf{Simulated} \\ \textbf{Annealing}} ]]
              [.{Closeness of a \\ ``population'' of $X$}
                [.{\textbf{Genetic} \\ \textbf{Algorithms}} ]]]]
          [.{\textbf{Random} \\ \textbf{Sampling}} ] ]
        [.\node(r1){Constructs surrogate \\ estimate $\hat{f}(X; \theta)$};
          [.{Explicit, variable \\ models of $\theta$}
            [.{$\hat{f} = M(X)\theta + \varepsilon$}
              [.{Independance \\ of effects}
                [.{\textbf{Screening}} ] ]
              [.{Homoscedasticity}
                [.{\textbf{Optimal} \\ \textbf{Design}} ] ] ] ]
          [.{Implicit, fixed \\ models of $\theta$}
            [.{\textbf{Neural Networks}} ] ]
          [.{Samples \\ functions}
            [.{$\hat{f} = \mathcal{GP}(X; \theta, \mathcal{K})$}
              [.{\textbf{Gaussian Process} \\ \textbf{Regression}} ] ] ] ] ]
    \end{scope}
    % \begin{scope}[thick]
    %   \draw [color = orange] (n0) to [bend left = 2] (r1);
    %   \draw [color = green] (n0) to [bend right = 2] (r1);
    % \end{scope}
  \end{tikzpicture}
}
#+end_export
** Application to the Microsoft Latin America PhD Award
*** Search Heuristics and Statistical Learning methods for Autotuning HPC Programs
:PROPERTIES:
:EXPORT_DATE:
:EXPORT_TITLE: @@latex: Search Heuristics and Statistical Learning \\ Methods for Program Autotuning@@
:EXPORT_FILE_NAME: application.pdf
:EXPORT_AUTHOR: Pedro Bruel
:END:

#+latex: \vspace{-4em}

High Performance Computing  has been a cornerstone of  collective scientific and
industrial progress  for at least  five decades.   Paying the cost  of increased
complexity,  software and  hardware  engineering advances  continue to  overcome
several challenges on the way of the sustained performance improvements observed
during the last  fifty years.  This mounting complexity means  that reaching the
advertised hardware  performance for  a given program  requires not  only expert
knowledge  of a  given hardware  architecture, but  also mastery  of programming
models  and  languages for  parallel  and  distributed  computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance,  we  can  draw   and  adapt  proven  methods  from  search,
mathematical  optimization and  statistics. The  effectiveness of  these adapted
methods  on autotuning  problems varies  greatly,  and hinges  on practical  and
mathematical properties of the problem and the corresponding /search space/.

When  adapting methods  for autotuning,  we must  face challenges  emerging from
practical properties  such as restricted  time and cost budgets,  constraints on
feasible  parameter values,  and the  need to  mix /categorical/,  /continuous/, and
/discrete/ parameters. To achieve useful results, we must also choose methods that
make hypotheses compatible with problem search  spaces, such as the existence of
discoverable,  or at  least  exploitable, relationships  between parameters  and
performance.   Choosing  an autotuning  method  requires  determining a  balance
between the exploration of a problem, when we would seek to discover and explain
relationships between  parameters and performance,  and the exploitation  of the
best optimizations we can find, when we would seek only to minimize performance.

The    effectiveness   of    search    heuristics   on    autotuning   can    be
limited\nbsp{}\cite{seymour2008comparison,balaprakash2011can,balaprakash2012experimental},
between other factors, by underlying hypotheses  about the search space, such as
the  reachability of  the  global optimum  and the  smoothness  of search  space
surfaces, which  are frequently not  respected. The derivation  of relationships
between  parameters  and  performance  from search  heuristic  optimizations  is
greatly hindered,  if not rendered impossible,  by the biased way  these methods
explore  parameters.   Some  parametric  learning methods,  such  as  Design  of
Experiments,  are  not widely  applied  to  autotuning.  These  methods  perform
structured  parameter  exploration,  and  can  be used  to  build  and  validate
performance     models,     generating    transparent     and     cost-effective
optimizations\nbsp{}\cite{mametjanov2015autotuning,bruel2019autotuning}. Other methods
from   the  parametric   family   are   more  widely   used,   such  as   Bandit
Algorithms\nbsp{}\cite{xu2017parallel}.   Nonparametric  learning   methods,  such  as
Decision    Trees\nbsp{}\cite{balaprakash2016automomml}     and    Gaussian    Process
Regression\nbsp{}\cite{parsa2019pabo}, are able  to reduce model bias  greatly, at the
expense of increased prediction variance. Figure\nbsp{}\ref{fig:tree} categorizes some
autotuning  methods  according to  some  of  the  key hypotheses  and  branching
questions underlying each method.

During this  thesis I have  adapted and  studied the effectiveness  of different
search heuristics and statistical learning  methods on optimizing performance on
several autotuning domains.  During the beginning of my PhD at the University of
São Paulo (USP), I have published a paper on optimizing the configuration of the
CUDA compiler\nbsp{}\cite{bruel2017autotuning},  where we have  reached up to  4 times
performance improvement  in comparison with a  high-level compiler optimization.
In collaboration with researchers from  Hewlett Packard Enterprise (HPE) in Palo
Alto, I wrote a  paper on the autotuning of a  compiler for High-Level Synthesis
for FPGAs\nbsp{}\cite{bruel2017autotuninghls}, where we  have reached, on average, 25%
improvements on  performance, size, and  complexity of  designs.

At the  end of 2017,  I joined  the /cotutelle/ PhD  program at the  University of
Grenoble Alpes  (UGA) and  became a member  of the POLARIS  Inria team,  where I
applied  Design  of   Experiments  to  the  autotuning   of  a  source-to-source
transformation  compiler\nbsp{}\cite{bruel2019autotuning},  where  we  showed  we  can
achieve significant speedup by exploiting  search space structure using a strict
budget.   I also  have  collaborated with  HPE on  another  paper, providing  an
analysis  of the  applicability  of autotuning  methods  to a  Hardware-Software
Co-design  problem\nbsp{}\cite{bruel2017generalize}.   During  my  Teaching  Assistant
internships,  I  have  published one  paper\nbsp{}\cite{bruel2017openmp}  on  parallel
programming  teaching, and  collaborated on  another\nbsp{}\cite{goncalves2016openmp},
where we showed that teaching lower level programming models, despite being more
challenging at first, provides a stronger core understanding.

I continue to collaborate with HPE  researchers on the application of autotuning
methods to  optimize Neural Networks,  hardware accelerators for  Deep Learning,
and  algorithms  for dealing  with  network  congestion.   With my  advisors,  I
currently manage  1 undergraduate and 4  masters students, who are  applying the
statistical  learning autotuning  methods  I studied  and  adapted to  different
domains  in the  context of  a joint  USP/HPE research  project.  I  am strongly
motivated to continue pursuing a career  on Computer Science research, aiming to
produce  rigorous and  value-adding  contributions. I  hereby  submit my  thesis
proposal and application to the Microsoft Latin America PhD Award.
#+begin_export latex
\begin{center}
  \begin{figure}[t]
    \resizebox{.9\textwidth}{!}{%
      \begin{forest}
        for tree={%
          anchor = north,
          align = center,
          l sep+=1em
        },
        [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
          draw,
          [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
            draw,
            color = NavyBlue
            [{Search Heuristics},
              draw,
              color = BurntOrange,
              edge label = {node[midway, fill=white, font = \scriptsize]{No}}
              [{\textbf{Random} \textbf{Sampling}}, draw]
              [{Reachable Optima},
                draw,
                color = BurntOrange
                [, phantom]
                [{Underlying Hypotheses \\ \textbf{Heuristics}}, draw]]]
            [{Statistical Learning},
              draw,
              color = BurntOrange,
              edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
              [{Parametric Learning},
                draw,
                color = BurntOrange
                [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                  draw,
                  color = BurntOrange
                  [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                  [, phantom]]
                [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                  draw,
                  color = BurntOrange
                  [, phantom]
                  [{Check for model adequacy?},
                    draw,
                    alias = adequacy,
                    color = NavyBlue
                    [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                      draw,
                      alias = interactions,
                      color = NavyBlue,
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                      [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                        draw
                        [, phantom]
                        [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                          edge = {-stealth, ForestGreen, semithick},
                          edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                          draw,
                          alias = estimate,
                          color = ForestGreen]]
                      [{\textbf{Optimal} \textbf{Design}},
                        draw,
                        alias = optimal,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [{\textbf{Space-filling} \textbf{Designs}},
                      draw,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                      [, phantom]
                      [{Model selection},
                        edge = {-stealth, ForestGreen, semithick},
                        edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                        draw,
                        alias = selection,
                        color = ForestGreen]]]]]
              [{Nonparametric Learning},
                draw,
                color = BurntOrange
                [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
                [{\textbf{Gaussian} \textbf{Process Regression}},
                  alias = gaussian,
                  draw]
                [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
        \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
        \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
        \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
        \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
      \end{forest}
    }
    \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
      denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
      denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
      highlight exploration and exploitation choices, and \textbf{bold} boxes denote methods.}
    \label{fig:tree}
  \end{figure}
\end{center}
#+end_export

#+latex: \newpage

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
*** (Short) Search Heuristics and Statistical Learning methods for Autotuning HPC Programs
:PROPERTIES:
:EXPORT_DATE:
:EXPORT_TITLE: @@latex: Search Heuristics and Statistical Learning \\ Methods for Program Autotuning@@
:EXPORT_FILE_NAME: short-application.pdf
:EXPORT_AUTHOR: Pedro Bruel
:END:

#+latex: \vspace{-3em}

High Performance Computing  has been a cornerstone of  collective scientific and
industrial progress  for at least  five decades.   Paying the cost  of increased
complexity,  software and  hardware  engineering advances  continue to  overcome
several challenges on the way of the sustained performance improvements observed
during the last  fifty years.  This mounting complexity means  that reaching the
advertised hardware  performance for  a given program  requires not  only expert
knowledge  of a  given hardware  architecture, but  also mastery  of programming
models  and  languages for  parallel  and  distributed  computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance,  we  can  draw   and  adapt  proven  methods  from  search,
mathematical  optimization and  statistics. The  effectiveness of  these adapted
methods  on autotuning  problems varies  greatly,  and hinges  on practical  and
mathematical properties of the problem and the corresponding /search space/.

When  adapting methods  for autotuning,  we must  face challenges  emerging from
practical properties  such as restricted  time and cost budgets,  constraints on
feasible  parameter values,  and the  need to  mix /categorical/,  /continuous/, and
/discrete/ parameters. To achieve useful results, we must also choose methods that
make hypotheses compatible with problem search  spaces, such as the existence of
discoverable,  or at  least  exploitable, relationships  between parameters  and
performance.   Choosing  an autotuning  method  requires  determining a  balance
between the exploration of a problem, when we would seek to discover and explain
relationships between  parameters and performance,  and the exploitation  of the
best optimizations we can find, when we would seek only to minimize performance.

During this  thesis I have  adapted and  studied the effectiveness  of different
search heuristics and statistical learning  methods on optimizing performance on
several autotuning domains.  During the beginning of my PhD at the University of
São Paulo (USP), I have published a paper on optimizing the configuration of the
CUDA compiler\nbsp{}\cite{bruel2017autotuning},  where we have  reached up to  4 times
performance improvement  in comparison with a  high-level compiler optimization.
In collaboration with researchers from  Hewlett Packard Enterprise (HPE) in Palo
Alto, I wrote a  paper on the autotuning of a  compiler for High-Level Synthesis
for FPGAs\nbsp{}\cite{bruel2017autotuninghls}, where we  have reached, on average, 25%
improvements on  performance, size, and  complexity of  designs.

At the  end of 2017,  I joined  the /cotutelle/ PhD  program at the  University of
Grenoble Alpes  (UGA) and  became a member  of the POLARIS  Inria team,  where I
applied  Design  of   Experiments  to  the  autotuning   of  a  source-to-source
transformation  compiler\nbsp{}\cite{bruel2019autotuning},  where  we  showed  we  can
achieve significant speedup by exploiting  search space structure using a strict
budget.   I also  have  collaborated with  HPE on  another  paper, providing  an
analysis  of the  applicability  of autotuning  methods  to a  Hardware-Software
Co-design  problem\nbsp{}\cite{bruel2017generalize}.

I continue to collaborate with HPE  researchers on the application of autotuning
methods to  optimize Neural Networks,  hardware accelerators for  Deep Learning,
and  algorithms  for dealing  with  network  congestion.   With my  advisors,  I
currently manage  1 undergraduate and 4  masters students, who are  applying the
statistical  learning autotuning  methods  I studied  and  adapted to  different
domains  in the  context of  a joint  USP/HPE research  project.  I  am strongly
motivated to continue pursuing a career  on Computer Science research, aiming to
produce  rigorous and  value-adding  contributions. I  hereby  submit my  thesis
proposal and application to the Microsoft Latin America PhD Award.

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
* Generating Figures                                               :noexport:
** Historical Trends
*** Introduction
**** 49 Years of Processor Data
***** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)
df_freq <- read.csv("data/wiki_data/frequency.csv", header = TRUE)
df_transistor <- read.csv("data/wiki_data/transistor_count.csv", header = TRUE)
#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_freq)
#+end_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  12 variables:
 $ date               : int  1971 1972 1972 1972 1972 1973 1973 1973 1974 1974 ...
 $ name               : chr  "4004" "PPS-25" "μPD700" "8008" ...
 $ designer           : chr  "Intel" "Fairchild" "NEC" "Intel" ...
 $ max_clock_khz      : int  740 400 NA 500 200 NA NA NA 715 NA ...
 $ max_clock_mhz      : num  NA NA NA NA NA 2 1 1 NA 2 ...
 $ max_clock_ghz      : num  NA NA NA NA NA NA NA NA NA NA ...
 $ process_micro_m    : num  10 NA NA 10 NA 7.5 6 NA NA 6 ...
 $ process_nm         : int  NA NA NA NA NA NA NA NA NA NA ...
 $ chips              : int  1 2 1 1 1 1 1 1 3 1 ...
 $ transistor_count   : int  2250 NA NA 3500 NA 2500 2800 NA NA 6000 ...
 $ transistor_millions: num  NA NA NA NA NA NA NA NA NA NA ...
 $ logical_cores      : int  1 1 1 1 1 1 1 1 1 1 ...
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_transistor)
#+end_SRC

#+RESULTS:
: 'data.frame':	151 obs. of  6 variables:
:  $ name            : chr  "Intel 4004 " "Intel 8008 " "Toshiba TLCS-12 " "Intel 4040 " ...
:  $ transistor_count: num  2250 3500 11000 3000 4100 ...
:  $ date            : int  1971 1972 1973 1974 1974 1974 1974 1975 1976 1976 ...
:  $ designer        : chr  "Intel" "Intel" "Toshiba" "Intel" ...
:  $ process_nm      : int  10000 10000 6000 10000 6000 6000 8000 8000 5000 4000 ...
:  $ area_mm         : num  12 14 32 12 16 20 11 21 27 18 ...

***** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/49_years_processor_data.pdf" :width 10 :height 5 :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")

point_alpha = 0.9
line_alpha = 0.4
point_size = 2

shapes = c(0, 1, 2, 5)

ggplot() +
    # geom_line(data = df_transistor,
    #           size = point_size,
    #           stat = "smooth",
    #           method = "lm",
    #           alpha = line_alpha,
    #           formula = y ~ x + I(x ^ 2),
    #           aes(x = date,
    #               y = process_nm,
    #               color = "Process (nanometers)"),
    #           show.legend = FALSE) +
    # geom_line(data = df_freq,
    #           size = point_size,
    #           stat = "smooth",
    #           method = "lm",
    #           alpha = line_alpha,
    #           formula = y ~ x + I(x ^ 2) + I(x ^ 3),
    #           aes(x = date,
    #               y = logical_cores,
    #               shape = "Logical Cores (Count)",
    #               color = "Logical Cores (Count)"),
    #           show.legend = FALSE) +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_micro_m * 1e3,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = logical_cores,
                   shape = "Logical Cores (Count)",
                   color = "Logical Cores (Count)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_khz * 1e-3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_mhz,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_ghz * 1e3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_millions * 1e3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    xlab("Year") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 18) +
    theme(axis.title.y = element_blank(),
          legend.position = c(0.14, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 14),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/49_years_processor_data.pdf]]
**** TOP500
***** Loading Data and Packages
Load the /csv/:

#+begin_SRC R :results output :session *R* :exports code :eval no-export
library(dplyr)
library(tidyr)
library(ggplot2)

df <- read.csv("./data/top500/TOP500_history.csv")
#+end_SRC

#+RESULTS:
***** Looking at Data
****** Column Names
We  have many  columns  filled with  `NA`s,  due to  how  metrics were  measured
differently over the years. There's data from 1993 to 2019!

#+begin_SRC R :results output :session *R* :exports both :eval no-export
names(df)
#+end_SRC

#+RESULTS:
#+begin_example
 [1] "Year"                            "Month"
 [3] "Day"                             "Rank"
 [5] "Site"                            "Manufacturer"
 [7] "Computer"                        "Country"
 [9] "Processors"                      "RMax"
[11] "RPeak"                           "Nmax"
[13] "Nhalf"                           "Processor.Family"
[15] "Processor"                       "Processor.Speed..MHz."
[17] "System.Family"                   "Operating.System"
[19] "Architecture"                    "Segment"
[21] "Application.Area"                "Interconnect.Family"
[23] "Interconnect"                    "Region"
[25] "Continent"                       "Power"
[27] "System.Model"                    "Total.Cores"
[29] "Measured.Size"                   "Processor.Cores"
[31] "Accelerator"                     "Name"
[33] "Accelerator.Cores"               "Efficiency...."
[35] "Mflops.Watt"                     "Processor.Technology"
[37] "OS.Family"                       "Cores.per.Socket"
[39] "Processor.Generation"            "Previous.Rank"
[41] "First.Appearance"                "First.Rank"
[43] "Accelerator.Co.Processor.Cores"  "Accelerator.Co.Processor"
[45] "Power.Source"                    "Rmax..TFlop.s."
[47] "Rpeak..TFlop.s."                 "HPCG..TFlop.s."
[49] "Power..kW."                      "Power.Effeciency..GFlops.Watts."
[51] "Site.ID"                         "System.ID"
#+end_example

****** Achieved and Theoretical Performance
#+begin_SRC R :results graphics output :session *R* :file "./img/top500_rmax_rpeak.pdf" :width 10 :height 5 :exports both :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")

point_size = 2.8
shapes = c(0, 1, 2, 5)

plot_df <- df %>%
    filter(Rank <= 1) %>%
    mutate(RMaxT = coalesce(RMax / 1e3, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak / 1e3, Rpeak..TFlop.s.),
           Power = coalesce(Power, Power..kW.)) %>%
    select(Rank,
           Year,
           Power,
           RMaxT,
           RPeakT) %>%
    distinct(Rank, Year, .keep_all = TRUE) %>%
    mutate(Ratio = RMaxT / RPeakT) %>%
    filter(is.finite(Ratio) & Ratio <= 1.0)

ggplot() +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RMaxT,
                   shape = "RMax",
                   color = "RMax")) +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RPeakT,
                   shape = "RPeak",
                   color = "RPeak")) +
    # geom_point(data = plot_df,
    #            size = point_size,
    #            aes(x = Year,
    #                y = Power,
    #                shape = "Power (kW)",
    #                color = "Power (kW)")) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    ylab("Tflops/s") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 20) +
    theme(legend.position = c(0.06, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 16),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/top500_rmax_rpeak.pdf]]

****** Accelerator Core Count
#+begin_SRC R :results graphics output :session *R* :file "./img/top500_accelerator_cores.pdf" :width 10 :height 5 :exports both :eval no-export
library(ggplot2)
library(extrafont)
library(scales)
library(tidyr)

loadfonts(device = "postscript")

point_size = 2.8
shapes = c(0, 1, 2, 5)

plot_df <- df %>%
    filter(Rank <= 1) %>%
    mutate(Accelerators = na_if(Accelerator.Co.Processor.Cores, 0),
           Cores = coalesce(Processors, Total.Cores) -
               replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    select(Rank,
           Year,
           Accelerators,
           Cores) %>%
    distinct(Rank, Year, .keep_all = TRUE)

ggplot() +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = Cores,
                   shape = "Processor",
                   color = "Processor")) +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = Accelerators,
                   shape = "Accelerator",
                   color = "Accelerator")) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    ylab("Cores") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 20) +
    theme(legend.position = c(0.09, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 16),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/top500_accelerator_cores.pdf]]
****** Other Plots
******* Processor Clock
Supercomputer  clock  explosion  and  range  broadening.  Even  top-tier  clocks
stagnate after 2008.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_processors_clock.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Processor.Speed..MHz. / 1000,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
                                        #scale_y_log10() +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Processor Clock (GHz)") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_processors_clock.pdf]]

******* Processors
Core count sustained  exponential increase.  Although top-tier  core count still
increases, range  broadening around  2012 can be  explained by  introduction and
ubiquity of accelerator cores on all tiers.

#+begin_SRC R :results graphics output :session *R* :file "./img/top500_total_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)
library(tidyr)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores) - replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    select(Rank, Year, AllCores, Accelerator.Co.Processor.Cores) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("AllCores",
                                    "Accelerator.Co.Processor.Cores"),
                         labels = c("Processor Cores",
                                    "Accelerator Cores"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Core Count") +
    scale_y_log10() +
    # annotation_logticks(sides = "l") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.67, 0.08),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 4)
#+end_SRC

#+RESULTS:
[[file:./img/top500_total_cores.pdf]]
******* RPeak and RMax
Sustained increase of theoretical peak and  achieved max performance on HPL and,
most recently,  on the  HPCG benchmark.  RPeak does not  guarantee rank  on some
cases.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rpeak.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(RMax = RMax / 1e3,
           RPeak = RPeak / 1e3,
           RMaxT = coalesce(RMax, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak, Rpeak..TFlop.s.)) %>%
    select(Rank,
           Year,
           RMaxT,
           RPeakT,
           HPCG..TFlop.s.) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakT",
                                    "RMaxT",
                                    "HPCG..TFlop.s."),
                         labels = c("RPeak (HPL)",
                                    "RMax (HPL)",
                                    "RMax (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance (TFlops/s)") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.83, 0.09),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 3)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rpeak.pdf]]
******* RMax / Cores
Ratio of performance and core count, for HPL and HPCG. Is this sustained increase due only to accelerator cores, or are there other engineering and software advances?
#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rmax_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores)) %>%
    mutate(RMax = (RMax / 1e3) / AllCores,
           RPeak = (RPeak / 1e3) / AllCores,
           Rmax..TFlop.s. = Rmax..TFlop.s. / AllCores,
           Rpeak..TFlop.s. = Rpeak..TFlop.s. / AllCores,
           RMaxC = coalesce(RMax, Rmax..TFlop.s.),
           RPeakC = coalesce(RPeak, Rpeak..TFlop.s.),
           HPCGC = HPCG..TFlop.s. / AllCores) %>%
    select(Rank,
           Year,
           RMaxC,
           RPeakC,
           HPCGC) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakC",
                                    "RMaxC",
                                    "HPCGC"),
                         labels = c("RPeak / Cores (HPL)",
                                    "RMax / Cores (HPL)",
                                    "RMax / Cores (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance / Core Count") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.85, 0.1),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          strip.text.x = element_text(size = 28),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 5)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rmax_cores.pdf]]

******* NMax
Exponential increase of problem size to reach max performance. Why is there
range broadening after 2011?

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_nmax.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Nmax,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Problem Size to Reach RMax") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_nmax.pdf]]

**** Search Spaces
***** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(extrafont)

df_search_spaces <- read.csv("data/search_spaces/search_spaces.csv")

loadfonts(device = "postscript")
#+end_SRC

#+RESULTS:
#+begin_example

Akaash already registered with postscriptFonts().
AkrutiMal1 already registered with postscriptFonts().
AkrutiMal2 already registered with postscriptFonts().
AkrutiTml1 already registered with postscriptFonts().
AkrutiTml2 already registered with postscriptFonts().
Anonymice Powerline already registered with postscriptFonts().
Arimo for Powerline already registered with postscriptFonts().
Bitstream Vera Sans already registered with postscriptFonts().
Bitstream Vera Sans Mono already registered with postscriptFonts().
Bitstream Vera Serif already registered with postscriptFonts().
Cousine for Powerline already registered with postscriptFonts().
IBM 3270 already registered with postscriptFonts().
IBM 3270 Narrow already registered with postscriptFonts().
IBM 3270 Semi-Narrow already registered with postscriptFonts().
DejaVu Math TeX Gyre already registered with postscriptFonts().
DejaVu Sans already registered with postscriptFonts().
DejaVu Sans Light already registered with postscriptFonts().
DejaVu Sans Condensed already registered with postscriptFonts().
DejaVu Sans Mono already registered with postscriptFonts().
DejaVu Sans Mono for Powerline already registered with postscriptFonts().
DejaVu Serif already registered with postscriptFonts().
DejaVu Serif Condensed already registered with postscriptFonts().
Droid Arabic Kufi already registered with postscriptFonts().
Droid Arabic Naskh already registered with postscriptFonts().
Droid Naskh Shift Alt already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans. Skipping setup for this font.
Droid Sans Arabic already registered with postscriptFonts().
Droid Sans Armenian already registered with postscriptFonts().
Droid Sans Devanagari already registered with postscriptFonts().
Droid Sans Ethiopic already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Fallback. Skipping setup for this font.
Droid Sans Georgian already registered with postscriptFonts().
Droid Sans Hebrew already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Mono. Skipping setup for this font.
Droid Sans Mono Dotted for Powerline already registered with postscriptFonts().
Droid Sans Mono Slashed for Powerline already registered with postscriptFonts().
Droid Sans Tamil already registered with postscriptFonts().
Droid Sans Thai already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Serif. Skipping setup for this font.
Font Awesome 5 Brands Regular already registered with postscriptFonts().
Font Awesome 5 Free Regular already registered with postscriptFonts().
Font Awesome 5 Free Solid already registered with postscriptFonts().
Gargi-1.2b already registered with postscriptFonts().
Goha-Tibeb Zemen already registered with postscriptFonts().
Go Mono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for GurbaniBoliLite. Skipping setup for this font.
Hack already registered with postscriptFonts().
Inconsolata Black already registered with postscriptFonts().
Inconsolata already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Inconsolata for Powerline. Skipping setup for this font.
Inconsolata Condensed already registered with postscriptFonts().
Inconsolata Condensed Black already registered with postscriptFonts().
Inconsolata Condensed Bold already registered with postscriptFonts().
Inconsolata Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Condensed Light already registered with postscriptFonts().
Inconsolata Condensed Medium already registered with postscriptFonts().
Inconsolata Condensed SemiBold already registered with postscriptFonts().
Inconsolata Expanded already registered with postscriptFonts().
Inconsolata Expanded Black already registered with postscriptFonts().
Inconsolata Expanded Bold already registered with postscriptFonts().
Inconsolata Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Expanded Light already registered with postscriptFonts().
Inconsolata Expanded Medium already registered with postscriptFonts().
Inconsolata Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed already registered with postscriptFonts().
Inconsolata Extra Condensed Black already registered with postscriptFonts().
Inconsolata Extra Condensed Bold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Extra Condensed Light already registered with postscriptFonts().
Inconsolata Extra Condensed Medium already registered with postscriptFonts().
Inconsolata Extra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Extra Expanded already registered with postscriptFonts().
Inconsolata Extra Expanded Black already registered with postscriptFonts().
Inconsolata Extra Expanded Bold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Extra Expanded Light already registered with postscriptFonts().
Inconsolata Extra Expanded Medium already registered with postscriptFonts().
Inconsolata Extra Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraLight already registered with postscriptFonts().
Inconsolata Light already registered with postscriptFonts().
Inconsolata Medium already registered with postscriptFonts().
Inconsolata SemiBold already registered with postscriptFonts().
Inconsolata Semi Condensed already registered with postscriptFonts().
Inconsolata Semi Condensed Black already registered with postscriptFonts().
Inconsolata Semi Condensed Bold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Semi Condensed Light already registered with postscriptFonts().
Inconsolata Semi Condensed Medium already registered with postscriptFonts().
Inconsolata Semi Condensed SemiBold already registered with postscriptFonts().
Inconsolata Semi Expanded already registered with postscriptFonts().
Inconsolata Semi Expanded Black already registered with postscriptFonts().
Inconsolata Semi Expanded Bold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Semi Expanded Light already registered with postscriptFonts().
Inconsolata Semi Expanded Medium already registered with postscriptFonts().
Inconsolata Semi Expanded SemiBold already registered with postscriptFonts().
Inconsolata Ultra Condensed already registered with postscriptFonts().
Inconsolata Ultra Condensed Black already registered with postscriptFonts().
Inconsolata Ultra Condensed Bold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Condensed Light already registered with postscriptFonts().
Inconsolata Ultra Condensed Medium already registered with postscriptFonts().
Inconsolata Ultra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Ultra Expanded already registered with postscriptFonts().
Inconsolata Ultra Expanded Black already registered with postscriptFonts().
Inconsolata Ultra Expanded Bold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Expanded Light already registered with postscriptFonts().
Inconsolata Ultra Expanded Medium already registered with postscriptFonts().
Inconsolata Ultra Expanded SemiBold already registered with postscriptFonts().
Liberation Mono already registered with postscriptFonts().
Liberation Sans already registered with postscriptFonts().
Liberation Serif already registered with postscriptFonts().
Ligconsolata already registered with postscriptFonts().
Likhan already registered with postscriptFonts().
Literation Mono Powerline already registered with postscriptFonts().
malayalam already registered with postscriptFonts().
MalOtf already registered with postscriptFonts().
Meslo LG L DZ for Powerline already registered with postscriptFonts().
Meslo LG L for Powerline already registered with postscriptFonts().
Meslo LG M DZ for Powerline already registered with postscriptFonts().
Meslo LG M for Powerline already registered with postscriptFonts().
Meslo LG S DZ for Powerline already registered with postscriptFonts().
Meslo LG S for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for monofur for Powerline. Skipping setup for this font.
More than one version of regular/bold/italic found for Mukti Narrow. Skipping setup for this font.
Noto Kufi Arabic already registered with postscriptFonts().
Noto Kufi Arabic Medium already registered with postscriptFonts().
Noto Kufi Arabic Semi bold already registered with postscriptFonts().
Noto Mono for Powerline already registered with postscriptFonts().
Noto Music already registered with postscriptFonts().
Noto Naskh Arabic already registered with postscriptFonts().
Noto Naskh Arabic UI already registered with postscriptFonts().
Noto Nastaliq Urdu already registered with postscriptFonts().
Noto Sans Black already registered with postscriptFonts().
Noto Sans already registered with postscriptFonts().
Noto Sans Light already registered with postscriptFonts().
Noto Sans Medium already registered with postscriptFonts().
Noto Sans Thin already registered with postscriptFonts().
Noto Sans Adlam already registered with postscriptFonts().
Noto Sans Adlam Unjoined already registered with postscriptFonts().
Noto Sans AnatoHiero already registered with postscriptFonts().
Noto Sans Arabic Blk already registered with postscriptFonts().
Noto Sans Arabic already registered with postscriptFonts().
Noto Sans Arabic Light already registered with postscriptFonts().
Noto Sans Arabic Med already registered with postscriptFonts().
Noto Sans Arabic Thin already registered with postscriptFonts().
Noto Sans Arabic UI Bk already registered with postscriptFonts().
Noto Sans Arabic UI already registered with postscriptFonts().
Noto Sans Arabic UI Lt already registered with postscriptFonts().
Noto Sans Arabic UI Md already registered with postscriptFonts().
Noto Sans Arabic UI Th already registered with postscriptFonts().
Noto Sans Armenian Blk already registered with postscriptFonts().
Noto Sans Armenian already registered with postscriptFonts().
Noto Sans Armenian Light already registered with postscriptFonts().
Noto Sans Armenian Med already registered with postscriptFonts().
Noto Sans Armenian Thin already registered with postscriptFonts().
Noto Sans Avestan already registered with postscriptFonts().
Noto Sans Bamum already registered with postscriptFonts().
Noto Sans Bassa Vah already registered with postscriptFonts().
Noto Sans Batak already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Blk. Skipping setup for this font.
Noto Sans Bengali already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Thin. Skipping setup for this font.
Noto Sans Bengali UI already registered with postscriptFonts().
Noto Sans Bhaiksuki already registered with postscriptFonts().
Noto Sans Brahmi already registered with postscriptFonts().
Noto Sans Buginese already registered with postscriptFonts().
Noto Sans Buhid already registered with postscriptFonts().
Noto Sans CanAborig Bk already registered with postscriptFonts().
Noto Sans CanAborig already registered with postscriptFonts().
Noto Sans CanAborig Lt already registered with postscriptFonts().
Noto Sans CanAborig Md already registered with postscriptFonts().
Noto Sans CanAborig Th already registered with postscriptFonts().
Noto Sans Carian already registered with postscriptFonts().
Noto Sans CaucAlban already registered with postscriptFonts().
Noto Sans Chakma already registered with postscriptFonts().
Noto Sans Cham Blk already registered with postscriptFonts().
Noto Sans Cham already registered with postscriptFonts().
Noto Sans Cham Light already registered with postscriptFonts().
Noto Sans Cham Med already registered with postscriptFonts().
Noto Sans Cham Thin already registered with postscriptFonts().
Noto Sans Cherokee Blk already registered with postscriptFonts().
Noto Sans Cherokee already registered with postscriptFonts().
Noto Sans Cherokee Light already registered with postscriptFonts().
Noto Sans Cherokee Med already registered with postscriptFonts().
Noto Sans Cherokee Thin already registered with postscriptFonts().
Noto Sans Coptic already registered with postscriptFonts().
Noto Sans Cuneiform already registered with postscriptFonts().
Noto Sans Cypriot already registered with postscriptFonts().
Noto Sans Deseret already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Bk. Skipping setup for this font.
Noto Sans Devanagari already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Lt. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Md. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Th. Skipping setup for this font.
Noto Sans Devanagari UI already registered with postscriptFonts().
Noto Sans Display Black already registered with postscriptFonts().
Noto Sans Display already registered with postscriptFonts().
Noto Sans Display Light already registered with postscriptFonts().
Noto Sans Display Medium already registered with postscriptFonts().
Noto Sans Display Thin already registered with postscriptFonts().
Noto Sans Duployan already registered with postscriptFonts().
Noto Sans EgyptHiero already registered with postscriptFonts().
Noto Sans Elbasan already registered with postscriptFonts().
Noto Sans Ethiopic Blk already registered with postscriptFonts().
Noto Sans Ethiopic already registered with postscriptFonts().
Noto Sans Ethiopic Light already registered with postscriptFonts().
Noto Sans Ethiopic Med already registered with postscriptFonts().
Noto Sans Ethiopic Thin already registered with postscriptFonts().
Noto Sans Georgian Blk already registered with postscriptFonts().
Noto Sans Georgian already registered with postscriptFonts().
Noto Sans Georgian Light already registered with postscriptFonts().
Noto Sans Georgian Med already registered with postscriptFonts().
Noto Sans Georgian Thin already registered with postscriptFonts().
Noto Sans Glagolitic already registered with postscriptFonts().
Noto Sans Gothic already registered with postscriptFonts().
Noto Sans Grantha already registered with postscriptFonts().
Noto Sans Gujarati already registered with postscriptFonts().
Noto Sans Gujarati UI already registered with postscriptFonts().
Noto Sans Gurmukhi Black already registered with postscriptFonts().
Noto Sans Gurmukhi already registered with postscriptFonts().
Noto Sans Gurmukhi Light already registered with postscriptFonts().
Noto Sans Gurmukhi Medium already registered with postscriptFonts().
Noto Sans Gurmukhi Thin already registered with postscriptFonts().
Noto Sans Gurmukhi UI Black already registered with postscriptFonts().
Noto Sans Gurmukhi UI already registered with postscriptFonts().
Noto Sans Gurmukhi UI Light already registered with postscriptFonts().
Noto Sans Gurmukhi UI Medium already registered with postscriptFonts().
Noto Sans Gurmukhi UI Thin already registered with postscriptFonts().
Noto Sans HanifiRohg already registered with postscriptFonts().
Noto Sans Hanunoo already registered with postscriptFonts().
Noto Sans Hatran already registered with postscriptFonts().
Noto Sans Hebrew Blk already registered with postscriptFonts().
Noto Sans Hebrew already registered with postscriptFonts().
Noto Sans Hebrew Light already registered with postscriptFonts().
Noto Sans Hebrew Med already registered with postscriptFonts().
Noto Sans Hebrew Thin already registered with postscriptFonts().
Noto Sans ImpAramaic already registered with postscriptFonts().
Noto Sans Indic Siyaq Numbers already registered with postscriptFonts().
Noto Sans InsPahlavi already registered with postscriptFonts().
Noto Sans InsParthi already registered with postscriptFonts().
Noto Sans Javanese already registered with postscriptFonts().
Noto Sans Kaithi already registered with postscriptFonts().
Noto Sans Kannada Black already registered with postscriptFonts().
Noto Sans Kannada already registered with postscriptFonts().
Noto Sans Kannada Light already registered with postscriptFonts().
Noto Sans Kannada Medium already registered with postscriptFonts().
Noto Sans Kannada Thin already registered with postscriptFonts().
Noto Sans Kannada UI Black already registered with postscriptFonts().
Noto Sans Kannada UI already registered with postscriptFonts().
Noto Sans Kannada UI Light already registered with postscriptFonts().
Noto Sans Kannada UI Medium already registered with postscriptFonts().
Noto Sans Kannada UI Thin already registered with postscriptFonts().
Noto Sans Kayah Li already registered with postscriptFonts().
Noto Sans Kharoshthi already registered with postscriptFonts().
Noto Sans Khmer Black already registered with postscriptFonts().
Noto Sans Khmer already registered with postscriptFonts().
Noto Sans Khmer Light already registered with postscriptFonts().
Noto Sans Khmer Medium already registered with postscriptFonts().
Noto Sans Khmer Thin already registered with postscriptFonts().
Noto Sans Khmer UI Black already registered with postscriptFonts().
Noto Sans Khmer UI already registered with postscriptFonts().
Noto Sans Khmer UI Light already registered with postscriptFonts().
Noto Sans Khmer UI Medium already registered with postscriptFonts().
Noto Sans Khmer UI Thin already registered with postscriptFonts().
Noto Sans Khojki already registered with postscriptFonts().
Noto Sans Khudawadi already registered with postscriptFonts().
Noto Sans Lao Blk already registered with postscriptFonts().
Noto Sans Lao already registered with postscriptFonts().
Noto Sans Lao Light already registered with postscriptFonts().
Noto Sans Lao Med already registered with postscriptFonts().
Noto Sans Lao Thin already registered with postscriptFonts().
Noto Sans Lao UI Blk already registered with postscriptFonts().
Noto Sans Lao UI already registered with postscriptFonts().
Noto Sans Lao UI Light already registered with postscriptFonts().
Noto Sans Lao UI Med already registered with postscriptFonts().
Noto Sans Lao UI Thin already registered with postscriptFonts().
Noto Sans Lepcha already registered with postscriptFonts().
Noto Sans Limbu already registered with postscriptFonts().
Noto Sans Linear A already registered with postscriptFonts().
Noto Sans Linear B already registered with postscriptFonts().
Noto Sans Lisu already registered with postscriptFonts().
Noto Sans Lycian already registered with postscriptFonts().
Noto Sans Lydian already registered with postscriptFonts().
Noto Sans Mahajani already registered with postscriptFonts().
Noto Sans Malayalam Black already registered with postscriptFonts().
Noto Sans Malayalam already registered with postscriptFonts().
Noto Sans Malayalam Light already registered with postscriptFonts().
Noto Sans Malayalam Medium already registered with postscriptFonts().
Noto Sans Malayalam Thin already registered with postscriptFonts().
Noto Sans Malayalam UI Black already registered with postscriptFonts().
Noto Sans Malayalam UI already registered with postscriptFonts().
Noto Sans Malayalam UI Light already registered with postscriptFonts().
Noto Sans Malayalam UI Medium already registered with postscriptFonts().
Noto Sans Malayalam UI Thin already registered with postscriptFonts().
Noto Sans Mandaic already registered with postscriptFonts().
Noto Sans Manichaean already registered with postscriptFonts().
Noto Sans Marchen already registered with postscriptFonts().
Noto Sans Math already registered with postscriptFonts().
Noto Sans Mayan Numerals already registered with postscriptFonts().
Noto Sans MeeteiMayek already registered with postscriptFonts().
Noto Sans Mende Kikakui already registered with postscriptFonts().
Noto Sans Meroitic already registered with postscriptFonts().
Noto Sans Miao already registered with postscriptFonts().
Noto Sans Modi already registered with postscriptFonts().
Noto Sans Mongolian already registered with postscriptFonts().
Noto Sans Mono Black already registered with postscriptFonts().
Noto Sans Mono already registered with postscriptFonts().
Noto Sans Mono Light already registered with postscriptFonts().
Noto Sans Mono Medium already registered with postscriptFonts().
Noto Sans Mono Thin already registered with postscriptFonts().
Noto Sans Mro already registered with postscriptFonts().
Noto Sans Multani already registered with postscriptFonts().
Noto Sans Myanmar Blk already registered with postscriptFonts().
Noto Sans Myanmar already registered with postscriptFonts().
Noto Sans Myanmar Light already registered with postscriptFonts().
Noto Sans Myanmar Med already registered with postscriptFonts().
Noto Sans Myanmar Thin already registered with postscriptFonts().
Noto Sans Myanmar UI Black already registered with postscriptFonts().
Noto Sans Myanmar UI already registered with postscriptFonts().
Noto Sans Myanmar UI Light already registered with postscriptFonts().
Noto Sans Myanmar UI Medium already registered with postscriptFonts().
Noto Sans Myanmar UI Thin already registered with postscriptFonts().
Noto Sans Nabataean already registered with postscriptFonts().
Noto Sans Newa already registered with postscriptFonts().
Noto Sans NewTaiLue already registered with postscriptFonts().
Noto Sans N'Ko already registered with postscriptFonts().
Noto Sans Ogham already registered with postscriptFonts().
Noto Sans Ol Chiki already registered with postscriptFonts().
Noto Sans OldHung already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Noto Sans Old Italic. Skipping setup for this font.
Noto Sans OldNorArab already registered with postscriptFonts().
Noto Sans Old Permic already registered with postscriptFonts().
Noto Sans OldPersian already registered with postscriptFonts().
Noto Sans OldSogdian already registered with postscriptFonts().
Noto Sans OldSouArab already registered with postscriptFonts().
Noto Sans Old Turkic already registered with postscriptFonts().
Noto Sans Oriya already registered with postscriptFonts().
Noto Sans Oriya UI already registered with postscriptFonts().
Noto Sans Osage already registered with postscriptFonts().
Noto Sans Osmanya already registered with postscriptFonts().
Noto Sans Pahawh Hmong already registered with postscriptFonts().
Noto Sans Palmyrene already registered with postscriptFonts().
Noto Sans PauCinHau already registered with postscriptFonts().
Noto Sans PhagsPa already registered with postscriptFonts().
Noto Sans Phoenician already registered with postscriptFonts().
Noto Sans PsaPahlavi already registered with postscriptFonts().
Noto Sans Rejang already registered with postscriptFonts().
Noto Sans Runic already registered with postscriptFonts().
Noto Sans Samaritan already registered with postscriptFonts().
Noto Sans Saurashtra already registered with postscriptFonts().
Noto Sans Sharada already registered with postscriptFonts().
Noto Sans Shavian already registered with postscriptFonts().
Noto Sans Siddham already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Blk. Skipping setup for this font.
Noto Sans Sinhala already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Thin. Skipping setup for this font.
Noto Sans Sinhala UI already registered with postscriptFonts().
Noto Sans SoraSomp already registered with postscriptFonts().
Noto Sans Sundanese already registered with postscriptFonts().
Noto Sans Syloti Nagri already registered with postscriptFonts().
Noto Sans Symbols Blk already registered with postscriptFonts().
Noto Sans Symbols already registered with postscriptFonts().
Noto Sans Symbols Light already registered with postscriptFonts().
Noto Sans Symbols Med already registered with postscriptFonts().
Noto Sans Symbols Thin already registered with postscriptFonts().
Noto Sans Symbols2 already registered with postscriptFonts().
Noto Sans Syriac Black already registered with postscriptFonts().
Noto Sans Syriac already registered with postscriptFonts().
Noto Sans Syriac Thin already registered with postscriptFonts().
Noto Sans Tagalog already registered with postscriptFonts().
Noto Sans Tagbanwa already registered with postscriptFonts().
Noto Sans Tai Le already registered with postscriptFonts().
Noto Sans Tai Tham already registered with postscriptFonts().
Noto Sans Tai Viet already registered with postscriptFonts().
Noto Sans Takri already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Blk. Skipping setup for this font.
Noto Sans Tamil already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Thin. Skipping setup for this font.
Noto Sans Tamil Supplement already registered with postscriptFonts().
Noto Sans Tamil UI already registered with postscriptFonts().
Noto Sans Telugu Black already registered with postscriptFonts().
Noto Sans Telugu already registered with postscriptFonts().
Noto Sans Telugu Light already registered with postscriptFonts().
Noto Sans Telugu Medium already registered with postscriptFonts().
Noto Sans Telugu Thin already registered with postscriptFonts().
Noto Sans Telugu UI Black already registered with postscriptFonts().
Noto Sans Telugu UI already registered with postscriptFonts().
Noto Sans Telugu UI Light already registered with postscriptFonts().
Noto Sans Telugu UI Medium already registered with postscriptFonts().
Noto Sans Telugu UI Thin already registered with postscriptFonts().
Noto Sans Thaana Black already registered with postscriptFonts().
Noto Sans Thaana already registered with postscriptFonts().
Noto Sans Thaana Light already registered with postscriptFonts().
Noto Sans Thaana Medium already registered with postscriptFonts().
Noto Sans Thaana Thin already registered with postscriptFonts().
Noto Sans Thai Blk already registered with postscriptFonts().
Noto Sans Thai already registered with postscriptFonts().
Noto Sans Thai Light already registered with postscriptFonts().
Noto Sans Thai Med already registered with postscriptFonts().
Noto Sans Thai Thin already registered with postscriptFonts().
Noto Sans Thai UI Blk already registered with postscriptFonts().
Noto Sans Thai UI already registered with postscriptFonts().
Noto Sans Thai UI Light already registered with postscriptFonts().
Noto Sans Thai UI Med already registered with postscriptFonts().
Noto Sans Thai UI Thin already registered with postscriptFonts().
Noto Sans Tibetan already registered with postscriptFonts().
Noto Sans Tifinagh already registered with postscriptFonts().
Noto Sans Tirhuta already registered with postscriptFonts().
Noto Sans Ugaritic already registered with postscriptFonts().
Noto Sans Vai already registered with postscriptFonts().
Noto Sans WarangCiti already registered with postscriptFonts().
Noto Sans Yi already registered with postscriptFonts().
Noto Serif Black already registered with postscriptFonts().
Noto Serif already registered with postscriptFonts().
Noto Serif Light already registered with postscriptFonts().
Noto Serif Medium already registered with postscriptFonts().
Noto Serif Thin already registered with postscriptFonts().
Noto Serif Ahom already registered with postscriptFonts().
Noto Serif Armenian Bk already registered with postscriptFonts().
Noto Serif Armenian already registered with postscriptFonts().
Noto Serif Armenian Lt already registered with postscriptFonts().
Noto Serif Armenian Md already registered with postscriptFonts().
Noto Serif Armenian Th already registered with postscriptFonts().
Noto Serif Balinese already registered with postscriptFonts().
Noto Serif Bengali Black already registered with postscriptFonts().
Noto Serif Bengali already registered with postscriptFonts().
Noto Serif Bengali Light already registered with postscriptFonts().
Noto Serif Bengali Medium already registered with postscriptFonts().
Noto Serif Bengali Thin already registered with postscriptFonts().
Noto Serif Devanagari Black already registered with postscriptFonts().
Noto Serif Devanagari already registered with postscriptFonts().
Noto Serif Devanagari Light already registered with postscriptFonts().
Noto Serif Devanagari Medium already registered with postscriptFonts().
Noto Serif Devanagari Thin already registered with postscriptFonts().
Noto Serif Display Black already registered with postscriptFonts().
Noto Serif Display already registered with postscriptFonts().
Noto Serif Display Light already registered with postscriptFonts().
Noto Serif Display Medium already registered with postscriptFonts().
Noto Serif Display Thin already registered with postscriptFonts().
Noto Serif Dogra already registered with postscriptFonts().
Noto Serif Ethiopic Bk already registered with postscriptFonts().
Noto Serif Ethiopic already registered with postscriptFonts().
Noto Serif Ethiopic Lt already registered with postscriptFonts().
Noto Serif Ethiopic Md already registered with postscriptFonts().
Noto Serif Ethiopic Th already registered with postscriptFonts().
Noto Serif Georgian Bk already registered with postscriptFonts().
Noto Serif Georgian already registered with postscriptFonts().
Noto Serif Georgian Lt already registered with postscriptFonts().
Noto Serif Georgian Md already registered with postscriptFonts().
Noto Serif Georgian Th already registered with postscriptFonts().
Noto Serif Gujarati Black already registered with postscriptFonts().
Noto Serif Gujarati already registered with postscriptFonts().
Noto Serif Gujarati Light already registered with postscriptFonts().
Noto Serif Gujarati Medium already registered with postscriptFonts().
Noto Serif Gujarati Thin already registered with postscriptFonts().
Noto Serif Gurmukhi Black already registered with postscriptFonts().
Noto Serif Gurmukhi already registered with postscriptFonts().
Noto Serif Gurmukhi Light already registered with postscriptFonts().
Noto Serif Gurmukhi Medium already registered with postscriptFonts().
Noto Serif Gurmukhi Thin already registered with postscriptFonts().
Noto Serif Hebrew Blk already registered with postscriptFonts().
Noto Serif Hebrew already registered with postscriptFonts().
Noto Serif Hebrew Light already registered with postscriptFonts().
Noto Serif Hebrew Med already registered with postscriptFonts().
Noto Serif Hebrew Thin already registered with postscriptFonts().
Noto Serif Kannada Black already registered with postscriptFonts().
Noto Serif Kannada already registered with postscriptFonts().
Noto Serif Kannada Light already registered with postscriptFonts().
Noto Serif Kannada Medium already registered with postscriptFonts().
Noto Serif Kannada Thin already registered with postscriptFonts().
Noto Serif Khmer Black already registered with postscriptFonts().
Noto Serif Khmer already registered with postscriptFonts().
Noto Serif Khmer Light already registered with postscriptFonts().
Noto Serif Khmer Medium already registered with postscriptFonts().
Noto Serif Khmer Thin already registered with postscriptFonts().
Noto Serif Lao Blk already registered with postscriptFonts().
Noto Serif Lao already registered with postscriptFonts().
Noto Serif Lao Light already registered with postscriptFonts().
Noto Serif Lao Med already registered with postscriptFonts().
Noto Serif Lao Thin already registered with postscriptFonts().
Noto Serif Malayalam Black already registered with postscriptFonts().
Noto Serif Malayalam already registered with postscriptFonts().
Noto Serif Malayalam Light already registered with postscriptFonts().
Noto Serif Malayalam Medium already registered with postscriptFonts().
Noto Serif Malayalam Thin already registered with postscriptFonts().
Noto Serif Myanmar Blk already registered with postscriptFonts().
Noto Serif Myanmar already registered with postscriptFonts().
Noto Serif Myanmar Light already registered with postscriptFonts().
Noto Serif Myanmar Med already registered with postscriptFonts().
Noto Serif Myanmar Thin already registered with postscriptFonts().
Noto Serif Sinhala Black already registered with postscriptFonts().
Noto Serif Sinhala already registered with postscriptFonts().
Noto Serif Sinhala Light already registered with postscriptFonts().
Noto Serif Sinhala Medium already registered with postscriptFonts().
Noto Serif Sinhala Thin already registered with postscriptFonts().
Noto Serif Tamil Blk already registered with postscriptFonts().
Noto Serif Tamil already registered with postscriptFonts().
Noto Serif Tamil Light already registered with postscriptFonts().
Noto Serif Tamil Med already registered with postscriptFonts().
Noto Serif Tamil Thin already registered with postscriptFonts().
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Black. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Light. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Medium. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Thin. Skipping setup for this font.
Noto Serif Tangut already registered with postscriptFonts().
Noto Serif Telugu Black already registered with postscriptFonts().
Noto Serif Telugu already registered with postscriptFonts().
Noto Serif Telugu Light already registered with postscriptFonts().
Noto Serif Telugu Medium already registered with postscriptFonts().
Noto Serif Telugu Thin already registered with postscriptFonts().
Noto Serif Thai Blk already registered with postscriptFonts().
Noto Serif Thai already registered with postscriptFonts().
Noto Serif Thai Light already registered with postscriptFonts().
Noto Serif Thai Med already registered with postscriptFonts().
Noto Serif Thai Thin already registered with postscriptFonts().
Noto Serif Tibetan Black already registered with postscriptFonts().
Noto Serif Tibetan already registered with postscriptFonts().
Noto Serif Tibetan Light already registered with postscriptFonts().
Noto Serif Tibetan Medium already registered with postscriptFonts().
Noto Serif Tibetan Thin already registered with postscriptFonts().
NovaMono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Nunito. Skipping setup for this font.
orya already registered with postscriptFonts().
More than one version of regular/bold/italic found for padmaa. Skipping setup for this font.
Pothana2000 already registered with postscriptFonts().
ProFont for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Roboto. Skipping setup for this font.
More than one version of regular/bold/italic found for Roboto Condensed. Skipping setup for this font.
Roboto Mono for Powerline already registered with postscriptFonts().
Roboto Mono Light for Powerline already registered with postscriptFonts().
Roboto Mono Medium for Powerline already registered with postscriptFonts().
Roboto Mono Thin for Powerline already registered with postscriptFonts().
Sagar already registered with postscriptFonts().
Space Mono already registered with postscriptFonts().
Space Mono for Powerline already registered with postscriptFonts().
Symbol Neu for Powerline already registered with postscriptFonts().
TAMu_Kadambri already registered with postscriptFonts().
TAMu_Kalyani already registered with postscriptFonts().
TAMu_Maduram already registered with postscriptFonts().
Tinos for Powerline already registered with postscriptFonts().
TSCu_Comic already registered with postscriptFonts().
TSCu_Paranar already registered with postscriptFonts().
TSCu_Times already registered with postscriptFonts().
Ubuntu already registered with postscriptFonts().
Ubuntu Light already registered with postscriptFonts().
Ubuntu Condensed already registered with postscriptFonts().
Ubuntu Mono already registered with postscriptFonts().
Ubuntu Mono derivative Powerline already registered with postscriptFonts().
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_search_spaces)
#+end_SRC

#+RESULTS:
: 'data.frame':	69 obs. of  8 variables:
:  $ name                   : chr  "atax" "dgemv3" "fdtd4d2d" "gemver" ...
:  $ year                   : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...
:  $ dimension              : int  19 49 30 24 11 15 14 12 20 25 ...
:  $ search_space_size      : num  1.65e+14 2.73e+30 7.06e+24 7.26e+17 1.56e+08 ...
:  $ log10_search_space_size: int  14 30 24 17 8 8 12 8 16 19 ...
:  $ domain                 : chr  "Linear Algebra" "Linear Algebra" "Linear Algebra" "Linear Algebra" ...
:  $ author                 : chr  "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" ...
:  $ gscholar_citation      : chr  "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" ...
***** Generate Caption
#+begin_SRC R :results output :session *R* :eval no-export :exports results
citations <- unique(df_search_spaces$gscholar_citation)
citations <- paste(citations[citations != ""], collapse = ",")
cat(paste("\\nbsp{}\\cite{", citations, "}", sep = ""))
#+end_SRC

#+RESULTS:
:
: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,abdelfattah2016performance,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}

***** Plots
- Geom_label around thesis work
- Mark Seymour et al. example
#+begin_SRC R :results graphics output :session *R* :file "./img/search_spaces.pdf" :width 18 :height 8.7 :eval no-export
library(ggplot2)
library(dplyr)
library(scales)
library(RColorBrewer)
library(ggrepel)
library(patchwork)

point_alpha = 1.0
point_size = 3
label_size = 6

shapes = c(15, 16, 17, 18, 6, 7, 9, 0, 3, 5, 12, 14, 13, 11)

legend_rows = length(unique(df_search_spaces$domain)) / 2
legend_position = c(0.66, 0.12)

base_size = 25
font_family = "Liberation Sans"

x_lims <- c(0, 60)
y_lims <- c(1, 50)

color_palette = colorRampPalette(brewer.pal(9,
                                            "Set1"))(
                                                length(
                                                    unique(
                                                        df_search_spaces$domain)))

x_text = element_text(size = 26)
y_text = element_text(size = 26)

x_label = element_text(size = 28)
y_label = element_text(size = 28)

scientific_10 <- function(x) {
    print(x)
    result <- parse(text = gsub("(.*)",
                                "10^\\1",
                                format(x)))
    print(result)
    return(result)
}

p1 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_rect(aes(xmin = x_lims[1],
                  xmax = x_lims[2],
                  ymin = y_lims[1],
                  ymax = y_lims[2]),
              show.legend = FALSE,
              fill = NA,
              color = "gray35",
              linetype = 2) +
    geom_text(data = data.frame(x = x_lims[2],
                                y = y_lims[1],
                                label = "Detailed"),
              aes(x = x,
                  y = y,
                  label = label,
                  shape = NA),
              color = "gray35",
              vjust = 1.3,
              hjust = 0,
              angle = 90,
              size = label_size,
              show.legend = FALSE) +
    geom_point(alpha = point_alpha,
               size = point_size,
               show.legend = FALSE) +
    geom_text_repel(data = filter(df_search_spaces,
                                  thesis == FALSE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    xlim = c(130, 200),
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  name != "resnet50_weights" &
                                  name != "gemv" &
                                  dimension > 40 &
                                  log10_search_space_size >= 30),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    xlim = c(130, NA),
                    ylim = c(NA, NA),
                    nudge_y = 1,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30) %>%
                     filter(name == "resnet50_weights"),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(60, NA),
                    xlim = c(130, NA),
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = filter(df_search_spaces,
                                  thesis == TRUE &
                                  dimension > 40 &
                                  log10_search_space_size >= 30) %>%
                     filter(name == "gemv"),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(50, NA),
                    xlim = c(130, NA),
                    size = label_size,
                    show.legend = FALSE) +
    xlab("Dimension") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(text = element_text(family = font_family),
          axis.text.x = x_text,
          axis.text.y = y_text,
          axis.title.y = element_blank())

p2 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_point(alpha = point_alpha,
               size = point_size) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 40 &
                               dimension < 60 &
                               thesis == FALSE &
                               log10_search_space_size >= 30,
                               log10_search_space_size < 50),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(30, 50),
                    xlim = c(NA, 45),
                    nudge_x = -3,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension > 40 &
                               dimension < 60 &
                               thesis == TRUE &
                               log10_search_space_size >= 30,
                               log10_search_space_size < 50),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(30, 50),
                    xlim = c(NA, 45),
                    nudge_x = -3,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension < 20 &
                               thesis == FALSE &
                               log10_search_space_size > 19),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(20, 50),
                    xlim = c(0, 25),
                    nudge_y = 1.6,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension < 20 &
                               thesis == TRUE &
                               log10_search_space_size > 19),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(20, 50),
                    xlim = c(0, 25),
                    nudge_y = 1.6,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 30 &
                               thesis == FALSE &
                               log10_search_space_size < 25),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(15, 50),
                    xlim = c(30, NA),
                    nudge_y = 2,
                    size = label_size,
                    show.legend = FALSE) +
    geom_label_repel(data = df_search_spaces %>%
                        filter(dimension > 30 &
                               thesis == TRUE &
                               log10_search_space_size < 25),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(15, 50),
                    xlim = c(30, NA),
                    nudge_y = 2,
                    size = label_size,
                    show.legend = FALSE) +
    xlim(x_lims[1], x_lims[2]) +
    xlab("Dimension") +
    ylab("Search Space Size") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(limits = y_lims, label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(axis.text.x = x_text,
          axis.text.y = y_text,
          legend.position = legend_position,
          legend.direction = "horizontal",
          legend.spacing.x = unit(0.0, 'cm'),
          legend.spacing.y = unit(0.0, 'cm'),
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 15),
          text = element_text(family = font_family)) +
    guides(color = guide_legend(nrow = legend_rows,
                                override.aes = list(alpha = 1.0,
                                                    size = 3)))

p2 * p1
#+end_SRC

#+RESULTS:
[[file:./img/search_spaces.pdf]]
**** Loop Blocking and Unrolling
***** First try
:PROPERTIES:
:EXPORT_FILE_NAME: blocking_unrolling.pdf
:END:

#+begin_src latex :fit true
% Export this heading with C-c C-e C-s C-b l p
\documentclass{standalone}
\usepackage{tikz}
\begin{document}
\begin{tikzpicture}
\draw[red] (0,0) circle (2cm);
\end{tikzpicture}
\end{document}
#+end_src try with export
***** Arnaud's tip with src blocks
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
\\usepackage[usenames]{color}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: \usepackage[usenames]{color}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage{tikz}")
#+HEADER: :exports results :results raw :file ./img/test.pdf
#+begin_src latex :eval no-export
\begin{tikzpicture}
\draw[blue] (0,0) circle (2cm);
\end{tikzpicture}
#+end_src

#+RESULTS:
[[file:./img/test.pdf]]

***** ggplot
****** Setup
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(extrafont)
loadfonts(device = "postscript")
#+end_SRC

#+RESULTS:
#+begin_example

Akaash already registered with postscriptFonts().
AkrutiMal1 already registered with postscriptFonts().
AkrutiMal2 already registered with postscriptFonts().
AkrutiTml1 already registered with postscriptFonts().
AkrutiTml2 already registered with postscriptFonts().
Anonymice Powerline already registered with postscriptFonts().
Arimo for Powerline already registered with postscriptFonts().
Bitstream Vera Sans already registered with postscriptFonts().
Bitstream Vera Sans Mono already registered with postscriptFonts().
Bitstream Vera Serif already registered with postscriptFonts().
Cousine for Powerline already registered with postscriptFonts().
IBM 3270 already registered with postscriptFonts().
IBM 3270 Narrow already registered with postscriptFonts().
IBM 3270 Semi-Narrow already registered with postscriptFonts().
DejaVu Math TeX Gyre already registered with postscriptFonts().
DejaVu Sans already registered with postscriptFonts().
DejaVu Sans Light already registered with postscriptFonts().
DejaVu Sans Condensed already registered with postscriptFonts().
DejaVu Sans Mono already registered with postscriptFonts().
DejaVu Sans Mono for Powerline already registered with postscriptFonts().
DejaVu Serif already registered with postscriptFonts().
DejaVu Serif Condensed already registered with postscriptFonts().
Droid Arabic Kufi already registered with postscriptFonts().
Droid Arabic Naskh already registered with postscriptFonts().
Droid Naskh Shift Alt already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans. Skipping setup for this font.
Droid Sans Arabic already registered with postscriptFonts().
Droid Sans Armenian already registered with postscriptFonts().
Droid Sans Devanagari already registered with postscriptFonts().
Droid Sans Ethiopic already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Fallback. Skipping setup for this font.
Droid Sans Georgian already registered with postscriptFonts().
Droid Sans Hebrew already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Mono. Skipping setup for this font.
Droid Sans Mono Dotted for Powerline already registered with postscriptFonts().
Droid Sans Mono Slashed for Powerline already registered with postscriptFonts().
Droid Sans Tamil already registered with postscriptFonts().
Droid Sans Thai already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Serif. Skipping setup for this font.
Font Awesome 5 Brands Regular already registered with postscriptFonts().
Font Awesome 5 Free Regular already registered with postscriptFonts().
Font Awesome 5 Free Solid already registered with postscriptFonts().
Gargi-1.2b already registered with postscriptFonts().
Goha-Tibeb Zemen already registered with postscriptFonts().
Go Mono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for GurbaniBoliLite. Skipping setup for this font.
Hack already registered with postscriptFonts().
Inconsolata Black already registered with postscriptFonts().
Inconsolata already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Inconsolata for Powerline. Skipping setup for this font.
Inconsolata Condensed already registered with postscriptFonts().
Inconsolata Condensed Black already registered with postscriptFonts().
Inconsolata Condensed Bold already registered with postscriptFonts().
Inconsolata Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Condensed Light already registered with postscriptFonts().
Inconsolata Condensed Medium already registered with postscriptFonts().
Inconsolata Condensed SemiBold already registered with postscriptFonts().
Inconsolata Expanded already registered with postscriptFonts().
Inconsolata Expanded Black already registered with postscriptFonts().
Inconsolata Expanded Bold already registered with postscriptFonts().
Inconsolata Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Expanded Light already registered with postscriptFonts().
Inconsolata Expanded Medium already registered with postscriptFonts().
Inconsolata Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed already registered with postscriptFonts().
Inconsolata Extra Condensed Black already registered with postscriptFonts().
Inconsolata Extra Condensed Bold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Extra Condensed Light already registered with postscriptFonts().
Inconsolata Extra Condensed Medium already registered with postscriptFonts().
Inconsolata Extra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Extra Expanded already registered with postscriptFonts().
Inconsolata Extra Expanded Black already registered with postscriptFonts().
Inconsolata Extra Expanded Bold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Extra Expanded Light already registered with postscriptFonts().
Inconsolata Extra Expanded Medium already registered with postscriptFonts().
Inconsolata Extra Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraLight already registered with postscriptFonts().
Inconsolata Light already registered with postscriptFonts().
Inconsolata Medium already registered with postscriptFonts().
Inconsolata SemiBold already registered with postscriptFonts().
Inconsolata Semi Condensed already registered with postscriptFonts().
Inconsolata Semi Condensed Black already registered with postscriptFonts().
Inconsolata Semi Condensed Bold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Semi Condensed Light already registered with postscriptFonts().
Inconsolata Semi Condensed Medium already registered with postscriptFonts().
Inconsolata Semi Condensed SemiBold already registered with postscriptFonts().
Inconsolata Semi Expanded already registered with postscriptFonts().
Inconsolata Semi Expanded Black already registered with postscriptFonts().
Inconsolata Semi Expanded Bold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Semi Expanded Light already registered with postscriptFonts().
Inconsolata Semi Expanded Medium already registered with postscriptFonts().
Inconsolata Semi Expanded SemiBold already registered with postscriptFonts().
Inconsolata Ultra Condensed already registered with postscriptFonts().
Inconsolata Ultra Condensed Black already registered with postscriptFonts().
Inconsolata Ultra Condensed Bold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Condensed Light already registered with postscriptFonts().
Inconsolata Ultra Condensed Medium already registered with postscriptFonts().
Inconsolata Ultra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Ultra Expanded already registered with postscriptFonts().
Inconsolata Ultra Expanded Black already registered with postscriptFonts().
Inconsolata Ultra Expanded Bold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Expanded Light already registered with postscriptFonts().
Inconsolata Ultra Expanded Medium already registered with postscriptFonts().
Inconsolata Ultra Expanded SemiBold already registered with postscriptFonts().
Liberation Mono already registered with postscriptFonts().
Liberation Sans already registered with postscriptFonts().
Liberation Serif already registered with postscriptFonts().
Ligconsolata already registered with postscriptFonts().
Likhan already registered with postscriptFonts().
Literation Mono Powerline already registered with postscriptFonts().
malayalam already registered with postscriptFonts().
MalOtf already registered with postscriptFonts().
Meslo LG L DZ for Powerline already registered with postscriptFonts().
Meslo LG L for Powerline already registered with postscriptFonts().
Meslo LG M DZ for Powerline already registered with postscriptFonts().
Meslo LG M for Powerline already registered with postscriptFonts().
Meslo LG S DZ for Powerline already registered with postscriptFonts().
Meslo LG S for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for monofur for Powerline. Skipping setup for this font.
More than one version of regular/bold/italic found for Mukti Narrow. Skipping setup for this font.
Noto Kufi Arabic already registered with postscriptFonts().
Noto Kufi Arabic Medium already registered with postscriptFonts().
Noto Kufi Arabic Semi bold already registered with postscriptFonts().
Noto Mono for Powerline already registered with postscriptFonts().
Noto Music already registered with postscriptFonts().
Noto Naskh Arabic already registered with postscriptFonts().
Noto Naskh Arabic UI already registered with postscriptFonts().
Noto Nastaliq Urdu already registered with postscriptFonts().
Noto Sans Black already registered with postscriptFonts().
Noto Sans already registered with postscriptFonts().
Noto Sans Light already registered with postscriptFonts().
Noto Sans Medium already registered with postscriptFonts().
Noto Sans Thin already registered with postscriptFonts().
Noto Sans Adlam already registered with postscriptFonts().
Noto Sans Adlam Unjoined already registered with postscriptFonts().
Noto Sans AnatoHiero already registered with postscriptFonts().
Noto Sans Arabic Blk already registered with postscriptFonts().
Noto Sans Arabic already registered with postscriptFonts().
Noto Sans Arabic Light already registered with postscriptFonts().
Noto Sans Arabic Med already registered with postscriptFonts().
Noto Sans Arabic Thin already registered with postscriptFonts().
Noto Sans Arabic UI Bk already registered with postscriptFonts().
Noto Sans Arabic UI already registered with postscriptFonts().
Noto Sans Arabic UI Lt already registered with postscriptFonts().
Noto Sans Arabic UI Md already registered with postscriptFonts().
Noto Sans Arabic UI Th already registered with postscriptFonts().
Noto Sans Armenian Blk already registered with postscriptFonts().
Noto Sans Armenian already registered with postscriptFonts().
Noto Sans Armenian Light already registered with postscriptFonts().
Noto Sans Armenian Med already registered with postscriptFonts().
Noto Sans Armenian Thin already registered with postscriptFonts().
Noto Sans Avestan already registered with postscriptFonts().
Noto Sans Bamum already registered with postscriptFonts().
Noto Sans Bassa Vah already registered with postscriptFonts().
Noto Sans Batak already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Blk. Skipping setup for this font.
Noto Sans Bengali already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Thin. Skipping setup for this font.
Noto Sans Bengali UI already registered with postscriptFonts().
Noto Sans Bhaiksuki already registered with postscriptFonts().
Noto Sans Brahmi already registered with postscriptFonts().
Noto Sans Buginese already registered with postscriptFonts().
Noto Sans Buhid already registered with postscriptFonts().
Noto Sans CanAborig Bk already registered with postscriptFonts().
Noto Sans CanAborig already registered with postscriptFonts().
Noto Sans CanAborig Lt already registered with postscriptFonts().
Noto Sans CanAborig Md already registered with postscriptFonts().
Noto Sans CanAborig Th already registered with postscriptFonts().
Noto Sans Carian already registered with postscriptFonts().
Noto Sans CaucAlban already registered with postscriptFonts().
Noto Sans Chakma already registered with postscriptFonts().
Noto Sans Cham Blk already registered with postscriptFonts().
Noto Sans Cham already registered with postscriptFonts().
Noto Sans Cham Light already registered with postscriptFonts().
Noto Sans Cham Med already registered with postscriptFonts().
Noto Sans Cham Thin already registered with postscriptFonts().
Noto Sans Cherokee Blk already registered with postscriptFonts().
Noto Sans Cherokee already registered with postscriptFonts().
Noto Sans Cherokee Light already registered with postscriptFonts().
Noto Sans Cherokee Med already registered with postscriptFonts().
Noto Sans Cherokee Thin already registered with postscriptFonts().
Noto Sans Coptic already registered with postscriptFonts().
Noto Sans Cuneiform already registered with postscriptFonts().
Noto Sans Cypriot already registered with postscriptFonts().
Noto Sans Deseret already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Bk. Skipping setup for this font.
Noto Sans Devanagari already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Lt. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Md. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Th. Skipping setup for this font.
Noto Sans Devanagari UI already registered with postscriptFonts().
Noto Sans Display Black already registered with postscriptFonts().
Noto Sans Display already registered with postscriptFonts().
Noto Sans Display Light already registered with postscriptFonts().
Noto Sans Display Medium already registered with postscriptFonts().
Noto Sans Display Thin already registered with postscriptFonts().
Noto Sans Duployan already registered with postscriptFonts().
Noto Sans EgyptHiero already registered with postscriptFonts().
Noto Sans Elbasan already registered with postscriptFonts().
Noto Sans Ethiopic Blk already registered with postscriptFonts().
Noto Sans Ethiopic already registered with postscriptFonts().
Noto Sans Ethiopic Light already registered with postscriptFonts().
Noto Sans Ethiopic Med already registered with postscriptFonts().
Noto Sans Ethiopic Thin already registered with postscriptFonts().
Noto Sans Georgian Blk already registered with postscriptFonts().
Noto Sans Georgian already registered with postscriptFonts().
Noto Sans Georgian Light already registered with postscriptFonts().
Noto Sans Georgian Med already registered with postscriptFonts().
Noto Sans Georgian Thin already registered with postscriptFonts().
Noto Sans Glagolitic already registered with postscriptFonts().
Noto Sans Gothic already registered with postscriptFonts().
Noto Sans Grantha already registered with postscriptFonts().
Noto Sans Gujarati already registered with postscriptFonts().
Noto Sans Gujarati UI already registered with postscriptFonts().
Noto Sans Gurmukhi Black already registered with postscriptFonts().
Noto Sans Gurmukhi already registered with postscriptFonts().
Noto Sans Gurmukhi Light already registered with postscriptFonts().
Noto Sans Gurmukhi Medium already registered with postscriptFonts().
Noto Sans Gurmukhi Thin already registered with postscriptFonts().
Noto Sans Gurmukhi UI Black already registered with postscriptFonts().
Noto Sans Gurmukhi UI already registered with postscriptFonts().
Noto Sans Gurmukhi UI Light already registered with postscriptFonts().
Noto Sans Gurmukhi UI Medium already registered with postscriptFonts().
Noto Sans Gurmukhi UI Thin already registered with postscriptFonts().
Noto Sans HanifiRohg already registered with postscriptFonts().
Noto Sans Hanunoo already registered with postscriptFonts().
Noto Sans Hatran already registered with postscriptFonts().
Noto Sans Hebrew Blk already registered with postscriptFonts().
Noto Sans Hebrew already registered with postscriptFonts().
Noto Sans Hebrew Light already registered with postscriptFonts().
Noto Sans Hebrew Med already registered with postscriptFonts().
Noto Sans Hebrew Thin already registered with postscriptFonts().
Noto Sans ImpAramaic already registered with postscriptFonts().
Noto Sans Indic Siyaq Numbers already registered with postscriptFonts().
Noto Sans InsPahlavi already registered with postscriptFonts().
Noto Sans InsParthi already registered with postscriptFonts().
Noto Sans Javanese already registered with postscriptFonts().
Noto Sans Kaithi already registered with postscriptFonts().
Noto Sans Kannada Black already registered with postscriptFonts().
Noto Sans Kannada already registered with postscriptFonts().
Noto Sans Kannada Light already registered with postscriptFonts().
Noto Sans Kannada Medium already registered with postscriptFonts().
Noto Sans Kannada Thin already registered with postscriptFonts().
Noto Sans Kannada UI Black already registered with postscriptFonts().
Noto Sans Kannada UI already registered with postscriptFonts().
Noto Sans Kannada UI Light already registered with postscriptFonts().
Noto Sans Kannada UI Medium already registered with postscriptFonts().
Noto Sans Kannada UI Thin already registered with postscriptFonts().
Noto Sans Kayah Li already registered with postscriptFonts().
Noto Sans Kharoshthi already registered with postscriptFonts().
Noto Sans Khmer Black already registered with postscriptFonts().
Noto Sans Khmer already registered with postscriptFonts().
Noto Sans Khmer Light already registered with postscriptFonts().
Noto Sans Khmer Medium already registered with postscriptFonts().
Noto Sans Khmer Thin already registered with postscriptFonts().
Noto Sans Khmer UI Black already registered with postscriptFonts().
Noto Sans Khmer UI already registered with postscriptFonts().
Noto Sans Khmer UI Light already registered with postscriptFonts().
Noto Sans Khmer UI Medium already registered with postscriptFonts().
Noto Sans Khmer UI Thin already registered with postscriptFonts().
Noto Sans Khojki already registered with postscriptFonts().
Noto Sans Khudawadi already registered with postscriptFonts().
Noto Sans Lao Blk already registered with postscriptFonts().
Noto Sans Lao already registered with postscriptFonts().
Noto Sans Lao Light already registered with postscriptFonts().
Noto Sans Lao Med already registered with postscriptFonts().
Noto Sans Lao Thin already registered with postscriptFonts().
Noto Sans Lao UI Blk already registered with postscriptFonts().
Noto Sans Lao UI already registered with postscriptFonts().
Noto Sans Lao UI Light already registered with postscriptFonts().
Noto Sans Lao UI Med already registered with postscriptFonts().
Noto Sans Lao UI Thin already registered with postscriptFonts().
Noto Sans Lepcha already registered with postscriptFonts().
Noto Sans Limbu already registered with postscriptFonts().
Noto Sans Linear A already registered with postscriptFonts().
Noto Sans Linear B already registered with postscriptFonts().
Noto Sans Lisu already registered with postscriptFonts().
Noto Sans Lycian already registered with postscriptFonts().
Noto Sans Lydian already registered with postscriptFonts().
Noto Sans Mahajani already registered with postscriptFonts().
Noto Sans Malayalam Black already registered with postscriptFonts().
Noto Sans Malayalam already registered with postscriptFonts().
Noto Sans Malayalam Light already registered with postscriptFonts().
Noto Sans Malayalam Medium already registered with postscriptFonts().
Noto Sans Malayalam Thin already registered with postscriptFonts().
Noto Sans Malayalam UI Black already registered with postscriptFonts().
Noto Sans Malayalam UI already registered with postscriptFonts().
Noto Sans Malayalam UI Light already registered with postscriptFonts().
Noto Sans Malayalam UI Medium already registered with postscriptFonts().
Noto Sans Malayalam UI Thin already registered with postscriptFonts().
Noto Sans Mandaic already registered with postscriptFonts().
Noto Sans Manichaean already registered with postscriptFonts().
Noto Sans Marchen already registered with postscriptFonts().
Noto Sans Math already registered with postscriptFonts().
Noto Sans Mayan Numerals already registered with postscriptFonts().
Noto Sans MeeteiMayek already registered with postscriptFonts().
Noto Sans Mende Kikakui already registered with postscriptFonts().
Noto Sans Meroitic already registered with postscriptFonts().
Noto Sans Miao already registered with postscriptFonts().
Noto Sans Modi already registered with postscriptFonts().
Noto Sans Mongolian already registered with postscriptFonts().
Noto Sans Mono Black already registered with postscriptFonts().
Noto Sans Mono already registered with postscriptFonts().
Noto Sans Mono Light already registered with postscriptFonts().
Noto Sans Mono Medium already registered with postscriptFonts().
Noto Sans Mono Thin already registered with postscriptFonts().
Noto Sans Mro already registered with postscriptFonts().
Noto Sans Multani already registered with postscriptFonts().
Noto Sans Myanmar Blk already registered with postscriptFonts().
Noto Sans Myanmar already registered with postscriptFonts().
Noto Sans Myanmar Light already registered with postscriptFonts().
Noto Sans Myanmar Med already registered with postscriptFonts().
Noto Sans Myanmar Thin already registered with postscriptFonts().
Noto Sans Myanmar UI Black already registered with postscriptFonts().
Noto Sans Myanmar UI already registered with postscriptFonts().
Noto Sans Myanmar UI Light already registered with postscriptFonts().
Noto Sans Myanmar UI Medium already registered with postscriptFonts().
Noto Sans Myanmar UI Thin already registered with postscriptFonts().
Noto Sans Nabataean already registered with postscriptFonts().
Noto Sans Newa already registered with postscriptFonts().
Noto Sans NewTaiLue already registered with postscriptFonts().
Noto Sans N'Ko already registered with postscriptFonts().
Noto Sans Ogham already registered with postscriptFonts().
Noto Sans Ol Chiki already registered with postscriptFonts().
Noto Sans OldHung already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Noto Sans Old Italic. Skipping setup for this font.
Noto Sans OldNorArab already registered with postscriptFonts().
Noto Sans Old Permic already registered with postscriptFonts().
Noto Sans OldPersian already registered with postscriptFonts().
Noto Sans OldSogdian already registered with postscriptFonts().
Noto Sans OldSouArab already registered with postscriptFonts().
Noto Sans Old Turkic already registered with postscriptFonts().
Noto Sans Oriya already registered with postscriptFonts().
Noto Sans Oriya UI already registered with postscriptFonts().
Noto Sans Osage already registered with postscriptFonts().
Noto Sans Osmanya already registered with postscriptFonts().
Noto Sans Pahawh Hmong already registered with postscriptFonts().
Noto Sans Palmyrene already registered with postscriptFonts().
Noto Sans PauCinHau already registered with postscriptFonts().
Noto Sans PhagsPa already registered with postscriptFonts().
Noto Sans Phoenician already registered with postscriptFonts().
Noto Sans PsaPahlavi already registered with postscriptFonts().
Noto Sans Rejang already registered with postscriptFonts().
Noto Sans Runic already registered with postscriptFonts().
Noto Sans Samaritan already registered with postscriptFonts().
Noto Sans Saurashtra already registered with postscriptFonts().
Noto Sans Sharada already registered with postscriptFonts().
Noto Sans Shavian already registered with postscriptFonts().
Noto Sans Siddham already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Blk. Skipping setup for this font.
Noto Sans Sinhala already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Thin. Skipping setup for this font.
Noto Sans Sinhala UI already registered with postscriptFonts().
Noto Sans SoraSomp already registered with postscriptFonts().
Noto Sans Sundanese already registered with postscriptFonts().
Noto Sans Syloti Nagri already registered with postscriptFonts().
Noto Sans Symbols Blk already registered with postscriptFonts().
Noto Sans Symbols already registered with postscriptFonts().
Noto Sans Symbols Light already registered with postscriptFonts().
Noto Sans Symbols Med already registered with postscriptFonts().
Noto Sans Symbols Thin already registered with postscriptFonts().
Noto Sans Symbols2 already registered with postscriptFonts().
Noto Sans Syriac Black already registered with postscriptFonts().
Noto Sans Syriac already registered with postscriptFonts().
Noto Sans Syriac Thin already registered with postscriptFonts().
Noto Sans Tagalog already registered with postscriptFonts().
Noto Sans Tagbanwa already registered with postscriptFonts().
Noto Sans Tai Le already registered with postscriptFonts().
Noto Sans Tai Tham already registered with postscriptFonts().
Noto Sans Tai Viet already registered with postscriptFonts().
Noto Sans Takri already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Blk. Skipping setup for this font.
Noto Sans Tamil already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Thin. Skipping setup for this font.
Noto Sans Tamil Supplement already registered with postscriptFonts().
Noto Sans Tamil UI already registered with postscriptFonts().
Noto Sans Telugu Black already registered with postscriptFonts().
Noto Sans Telugu already registered with postscriptFonts().
Noto Sans Telugu Light already registered with postscriptFonts().
Noto Sans Telugu Medium already registered with postscriptFonts().
Noto Sans Telugu Thin already registered with postscriptFonts().
Noto Sans Telugu UI Black already registered with postscriptFonts().
Noto Sans Telugu UI already registered with postscriptFonts().
Noto Sans Telugu UI Light already registered with postscriptFonts().
Noto Sans Telugu UI Medium already registered with postscriptFonts().
Noto Sans Telugu UI Thin already registered with postscriptFonts().
Noto Sans Thaana Black already registered with postscriptFonts().
Noto Sans Thaana already registered with postscriptFonts().
Noto Sans Thaana Light already registered with postscriptFonts().
Noto Sans Thaana Medium already registered with postscriptFonts().
Noto Sans Thaana Thin already registered with postscriptFonts().
Noto Sans Thai Blk already registered with postscriptFonts().
Noto Sans Thai already registered with postscriptFonts().
Noto Sans Thai Light already registered with postscriptFonts().
Noto Sans Thai Med already registered with postscriptFonts().
Noto Sans Thai Thin already registered with postscriptFonts().
Noto Sans Thai UI Blk already registered with postscriptFonts().
Noto Sans Thai UI already registered with postscriptFonts().
Noto Sans Thai UI Light already registered with postscriptFonts().
Noto Sans Thai UI Med already registered with postscriptFonts().
Noto Sans Thai UI Thin already registered with postscriptFonts().
Noto Sans Tibetan already registered with postscriptFonts().
Noto Sans Tifinagh already registered with postscriptFonts().
Noto Sans Tirhuta already registered with postscriptFonts().
Noto Sans Ugaritic already registered with postscriptFonts().
Noto Sans Vai already registered with postscriptFonts().
Noto Sans WarangCiti already registered with postscriptFonts().
Noto Sans Yi already registered with postscriptFonts().
Noto Serif Black already registered with postscriptFonts().
Noto Serif already registered with postscriptFonts().
Noto Serif Light already registered with postscriptFonts().
Noto Serif Medium already registered with postscriptFonts().
Noto Serif Thin already registered with postscriptFonts().
Noto Serif Ahom already registered with postscriptFonts().
Noto Serif Armenian Bk already registered with postscriptFonts().
Noto Serif Armenian already registered with postscriptFonts().
Noto Serif Armenian Lt already registered with postscriptFonts().
Noto Serif Armenian Md already registered with postscriptFonts().
Noto Serif Armenian Th already registered with postscriptFonts().
Noto Serif Balinese already registered with postscriptFonts().
Noto Serif Bengali Black already registered with postscriptFonts().
Noto Serif Bengali already registered with postscriptFonts().
Noto Serif Bengali Light already registered with postscriptFonts().
Noto Serif Bengali Medium already registered with postscriptFonts().
Noto Serif Bengali Thin already registered with postscriptFonts().
Noto Serif Devanagari Black already registered with postscriptFonts().
Noto Serif Devanagari already registered with postscriptFonts().
Noto Serif Devanagari Light already registered with postscriptFonts().
Noto Serif Devanagari Medium already registered with postscriptFonts().
Noto Serif Devanagari Thin already registered with postscriptFonts().
Noto Serif Display Black already registered with postscriptFonts().
Noto Serif Display already registered with postscriptFonts().
Noto Serif Display Light already registered with postscriptFonts().
Noto Serif Display Medium already registered with postscriptFonts().
Noto Serif Display Thin already registered with postscriptFonts().
Noto Serif Dogra already registered with postscriptFonts().
Noto Serif Ethiopic Bk already registered with postscriptFonts().
Noto Serif Ethiopic already registered with postscriptFonts().
Noto Serif Ethiopic Lt already registered with postscriptFonts().
Noto Serif Ethiopic Md already registered with postscriptFonts().
Noto Serif Ethiopic Th already registered with postscriptFonts().
Noto Serif Georgian Bk already registered with postscriptFonts().
Noto Serif Georgian already registered with postscriptFonts().
Noto Serif Georgian Lt already registered with postscriptFonts().
Noto Serif Georgian Md already registered with postscriptFonts().
Noto Serif Georgian Th already registered with postscriptFonts().
Noto Serif Gujarati Black already registered with postscriptFonts().
Noto Serif Gujarati already registered with postscriptFonts().
Noto Serif Gujarati Light already registered with postscriptFonts().
Noto Serif Gujarati Medium already registered with postscriptFonts().
Noto Serif Gujarati Thin already registered with postscriptFonts().
Noto Serif Gurmukhi Black already registered with postscriptFonts().
Noto Serif Gurmukhi already registered with postscriptFonts().
Noto Serif Gurmukhi Light already registered with postscriptFonts().
Noto Serif Gurmukhi Medium already registered with postscriptFonts().
Noto Serif Gurmukhi Thin already registered with postscriptFonts().
Noto Serif Hebrew Blk already registered with postscriptFonts().
Noto Serif Hebrew already registered with postscriptFonts().
Noto Serif Hebrew Light already registered with postscriptFonts().
Noto Serif Hebrew Med already registered with postscriptFonts().
Noto Serif Hebrew Thin already registered with postscriptFonts().
Noto Serif Kannada Black already registered with postscriptFonts().
Noto Serif Kannada already registered with postscriptFonts().
Noto Serif Kannada Light already registered with postscriptFonts().
Noto Serif Kannada Medium already registered with postscriptFonts().
Noto Serif Kannada Thin already registered with postscriptFonts().
Noto Serif Khmer Black already registered with postscriptFonts().
Noto Serif Khmer already registered with postscriptFonts().
Noto Serif Khmer Light already registered with postscriptFonts().
Noto Serif Khmer Medium already registered with postscriptFonts().
Noto Serif Khmer Thin already registered with postscriptFonts().
Noto Serif Lao Blk already registered with postscriptFonts().
Noto Serif Lao already registered with postscriptFonts().
Noto Serif Lao Light already registered with postscriptFonts().
Noto Serif Lao Med already registered with postscriptFonts().
Noto Serif Lao Thin already registered with postscriptFonts().
Noto Serif Malayalam Black already registered with postscriptFonts().
Noto Serif Malayalam already registered with postscriptFonts().
Noto Serif Malayalam Light already registered with postscriptFonts().
Noto Serif Malayalam Medium already registered with postscriptFonts().
Noto Serif Malayalam Thin already registered with postscriptFonts().
Noto Serif Myanmar Blk already registered with postscriptFonts().
Noto Serif Myanmar already registered with postscriptFonts().
Noto Serif Myanmar Light already registered with postscriptFonts().
Noto Serif Myanmar Med already registered with postscriptFonts().
Noto Serif Myanmar Thin already registered with postscriptFonts().
Noto Serif Sinhala Black already registered with postscriptFonts().
Noto Serif Sinhala already registered with postscriptFonts().
Noto Serif Sinhala Light already registered with postscriptFonts().
Noto Serif Sinhala Medium already registered with postscriptFonts().
Noto Serif Sinhala Thin already registered with postscriptFonts().
Noto Serif Tamil Blk already registered with postscriptFonts().
Noto Serif Tamil already registered with postscriptFonts().
Noto Serif Tamil Light already registered with postscriptFonts().
Noto Serif Tamil Med already registered with postscriptFonts().
Noto Serif Tamil Thin already registered with postscriptFonts().
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Black. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Light. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Medium. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Thin. Skipping setup for this font.
Noto Serif Tangut already registered with postscriptFonts().
Noto Serif Telugu Black already registered with postscriptFonts().
Noto Serif Telugu already registered with postscriptFonts().
Noto Serif Telugu Light already registered with postscriptFonts().
Noto Serif Telugu Medium already registered with postscriptFonts().
Noto Serif Telugu Thin already registered with postscriptFonts().
Noto Serif Thai Blk already registered with postscriptFonts().
Noto Serif Thai already registered with postscriptFonts().
Noto Serif Thai Light already registered with postscriptFonts().
Noto Serif Thai Med already registered with postscriptFonts().
Noto Serif Thai Thin already registered with postscriptFonts().
Noto Serif Tibetan Black already registered with postscriptFonts().
Noto Serif Tibetan already registered with postscriptFonts().
Noto Serif Tibetan Light already registered with postscriptFonts().
Noto Serif Tibetan Medium already registered with postscriptFonts().
Noto Serif Tibetan Thin already registered with postscriptFonts().
NovaMono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Nunito. Skipping setup for this font.
orya already registered with postscriptFonts().
More than one version of regular/bold/italic found for padmaa. Skipping setup for this font.
Pothana2000 already registered with postscriptFonts().
ProFont for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Roboto. Skipping setup for this font.
More than one version of regular/bold/italic found for Roboto Condensed. Skipping setup for this font.
Roboto Mono for Powerline already registered with postscriptFonts().
Roboto Mono Light for Powerline already registered with postscriptFonts().
Roboto Mono Medium for Powerline already registered with postscriptFonts().
Roboto Mono Thin for Powerline already registered with postscriptFonts().
Sagar already registered with postscriptFonts().
Space Mono already registered with postscriptFonts().
Space Mono for Powerline already registered with postscriptFonts().
Symbol Neu for Powerline already registered with postscriptFonts().
TAMu_Kadambri already registered with postscriptFonts().
TAMu_Kalyani already registered with postscriptFonts().
TAMu_Maduram already registered with postscriptFonts().
Tinos for Powerline already registered with postscriptFonts().
TSCu_Comic already registered with postscriptFonts().
TSCu_Paranar already registered with postscriptFonts().
TSCu_Times already registered with postscriptFonts().
Ubuntu already registered with postscriptFonts().
Ubuntu Light already registered with postscriptFonts().
Ubuntu Condensed already registered with postscriptFonts().
Ubuntu Mono already registered with postscriptFonts().
Ubuntu Mono derivative Powerline already registered with postscriptFonts().
#+end_example

****** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/blocking_unrolling.pdf" :width 17.4 :height 6 :eval no-export
library(ggplot2)
library(dplyr)
library(tidyr)
library(paletteer)
library(patchwork)

font_family = "Liberation Sans"
base_size = 20

x_lab = "Columns"
y_lab = "Rows"

x_text = element_text(size = 20)
y_text = element_text(size = 20)

regular <- function(block_size) {
    matrix_size <- block_size * 4
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size)){
        for(j in seq(1, matrix_size)){
            df_matrix[i, j] <- iteration
            iteration <- iteration + 1
        }
    }

    return(df_matrix)
}

tiled <- function(block_size) {
    matrix_size <- block_size * 4
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size, by = block_size)){
        for(j in seq(1, matrix_size, by = block_size)){
            for(x in seq(i, min(i + block_size, matrix_size + 1) - 1)){
                for(y in seq(j, min(j + block_size, matrix_size + 1) - 1)){
                    df_matrix[x, y] <- iteration
                    iteration <- iteration + 1
                }
            }
        }
    }

    return(df_matrix)
}

unrolled <- function(block_size) {
    matrix_size <- block_size * 4
    unrolling_factor <- block_size
    df_matrix <- matrix(rnorm(matrix_size ^ 2),
                        nrow = matrix_size)
    iteration <- 1

    for(i in seq(1, matrix_size, by = block_size)){
        for(j in seq(1, matrix_size, by = block_size)){
            df_matrix[seq(i, min(i + block_size, matrix_size + 1) - 1),
                      seq(j, min(j + block_size, matrix_size + 1) - 1)] <- iteration
            iteration <- iteration + 1
        }
    }

    return(df_matrix)
}

to_tile <- function(df) {
    df %>%
        data.frame() %>%
        mutate(row = row_number()) %>%
        gather(key = "col", value = "order", -row) %>%
        mutate(col = unlist(
                   lapply(col,
                          function(x) {
                              as.integer(strsplit(x, "X")[[1]][2])
                          })))
}

regular_access <- to_tile(regular(4))
tiled_access <- to_tile(tiled(4))
unrolled_access <- to_tile(unrolled(4))

t_regular_access <- to_tile(t(regular(4)))
t_tiled_access <- to_tile(t(tiled(4)))
t_unrolled_access <- to_tile(t(unrolled(4)))

cf_palette <- "pals::jet"

p1 <- ggplot() +
    geom_tile(data = regular_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order),
              show.legend = FALSE) +
    scale_fill_paletteer_c(cf_palette) +
    scale_color_paletteer_c(cf_palette) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(a) Regular Access Pattern") +
    theme_bw(base_size = base_size) +
    theme(axis.title.x = x_text,
          axis.title.y = y_text,
          text = element_text(family = font_family),
          legend.position = "bottom",
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 16),
          legend.title = element_text(size = 23, margin = margin(r = 10)),
          legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

p2 <- ggplot() +
    geom_tile(data = tiled_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order),
              show.legend = FALSE) +
    scale_fill_paletteer_c(cf_palette) +
    scale_color_paletteer_c(cf_palette) +
    #scale_color_paletteer_c(name = "Access Order",
    #                        cf_palette,
    #                        guide = guide_colorbar(title.vjust = 1.1,
    #                                               title.hjust = 1,
    #                                               label.position = "bottom"),
    #                        breaks = c(1, 256),
    #                        labels = c("First", "Last")) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(b) After Tiling") +
    theme_bw(base_size = base_size) +
    theme(axis.title.x = x_text,
          axis.title.y = y_text,
          text = element_text(family = font_family),
          legend.position = "bottom",
          legend.direction = "horizontal",
          # legend.background = element_rect(fill = "transparent", colour = NA),
          # legend.text = element_text(size = 16),
          legend.title = element_text(margin = margin(r = 40)),
          # legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

p3 <- ggplot() +
    geom_tile(data = tiled_access,
              aes(x = rev(col),
                  y = row,
                  fill = order,
                  color = order)) +
    scale_color_paletteer_c(cf_palette) +
    scale_fill_paletteer_c(name = "Order",
                           cf_palette,
                           guide = guide_colorbar(barwidth = 0.7,
                                                  barheight = 20,
                                                  direction = "vertical",
                                                  label.hjust = 1,
                                                  ticks = FALSE,
                                                  reverse = FALSE),
                           limits = c(1, 256),
                           breaks = c(1, 256),
                           labels = c("Last", "First")) +
    scale_x_discrete(expand = c(0,0))+
    scale_y_discrete(expand = c(0,0)) +
    labs(x = x_lab,
         y = y_lab,
         title = "(c) After Tiling and Unrolling") +
    theme_bw(base_size = base_size) +
    theme(text = element_text(family = font_family),
          axis.title.x = x_text,
          axis.title.y = y_text,
          legend.position = "right",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 20),
          legend.title = element_text(size = 23, margin = margin(b = 15)),
          # legend.spacing.x = unit(0.0, 'cm'),
          axis.ticks.y = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    guides(color = FALSE)

p1 + p2 + p3
#+end_SRC

#+RESULTS:
[[file:./img/blocking_unrolling.pdf]]



** Optimization Methods
*** Introduction
**** (Old) Complete Classification Tree for Function Minimization Methods
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" )
#+HEADER: :exports results :results raw :file ./img/old_tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    l sep+=1em
  },
  [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
      draw,
      color = NavyBlue
      [{Search Heuristics},
        draw,
        color = BurntOrange,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{\textbf{Random} \textbf{Sampling}}, draw]
        [{Reachable Optima},
          draw,
          color = BurntOrange
          [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
            draw,
            color = BurntOrange
            [{Strong $corr(f(X),d(X,X_{*}))$?},
              draw,
              color = NavyBlue
              [{More Global},
                draw,
                color = BurntOrange,
                edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                [, phantom]]
              [{More Local},
                draw,
                color = BurntOrange,
                edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                [, phantom]
                [{High local optima density?},
                  draw,
                  color = NavyBlue
                  [{Exploit Steepest Descent},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                    [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                  [{Allows\\exploration},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                    [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
            [,phantom]]
          [,phantom]]]
      [{Statistical Learning},
        draw,
        color = BurntOrange,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Parametric Learning},
          draw,
          color = BurntOrange
          [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
            draw,
            color = BurntOrange
            [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
            [, phantom]]
          [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
            draw,
            color = BurntOrange
            [, phantom]
            [{Check for model adequacy?},
              draw,
              alias = adequacy,
              color = NavyBlue
              [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                draw,
                alias = interactions,
                color = NavyBlue,
                edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                  edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                  draw
                  [, phantom]
                  [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                    edge = {-stealth, ForestGreen, semithick},
                    edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                    draw,
                    alias = estimate,
                    color = ForestGreen]]
                [{\textbf{Optimal} \textbf{Design}},
                  draw,
                  alias = optimal,
                  edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [{\textbf{Space-filling} \textbf{Designs}},
                draw,
                edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                [, phantom]
                [{Model selection},
                  edge = {-stealth, ForestGreen, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                  draw,
                  alias = selection,
                  color = ForestGreen]]]]]
        [{Nonparametric Learning},
          draw,
          color = BurntOrange
          [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
            draw
            [, phantom]
            [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
              edge = {-stealth, ForestGreen, semithick},
              draw,
              alias = uncertainty,
              color = ForestGreen
              [{Minimize $uncertainty(\hat{f}(X))$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                draw,
                color = ForestGreen]
              [{Minimize $\hat{f}(X)$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                draw,
                color = ForestGreen]
              [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                edge = {ForestGreen, semithick},
                edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                draw,
                color = ForestGreen]]]
          [{\textbf{Gaussian} \textbf{Process Regression}},
            alias = gaussian,
            draw]
          [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
  \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
  \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
  \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
  \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/old_tree.pdf]]

**** Complete Classification Tree for Function Minimization Methods
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" "\\usepackage{amsmath}" "\\DeclareMathOperator*{\\argmin}{arg\\,min}" )
#+HEADER: :exports results :results raw :file ./img/tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    l sep+=1.4em
  },
  [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = [x_1,\dots,x_k]^{\top} \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Is $f(X)$ known,\\or cheap to measure?},
      draw
      [{\textit{Search Heuristics}\\Leverage measurements\\to minimize $f(X)$},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Can $\nabla{}f$ and $\mathbf{H}f$ be computed?},
          draw
          [{Stochastic\\Descent},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{Population\\based\\\textbf{Genetic}\\\textbf{Algorithms}\\$\dots$}, draw]
            [{Direct\\\textbf{Nelder-Mead}\\$\dots$}, draw]
            [{Single-state\\\textbf{Simulated}\\\textbf{Annealing}\\$\dots$}, draw]]
          [{\textbf{Gradient}\\\textbf{Descent}\\$\dots$},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]]
      [{\textit{Statistical Learning}\\Construct a surrogate\\$\hat{f}(\cdot, \theta(X))$, estimate $\theta(X)$},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{Is $|\theta(X)|$ finite?},
          draw
          [{Parametric Models},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
              draw
              [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]]
            [{\textbf{Design of Experiments}\\using linear models\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
              draw
              [{Find \textit{significant} $x_i \in X$,\\or check \textit{lack of fit} of $\hat{f}$?},
                draw,
                alias = adequacy
                [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                  draw,
                  alias = interactions,
                  edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                  [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                    draw
                    [, phantom]
                    [{Pick $\hat{X}_{*} = \argmin_{X \in \mathcal{X}}\hat{f}$,\\reduce dimension of $\mathcal{X}$},
                      edge = {-stealth, Black, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      alias = estimate,
                      color = Black]]
                  [{\textbf{Optimal}\\\textbf{Design}},
                    draw,
                    alias = optimal,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                [, phantom]
                [, phantom]
                [, phantom]
                [, phantom]
                [, phantom]
                [, phantom]
                [{\textbf{Space-filling}\\\textbf{Designs}},
                  draw,
                  edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                  [{Model selection},
                    edge = {-stealth, Black, semithick},
                    edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                    draw,
                    alias = selection,
                    color = Black]]]]]
          [{Nonparametric Models},
            draw,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
              draw
              [, phantom]
              [, phantom]
              [, phantom]
              [, phantom]
              [{Estimate $\hat{f}(\cdot)$ and\\$uncertainty(\hat{f}(\cdot))$},
                edge = {-stealth, Black, semithick},
                draw,
                alias = uncertainty,
                color = Black
                [{Minimize\\$uncertainty(\hat{f}(X))$},
                  edge = {Black, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                  draw,
                  color = Black]
                [{Minimize $\hat{f}(X)$},
                  edge = {Black, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                  draw,
                  color = Black]
                [{Minimize \textit{EI}, or\\$\hat{f}(X) - uncertainty(\hat{f}(X))$},
                  edge = {Black, semithick},
                  edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                  draw,
                  color = Black]]]
            [{\textbf{Gaussian}\\\textbf{Process Regression}},
              alias = gaussian,
              draw]]]]]]
  \draw [-stealth, semithick, Black](selection) to [bend left=22] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
  \draw [-stealth, semithick, Black](estimate.east) to [bend right=32] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
  \draw [-stealth, semithick, Black](gaussian) to (uncertainty);
  \draw [-stealth, semithick, Black](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/tree.pdf]]

**** Simplest Version
#+begin_SRC emacs-lisp :eval no-export
(setq org-format-latex-header "\\documentclass{standalone}
[PACKAGES]
[DEFAULT-PACKAGES]
\\pagestyle{empty} % do not remove")
#+end_SRC

#+RESULTS:
: \documentclass{standalone}
: [PACKAGES]
: [DEFAULT-PACKAGES]
: \pagestyle{empty} % do not remove

#+HEADER: :headers '("\\usepackage[dvipsnames]{xcolor}" "\\usepackage{tikz}" "\\usepackage{forest}" )
#+HEADER: :exports results :results raw :file ./img/simplest_tree.pdf
#+begin_src latex :eval no-export
\begin{forest}
  for tree={%
    anchor = north,
    align = center,
    if n children=0{tier=terminal}{},
    l sep+=1em
  },
  [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
    draw,
    [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$?},
      draw,
      [{Search\\Heuristics},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
        [{\textbf{Random}\\\textbf{Sampling}}, draw]
        [{Reachable\\Optima},
          draw,
          [{Introduce a\\\textit{population} of $X$\\[0.5em]\textbf{Genetic}\\\textbf{Algorithms}}, draw]
          [{Estimate $f^{\prime}(X)$\\[0.5em]\textbf{Gradient}\\\textbf{Descent}}, draw]
          [{Allow worse $f(X)$\\[0.5em]\textbf{Simulated}\\\textbf{Annealing}}, draw]]]
      [{Statistical\\Learning},
        draw,
        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
        [{Parametric\\Learning},
          draw,
          [{\textbf{Independent}\\\textbf{Bandits}}, draw]
          [{Choose best $X$ for\\
              a Linear Model\\
              $\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$\\[0.5em]
              \textbf{Experimental}\\\textbf{Design}},
              draw]
          [{Unsupervised\\[0.5em]\textbf{Clustering}}, draw]]
        [{Nonparametric\\Learning},
          draw,
          [{\textbf{Decision}\\\textbf{Trees}},
            draw]
          [{\textbf{Gaussian}\\\textbf{Process}\\\textbf{Regression}},
            draw]
          [{\textbf{Neural}\\\textbf{Networks}}, draw]]]]]
\end{forest}
#+end_src

#+RESULTS:
[[file:./img/simplest_tree.pdf]]

**** Representing Sampling Strategies
***** Generate Fake Data with Algorithms
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
fake_gradient_df_seed <- data.frame(x1 = c(1, 1, 99, 99),
                                    x2 = c(1, 99, 1, 99),
                                    run = c(1, 2, 3, 4),
                                    sign1 = c(1, 1, -1, -1),
                                    sign2 = c(1, -1, 1, -1))

fake_gradient_df <- NULL

for(run_id in c(1, 2, 3, 4)) {
    if (is.null(fake_gradient_df)) {
        fake_gradient_df <- fake_gradient_df_seed[run_id, ]
    } else {
        fake_gradient_df <- rbind(fake_gradient_df, fake_gradient_df_seed[run_id, ])
    }

    for(i in 1:10) {
        row <- nrow(fake_gradient_df)
        fake_descent <- data.frame(x1 = ceiling(fake_gradient_df[row, "x1"] + (fake_gradient_df[row, "sign1"] * runif(1, min = 1, max = 5))),
                                   x2 = ceiling(fake_gradient_df[row, "x2"] + (fake_gradient_df[row, "sign2"] * runif(1, min = 1, max = 5))),
                                   run = fake_gradient_df[row, "run"],
                                   sign1 = fake_gradient_df[row, "sign1"],
                                   sign2 = fake_gradient_df[row, "sign2"])
        fake_gradient_df <- rbind(fake_gradient_df, fake_descent)
    }
}

fake_gradient_df$name <- rep("Gradient Descent", nrow(fake_gradient_df))
df <- bind_rows(df, fake_gradient_df)

fake_sima_df_seed <- data.frame(x1 = c(30, 30, 70, 70),
                                x2 = c(30, 70, 30, 70),
                                run = c(1, 2, 3, 4),
                                sign1 = c(1, 1, -1, -1),
                                sign2 = c(1, -1, 1, -1))

fake_sima_df <- NULL

for(run_id in c(1, 2, 3, 4)) {
    if (is.null(fake_sima_df)) {
        fake_sima_df <- fake_sima_df_seed[run_id, ]
    } else {
        fake_sima_df <- rbind(fake_sima_df, fake_sima_df_seed[run_id, ])
    }

    for(i in 1:10) {
        row <- nrow(fake_sima_df)
        fake_descent <- data.frame(x1 = ceiling(fake_sima_df[row, "x1"] + (fake_sima_df[row, "sign1"] * runif(1, min = -5, max = 5))),
                                   x2 = ceiling(fake_sima_df[row, "x2"] + (fake_sima_df[row, "sign2"] * runif(1, min = -5, max = 5))),
                                   run = fake_sima_df[row, "run"],
                                   sign1 = fake_sima_df[row, "sign1"],
                                   sign2 = fake_sima_df[row, "sign2"])
        fake_sima_df <- rbind(fake_sima_df, fake_descent)
    }
}

fake_sima_df$name <- rep("Simulated Annealing", nrow(fake_sima_df))
df <- bind_rows(df, fake_sima_df)
#+END_SRC

#+RESULTS:

***** Generate Data
#+HEADER: :results output :session *R* :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)
library(RColorBrewer)

sample_size <- 50
pre_sample_size <- 30 * sample_size
search_space_size <- 100

center_x1 <- (search_space_size / 2) - 30
center_x2 <- (search_space_size / 2) - 30

get_cost <- function(df) {
    return(((df$x1 - center_x1) ^ 2) + ((df$x2 - center_x2) ^ 2) + ((abs((df$x1 - center_x1) * (df$x2 - center_x2)))**.7 * sin((df$x1 - center_x1) * (df$x2 - center_x2))))
}

objective_df <- expand.grid(seq(0, search_space_size, 1),
                            seq(0, search_space_size, 1))
names(objective_df) <- c("x1", "x2")

objective_df$Y <- get_cost(objective_df)

sima_samples <- 15

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_sima_df <- as.data.frame(locator(n = sima_samples, type = "l"))
names(fake_sima_df) <- c("x1", "x2")
dev.off()

fake_sima_df$run <- c(rep(1, nrow(fake_sima_df)))
fake_sima_df$name <- rep("Simulated Annealing", nrow(fake_sima_df))

fake_sima_df$cost <- get_cost(fake_sima_df)
fake_sima_df$min <- fake_sima_df$cost == min(fake_sima_df$cost)

df <- fake_sima_df

descent_samples <- 20

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_descent_df <- as.data.frame(locator(n = descent_samples, type = "l"))
names(fake_descent_df) <- c("x1", "x2")
dev.off()

paths <- 5
fake_runs <- rep(1, descent_samples / paths)
for(i in 2:paths){
    fake_runs <- c(fake_runs, rep(i, descent_samples / paths))
}

fake_descent_df$run <- fake_runs
fake_descent_df$name <- rep("Gradient Descent", nrow(fake_descent_df))

fake_descent_df$cost <- get_cost(fake_descent_df)
fake_descent_df$min <- fake_descent_df$cost == min(fake_descent_df$cost)

df <- bind_rows(df, fake_descent_df)

rs_df <- data.frame(x1 = sample(0:search_space_size, sample_size, replace = T),
                    x2 = sample(0:search_space_size, sample_size, replace = T))
rs_df$name <- rep("Random Sampling", nrow(rs_df))

rs_df$cost <- get_cost(rs_df)
rs_df$min <- rs_df$cost == min(rs_df$cost)

df <- bind_rows(df, rs_df)

lhs_df <- lhs.design(nruns = sample_size, nfactors = 2, digits = 0, type = "maximin",
                     factor.names = list(x1 = c(0, search_space_size), x2 = c(0, search_space_size)))
lhs_df$name <- rep("Latin Hypercube Sampling", nrow(lhs_df))

lhs_df$cost <- get_cost(lhs_df)
lhs_df$min <- lhs_df$cost == min(lhs_df$cost)

df <- bind_rows(df, lhs_df)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2, full_factorial, nTrials = sample_size)
dopt_df <- output$design

dopt_df$name <- rep("DOpt. Linear Model", nrow(dopt_df))
dopt_df$cost <- get_cost(dopt_df)
dopt_df$min <- rep(FALSE, nrow(dopt_df))

regression <- lm(cost ~ x1 + x2, data = dopt_df)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Linear Model"
best$min <- TRUE

dopt_df <- bind_rows(dopt_df, best)
df <- bind_rows(df, dopt_df)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
doptq_df <- output$design

doptq_df$name <- rep("DOpt. Quadratic Model", nrow(doptq_df))
doptq_df$cost <- get_cost(doptq_df)
doptq_df$min <- rep(FALSE, nrow(doptq_df))

regression <- lm(cost ~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), data = doptq_df)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Quadratic Model"
best$min <- TRUE

doptq_df <- bind_rows(doptq_df, best)
df <- bind_rows(df, doptq_df)
#+END_SRC

#+RESULTS:
#+begin_example
Loading required package: FrF2
Loading required package: DoE.base
Loading required package: grid
Loading required package: conf.design
Registered S3 method overwritten by 'DoE.base':
  method           from
  factorize.factor conf.design

Attaching package: ‘DoE.base’

The following objects are masked from ‘package:stats’:

    aov, lm

The following object is masked from ‘package:graphics’:

    plot.design

The following object is masked from ‘package:base’:

    lengths

Loading required package: rsm
null device
          1
null device
          1
#+end_example

***** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file ./img/sampling_comparison.pdf :exports none :width 15 :height 11.5 :eval no-export
#+BEGIN_SRC R
library(extrafont)
df$facet <- factor(df$name, levels = c("Random Sampling",
                                           "Latin Hypercube Sampling",
                                           "Gradient Descent",
                                           "Simulated Annealing",
                                           "DOpt. Linear Model",
                                           "DOpt. Quadratic Model"))
ggplot(df, aes(x = x1, y = x2)) +
    scale_x_continuous(limits = c(-1, 101),
                       expand = c(0, 0)) +
    scale_y_continuous(limits = c(-1, 101),
                       expand = c(0, 0)) +
    xlab("x") +
    ylab("y") +
    facet_wrap(facet ~ .,
               ncol = 3) +
    #geom_raster(data = objective_df, aes(fill = Y), show.legend = FALSE) +
    #geom_contour(data = objective_df, aes(z = Y), colour = "white", linetype = 8) + #, breaks = 1 * (2 ^ (2:20))) +
    geom_contour(data = objective_df,
                 aes(z = Y),
                 linetype = 1,
                 colour = "black",
                 alpha = 0.6,
                 show.legend = FALSE,
                 breaks = 1 * (2 ^ (4:20))) +
    geom_path(data = subset(df,
                            name %in% c("Gradient Descent", "Simulated Annealing")),
              aes(group = run),
              color = "black",
              alpha = 0.55,
              size = 1) +
    geom_point(shape = 19,
               size = 3,
               colour = "black",
               alpha = 0.55) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.55,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.55,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.45,
                width = 8,
                height = 8) +
    geom_jitter(data = subset(df,
                              name %in% c("Gradient Descent")),
                color = "black",
                size = 3,
                shape = 4,
                alpha = 0.45,
                width = 8,
                height = 8) +
    scale_fill_distiller(palette = "Greys",
                         direction = -1,
                         limits = c(min(objective_df$Y) - 1000,
                                    max(objective_df$Y))) +
    geom_point(data = subset(df,
                             min == TRUE),
               color = "red",
               shape = 3,
               size = 12,
               alpha = 1,
               stroke = 2) +
    theme_bw(base_size = 35) +
    theme(panel.grid = element_blank(),
          text = element_text(family="serif"),
          strip.background = element_rect(fill = "white"),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/sampling_comparison.pdf]]

**** Search Space Introduction
***** Simple Search Spaces
****** Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
a = 0.6
c = -30

l = 3
k = -6
#+end_SRC

#+RESULTS:

****** Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

resolution = 100

new_grid = expand.grid(seq(from = -10.0, to = 10.0, length.out = resolution),
                       seq(from = -10.0, to = 10.0, length.out = resolution))

df = data.frame(x = new_grid$Var1,
                y = new_grid$Var2)

amp_x = 1
amp_y = 1.5

#noise_sd = 0.033
noise_sd = 20

# f <- function(x, y){
#     return((x ^ 2) * y)
# }

# f <- function(x, y){
#     return(((x ^ 2) * amp_x) + ((y ^ 2) * amp_y))
# }

f <- function(y, x){
    return(((x + (2 * y) - 7) ^ 2) + (((2 * x) + y - 5)^ 2))
}


df$z = f(df$x, df$y) +
    rnorm(resolution ^ 2,
          mean = 0.0,
          sd = noise_sd)

df$type = "noise"

plot_df = df

df$z = f(df$x, df$y)

df$type = "no_noise"

plot_df = bind_rows(plot_df, df)

df$z = f(df$x, df$y) +
    rnorm(resolution ^ 2,
          mean = 0.0,
          sd = noise_sd)

df$type = "constrained"

constraint = data.frame(x_min = 0.1,
                        x_max = 0.4,
                        y_min = -0.3,
                        y_max = 0.4)

quad_constraint <- function(x1, x2, a, c){
    return(x2 < ((a * (x1 ^ 2)) + c))
}

lin_constraint <- function(x1, x2, l, k){
    return(x1 > l | x1 < k)
}

#df[quad_constraint(df$y, df$x, a, c), "z"] = NA
df[lin_constraint(df$y, df$x, l, k), "z"] = NA

plot_df = bind_rows(plot_df, df)

write.csv(plot_df,
          "data/search_spaces/simple_search_space.csv",
          row.names = FALSE)

#+end_SRC

#+RESULTS:

****** Search Spaces Overview
j#+begin_SRC R :results graphics output :session *R* :file "./img/simple_search_space.pdf" :width 30 :height 21 :eval no-export
library(dplyr)
library(ggplot2)
library(paletteer)
library(patchwork)
library(latex2exp)

plot_df = read.csv("data/search_spaces/simple_search_space.csv",
                   header = TRUE)

color_palette = "viridis::viridis"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 10
optimum_stroke = 3
optima_shape = 4

noise_sd = 0.033

constraint_label = "Not feasible"

constraint = data.frame(x_min = 0.1,
                        x_max = 0.4,
                        y_min = -0.3,
                        y_max = 0.4)

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

theme_config_line = theme(axis.title.y = element_text(angle = 0,
                                                      margin = margin(t = 0,
                                                                      b = 0,
                                                                      r = 0.7,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      vjust = 0.5,
                                                      size = title_size),
                          axis.title.x = element_text(margin = margin(t = 0.4,
                                                                      b = -0.7,
                                                                      r = 0,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      size = title_size),
                          axis.ticks = element_blank(),
                          axis.text.x = element_blank(),
                          axis.text.y = element_blank())

p_no_noise = ggplot() +
    scale_x_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    scale_y_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    geom_contour(data = filter(plot_df,
                               type == "no_noise"),
                 aes(x = x,
                     y = y,
                     z = z,
                     color = ..level..),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "no_noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    scale_y_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    geom_contour(data = filter(plot_df, type == "noise"),
                 aes(x = x,
                     y = y,
                     z = z,
                     color = ..level..),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    scale_y_continuous(limits = c(-10, 10), expand = c(0, 0)) +
    geom_contour(data = filter(plot_df,
                               type == "constrained"),
                 aes(x = x,
                     y = y,
                     z = z,
                     color = ..level..),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(type == "constrained") %>%
                    filter(is.na(z)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(type == "constrained") %>%
                    filter(is.na(z)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(type == "constrained") %>%
                   filter(!is.na(z)) %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    # geom_text(aes(x = 0.6,
    #               y = 0.83),
    #           size = 12,
    #           color = "gray50",
    #           angle = 0,
    #           label = constraint_label) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "no_noise") %>%
                  filter(y == plot_df %>%
                         filter(type == "no_noise") %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = line_thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "no_noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(d)}")) +
    ylab(TeX("$y$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_noise_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "noise") %>%
                  filter(y == plot_df %>%
                         filter(type == "noise") %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = line_thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(e)}")) +
    ylab(TeX("$y$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_constrained_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "constrained") %>%
                  filter(y == plot_df %>%
                         filter(type == "constrained") %>%
                         filter(!is.na(z)) %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = line_thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "constrained") %>%
                   filter(!is.na(z)) %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    # geom_vline(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #            aes(xintercept = max(x)),
    #            linetype = 2,
    #            color = constraint_border,
    #            size = 1.3) +
    # geom_ribbon(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #             aes(y = z,
    #                 xmax = max(x),
    #                 xmin = 10),
    #             alpha = 0.1) +
    # geom_text(aes(x = (constraint$x_max - constraint$x_min),
    #               y = 0.455),
    #           size = 12,
    #           hjust = -0.64,
    #           vjust = 1.1,
    #           color = "gray58",
    #           angle = -90,
    #           label = constraint_label) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(f)}")) +
    ylab(TeX("$y$")) +
    theme_bw(base_size = base_size) +
    theme_config_line

(p_no_noise | p_noise | p_constrained) /
    (p_no_noise_line | p_noise_line | p_constrained_line)
#+end_SRC

#+RESULTS:
[[file:./img/simple_search_space.pdf]]

***** Local, global optimum, convexity
****** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/optima_convexity.svg" :width 20 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(paletteer)
library(patchwork)
library(latex2exp)

plot_df = read.csv("data/search_spaces/simple_search_space.csv",
                   header = TRUE)

color_palette = "viridis::viridis"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.5
base_size = 32
title_size = 39
optimum_size = 10
optimum_stroke = 3

optima_shape = 4
constraint_label = "Not feasible"

constraint = data.frame(x_min = 0.1,
                        x_max = 0.4,
                        y_min = -0.3,
                        y_max = 0.4)

convex_line = plot_df %>%
    filter(y == plot_df %>%
           filter(type == "no_noise") %>%
           filter(z == min(z)) %>%
           select(y) %>% first()) %>%
    filter(type == "no_noise") %>%
    mutate(n = row_number()) %>%
    filter(n == round(0.08 * max(n)) |
           n == round(0.88 * max(n)))

convex_line_model = lm(z ~ x, convex_line)

convex_line_prediction = plot_df %>%
    filter(y == plot_df %>%
           filter(type == "no_noise") %>%
           filter(z == min(z)) %>%
           select(y) %>%
           first()) %>%
    filter(type == "no_noise")

convex_line_prediction$pred = predict(convex_line_model, convex_line_prediction)

theme_config_line = theme(axis.title.y = element_text(angle = 0,
                                                      margin = margin(t = 0,
                                                                      b = 0,
                                                                      r = 0.7,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      vjust = 0.5,
                                                      size = title_size),
                          axis.title.x = element_text(margin = margin(t = 0.4,
                                                                      b = -0.7,
                                                                      r = 0,
                                                                      l = 0,
                                                                      unit = "cm"),
                                                      size = title_size),
                          axis.ticks = element_blank(),
                          axis.text.x = element_blank(),
                          axis.text.y = element_blank())

constrained_local_optima = data.frame(x = c(-0.31, -0.47, -0.542),
                                      y = c(0.14, 0.24, 0.34),
                                      x_b = c(-0.25, -0.25, -0.25),
                                      y_b = c(0.5, 0.5, 0.5))

local_optima_curvature = 0.5

p_no_noise_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "no_noise") %>%
                  filter(y == plot_df %>%
                         filter(type == "no_noise") %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(type == "no_noise") %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = optima_shape) +
    geom_line(data = convex_line_prediction,
              aes(x = x,
                  y = pred),
              color = constraint_border,
              linetype = 3,
              size = 1.9,
              show.legend = FALSE) +
    geom_line(data = convex_line,
              aes(x = x,
                  y = z),
              color = constraint_border,
              size = 2.3,
              show.legend = FALSE) +
    geom_point(data = convex_line,
               aes(x = x,
                   y = z,
                   color = z),
               size = 6,
               show.legend = FALSE) +
    # geom_curve(aes(x = 0.0,
    #                y = 0.1,
    #                xend = -0.01,
    #                yend = 0.01),
    #            angle = 90,
    #            curvature = 0.05,
    #            lineend = "round",
    #            linejoin = "round",
    #            size = 1.4,
    #            color = constraint_border,
    #            arrow = arrow(length = unit(0.3, "inches"))) +
    # geom_rect(aes(xmin = -0.25,
    #               xmax = 0.25,
    #               ymin = 0.1,
    #               ymax = 0.26),
    #           show.legend = FALSE,
    #           fill = "gray94",
    #           color = constraint_border,
    #           size = 0.4) +
    # geom_text(aes(x = 0., y = 0.18),
    #           size = 12,
    #           color = constraint_border,
    #           label = TeX("\\overset{Global}{Optimum}")) +
    # geom_rect(aes(xmin = -0.3,
    #               xmax = 0.3,
    #               ymin = 0.69,
    #               ymax = 0.85),
    #           show.legend = FALSE,
    #           fill = "gray94",
    #           color = constraint_border,
    #           size = 0.4) +
    # geom_text(aes(x = 0., y = 0.77),
    #           size = 12,
    #           color = constraint_border,
    #           label = TeX("\\overset{$y = f(\\cdot,\\, x_2)$}{is convex}")) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$y$")) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_constrained_line = ggplot() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    geom_line(data = plot_df %>%
                  filter(type == "constrained") %>%
                  filter(y == plot_df %>%
                         filter(type == "constrained") %>%
                         filter(!is.na(z)) %>%
                         filter(z == min(z)) %>%
                         select(y) %>% first()),
                 aes(x = x,
                     y = z,
                     color = z),
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    # geom_curve(data = constrained_local_optima,
    #            aes(x = x_b,
    #                y = y_b,
    #                xend = x,
    #                yend = y),
    #            angle = 90,
    #            curvature = 0.3,
    #            lineend = "round",
    #            linejoin = "round",
    #            size = 1.4,
    #            color = constraint_border,
    #            arrow = arrow(length = unit(0.3, "inches"))) +
    # geom_vline(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #            aes(xintercept = max(x)),
    #            linetype = 2,
    #            color = constraint_border,
    #            size = 1.3) +
    # geom_ribbon(data = plot_df %>%
    #                filter(type == "constrained") %>%
    #                filter(y == plot_df %>%
    #                       filter(type == "constrained") %>%
    #                       filter(!is.na(z)) %>%
    #                       filter(z == min(z)) %>%
    #                       select(y) %>% first()) %>%
    #                filter(!is.na(z)),
    #             aes(y = z,
    #                 xmax = max(x),
    #                 xmin = 1),
    #             alpha = 0.1) +
    # geom_rect(aes(xmin = -0.22,
    #               xmax = 0.55,
    #               ymin = 0.45,
    #               ymax = 0.55),
    #           show.legend = FALSE,
    #           fill = "gray94",
    #           color = constraint_border,
    #           size = 0.4) +
    # geom_text(aes(x = 0.16, y = 0.5),
    #           size = 12,
    #           color = constraint_border,
    #           label = "Local Optima") +
    geom_point(data = plot_df %>%
                   filter(type == "constrained") %>%
                   filter(!is.na(z)) %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = z),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = optima_shape) +
    scale_color_paletteer_c(palette = color_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$y$")) +
    theme_bw(base_size = base_size) +
    theme_config_line

p_no_noise_line | p_constrained_line
#+end_SRC

#+RESULTS:
[[file:./img/optima_convexity.svg]]
*** Descent Template for Booth's Function (With gradient)
**** Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
l = 3
k = -6
#+end_SRC

#+RESULTS:
**** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_descent_gradient_template.pdf" :width 30 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(pracma)
library(paletteer)
library(patchwork)
library(latex2exp)

color_palette = "viridis::viridis"
fill_palette = "ggthemes::Gray"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 8
optimum_stroke = 3
optima_shape = 4
contour_color = "gray65"
grad_alpha = 0.45

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

resolution = 100

noise_sd = 20

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)

grad_resolution = 6
grad_length = 0.83
picked = unique(plot_df$x)[seq(0, resolution, by = grad_resolution)]

scale_limits = c(-10.1, 10.1)

p_no_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    color = sqrt((x_grad ^ 2) + (y_grad ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_noisy),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                alpha = grad_alpha,
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked) %>%
                    filter(!is.na(z_constrained)),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_constrained),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(!is.na(z_constrained)) %>%
                   filter(z_constrained == min(z_constrained)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise | p_noise | p_constrained
#+end_SRC

#+RESULTS:
[[file:./img/booth_descent_gradient_template.pdf]]

*** Methods Based on Derivatives
**** Gradient of Booth's Function
***** Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
l = 3
k = -6
#+end_SRC

#+RESULTS:
***** Generate Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)

resolution = 100

new_grid = expand.grid(seq(from = -10.0, to = 10.0, length.out = resolution),
                       seq(from = -10.0, to = 10.0, length.out = resolution))

df = data.frame(x = new_grid$Var1,
                y = new_grid$Var2)

noise_sd = 20
grad_noise_sd = 30

f <- function(y, x){
    return(((x + (2 * y) - 7) ^ 2) + (((2 * x) + y - 5)^ 2))
}

f_grad <- function(x, y){
    return(data.frame(grad_x = (10 * x) + (8 * y) - 34,
                      grad_y = (8 * x) + (10 * y) - 38))
}

lin_constraint <- function(x1, x2, l, k){
    return(x1 > l | x1 < k)
}

df$z = f(df$x, df$y)

noise = rnorm(resolution ^ 2,
              mean = 0.0,
              sd = noise_sd)

grad_noise_x = rnorm(resolution ^ 2,
                     mean = 0.0,
                     sd = grad_noise_sd)
grad_noise_y = rnorm(resolution ^ 2,
                     mean = 0.0,
                     sd = grad_noise_sd)

df$z_noisy = f(df$x, df$y) + noise

df$z_constrained = df$z_noisy
df[lin_constraint(df$y, df$x, l, k), "z_constrained"] = NA

grad_f = f_grad(df$x, df$y)
grad_f_noisy = f_grad(df$x, df$y)

grad_f_noisy$grad_x = grad_f_noisy$grad_x + grad_noise_x
grad_f_noisy$grad_y = grad_f_noisy$grad_y + grad_noise_y

plot_df = df %>%
    mutate(x_grad = grad_f$grad_x,
           y_grad = grad_f$grad_y,
           x_grad_noisy = grad_f_noisy$grad_x,
           y_grad_noisy = grad_f_noisy$grad_y)

write.csv(plot_df,
          "data/search_spaces/gradient.csv",
          row.names = FALSE)
#+end_SRC

#+RESULTS:

***** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_gradient.pdf" :width 30 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(pracma)
library(paletteer)
library(patchwork)
library(latex2exp)

color_palette = "viridis::viridis"
fill_palette = "ggthemes::Green"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 8
optimum_stroke = 3
optima_shape = 4
contour_color = "gray65"

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

resolution = 100

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)

grad_resolution = 6
grad_length = 0.83
picked = unique(plot_df$x)[seq(0, resolution, by = grad_resolution)]

scale_limits = c(-10.1, 10.1)

p_no_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    color = sqrt((x_grad ^ 2) + (y_grad ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_noisy),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked) %>%
                    filter(!is.na(z_constrained)),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                show.legend = FALSE) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_constrained),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(!is.na(z_constrained)) %>%
                   filter(z_constrained == min(z_constrained)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise | p_noise | p_constrained
#+end_SRC

#+RESULTS:
[[file:./img/booth_gradient.pdf]]

**** Descent Template for Booth's Function (With gradient)
***** Constants
#+begin_SRC R :results output :session *R* :eval no-export :exports results
l = 3
k = -6
#+end_SRC

#+RESULTS:
***** Plot
#+begin_SRC R :results graphics output :session *R* :file "./img/booth_descent_gradient_template.pdf" :width 30 :height 10.5 :eval no-export
library(dplyr)
library(ggplot2)
library(pracma)
library(paletteer)
library(patchwork)
library(latex2exp)

color_palette = "viridis::viridis"
fill_palette = "ggthemes::Gray"

constraint_border = paletteer_c(palette = "viridis::viridis",
                                n = 2)[[1]]

thickness = 1.3
line_thickness = 1.4
base_size = 32
title_size = 39
optimum_size = 8
optimum_stroke = 3
optima_shape = 4
contour_color = "gray65"
grad_alpha = 0.45

theme_config = theme(axis.title.y = element_text(angle = 0,
                                                 margin = margin(t = 0,
                                                                 b = 0,
                                                                 r = 0.3,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 vjust = 0.5,
                                                 size = title_size),
                     axis.title.x = element_text(margin = margin(t = 0.4,
                                                                 b = -0.7,
                                                                 r = 0,
                                                                 l = 0,
                                                                 unit = "cm"),
                                                 size = title_size),
                     axis.ticks = element_blank(),
                     axis.text.x = element_blank(),
                     axis.text.y = element_blank())

resolution = 100

noise_sd = 20

plot_df = read.csv("data/search_spaces/gradient.csv", header = TRUE)

grad_resolution = 6
grad_length = 0.83
picked = unique(plot_df$x)[seq(0, resolution, by = grad_resolution)]

scale_limits = c(-10.1, 10.1)

p_no_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = y,
                    yend = x,
                    y = (x + (grad_length *
                              (x_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    x = (y + (grad_length *
                              (y_grad / sqrt((x_grad ^ 2) +
                                             (y_grad ^ 2))))),
                    color = sqrt((x_grad ^ 2) + (y_grad ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(a)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_noise = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_noisy),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                alpha = grad_alpha,
                size = 1.4,
                show.legend = FALSE) +
    geom_point(data = plot_df %>%
                   filter(z == min(z)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    xlab(TeX("\\overset{$x_2$}{(b)}")) +
    ylab(TeX("$x_1$")) +
    #scale_color_paletteer_c(palette = color_palette, direction = 1) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    theme_bw(base_size = base_size) +
    theme_config

p_constrained = ggplot() +
    scale_x_continuous(limits = scale_limits, expand = c(0, 0)) +
    scale_y_continuous(limits = scale_limits, expand = c(0, 0)) +
    geom_segment(data = plot_df %>%
                    filter(x %in% picked & y %in% picked) %>%
                    filter(!is.na(z_constrained)),
                aes(xend = x,
                    yend = y,
                    x = (x + (grad_length *
                              (x_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    y = (y + (grad_length *
                              (y_grad_noisy / sqrt((x_grad_noisy ^ 2) +
                                             (y_grad_noisy ^ 2))))),
                    color = sqrt((x_grad_noisy ^ 2) + (y_grad_noisy ^ 2))),
                arrow = arrow(type = "closed", length = unit(0.01, "npc")),
                size = 1.4,
                alpha = grad_alpha,
                show.legend = FALSE) +
    geom_contour(data = plot_df,
                 aes(x = x,
                     y = y,
                     z = z_constrained),
                 color = contour_color,
                 size = thickness,
                 linejoin = "round",
                 lineend = "round",
                 show.legend = FALSE) +
    geom_hline(yintercept = l,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_hline(yintercept = k,
               linetype = 2,
               color = constraint_border,
               size = 1.3) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = l,
                    ymax = 10),
                alpha = 0.1) +
    geom_ribbon(data = plot_df %>%
                    filter(is.na(z_constrained)),
                aes(x = x,
                    ymin = -10,
                    ymax = k),
                alpha = 0.1) +
    geom_point(data = plot_df %>%
                   filter(!is.na(z_constrained)) %>%
                   filter(z_constrained == min(z_constrained)),
               aes(x = x,
                   y = y),
               color = "red",
               size = optimum_size,
               stroke = optimum_stroke,
               shape = 4) +
    scale_color_paletteer_c(palette = fill_palette, direction = 1) +
    xlab(TeX("\\overset{$x_2$}{(c)}")) +
    ylab(TeX("$x_1$")) +
    theme_bw(base_size = base_size) +
    theme_config

p_no_noise | p_noise | p_constrained
#+end_SRC

#+RESULTS:
[[file:./img/booth_descent_gradient_template.pdf]]

* Historical Hardware Design Trends and Consequences for Code Optimization
** Introduction
Computer  performance has  sustained exponential  increases over  the last  half
century,  despite   current  physical  limits  on   hardware  design.   Software
optimization  has  performed an  increasingly  substantial  role on  performance
improvement over the last 15 years, requiring the exploration of larger and more
complex search spaces than ever before.

Autotuning  methods  are one  approach  to  tackle performance  optimization  of
complex search  spaces, enabling exploitation of  existing relationships between
program  parameters and  performance.   We can  derive  autotuning methods  from
well-established  statistics,  although   their  usage  is  not   common  in  or
standardized for autotuning domains.

Initial work on this thesis studied  the effectiveness of classical and standard
search heuristics,  such as  Simulated Annealing,  on autotuning  problems.  The
first target autotuning domain was the set  of parameters of a compiler for CUDA
programs.  The search heuristics for this  case study were implemented using the
OpenTuner framework\nbsp\cite{ansel2014opentuner}, and  consisted of an ensemble
of  search  heuristics  coordinated  by a  Multi-Armed  Bandit  algorithm.   The
autotuner  searched  for a  set  of  compilation  parameters that  optimized  17
heterogeneous  GPU  kernels, from  a  set  of approximately  $10^{23}$  possible
combinations of all  parameters.  With 1.5h autotuning runs we  have achieved up
to  $4\times$  speedup  in  comparison   with  the  CUDA  compiler's  high-level
optimizations.   The  compilation  and  execution  times  of  programs  in  this
autotuning domain are relatively fast, and were in the order of a few seconds to
a minute.  Since measurement costs are relatively small, search heuristics could
find  good optimizations  using  as  many measurements  as  needed.  A  detailed
description      of      this      work       is      available      in      our
paper\nbsp\cite{bruel2017autotuning}   published   in    the   /Concurrency   and
Computation: Practice and Experience/ journal.

The  next  case  study  was  developed  in  collaboration  with  /Hewlett-Packard
Enterprise/,  and  consisted of  applying  the  same heuristics-based  autotuning
approach to the  configuration of parameters involved in the  generation of FPGA
hardware specification  from source  code in  the C  language, a  process called
/High-Level Synthesis/ (HLS).  The main difference from our work with GPU compiler
parameters was the time to obtain  the hardware specification, which could be in
the order of hours for a single kernel.

In this  more complex  scenario, we  achieved up  to $2\times$  improvements for
different hardware metrics using  conventional search algorithms.  These results
were obtained in a simple HLS benchmark, for which compilation times were in the
order of  minutes.  The  search space was  composed of  approximately $10^{123}$
possible  configurations, which  is much  larger than  the search  space in  our
previous work with GPUs. Search space size and the larger measurement cost meant
that  we  did  not  expect  the  heuristics-based  approach  to  have  the  same
effectiveness   as  in   the   GPU   compiler  case   study.    This  work   was
published\nbsp\cite{bruel2017autotuninghls}  at  the  2017  /IEEE  International
Conference on ReConFigurable Computing and FPGAs/.

Approaches using  classical machine  learning and optimization  techniques would
not scale  to industrial-level  HLS, where  each compilation  can take  hours to
complete.  Search space properties also  increase the complexity of the problem,
in  particular  its  structure  composed of  binary,  factorial  and  continuous
variables with potentially complex interactions.   Our results on autotuning HLS
for  FPGAs  corroborate   the  conclusion  that  the   empirical  autotuning  of
expensive-to-evaluate functions, such as those  that appear on the autotuning of
HLS, require a more parsimonious  and transparent approach, that can potentially
be achieved using  the DoE methodology.  The next section  describes our work on
applying the DoE methodology to autotuning.

The  main contribution  of this  thesis is  a strategy  to apply  the Design  of
Experiments methodology to autotuning problems.  The strategy is based on linear
regression  and  its  extensions,  Analysis of  Variance  (ANOVA),  and  Optimal
Design. The strategy  requires the formulation of initial  assumptions about the
target autotuning problem, which are  refined with data collected by efficiently
selected experiments. The main objectives  are to identify relationships between
parameters, suggesting regions for further experimentation in a transparent way,
that is, in a  way that is supported by statistical  tests of significance.  The
effectiveness of the proposed strategy, and its ability to explain optimizations
it  finds,  are  evaluated  on   autotuning  problems  in  the  source-to-source
transformation domain. The initial stages of this work resulted in a publication
on IEEE/ACM CCGrid\nbsp{}\cite{bruel2019autotuning}.

Further efforts in this direction were  dedicated to describe precisely what can
be learned about  search spaces from the application of  the methodology, and to
refine  the  differentiation  of  the approach  for  the  sometimes  conflicting
objectives of model  assessment and prediction. These  discussions are presented
in Chapter\nbsp{}[[Application 2: GPU Laplacian Kernel]].

Because the  Design of Experiments  methodology requires the specification  of a
class of initial performance models, the methodology can sometimes achieve worse
prediction  capabilities  when  there  is considerable  uncertainty  on  initial
assumptions  about the  underlying relationships.   Chapters\nbsp{}[[Application 4:  CPU
Kernels from the SPAPT Benchmark]] and  [[Application 5: Bit Quantization for Neural
Networks]] describes the application to autotuning of Gaussian Process Regression,
an  approach that  trades some  explanatory  power for  a much  larger and  more
flexible  class of  underlying  models.   We evaluate  the  performance of  this
approach on the source-to-source transformation problem from Chapter\nbsp{}[[Application
2: GPU Laplacian Kernel]], and on larger autotuning problem on the quantization of
/Deep Neural Network/ (DNN) layers.
*** Text Structure                                               :noexport:
The remainder of this thesis is organized as follows.  Chapter\nbsp{}[[Efforts for
Reproducible Science]] describes the efforts made over the duration of the work on
this thesis  to establish a  workflow that promotes reproducible  science, using
computational  documents  and  versioning   for  code,  results,  and  analyses.
Chapter\nbsp{}[[Methods  for Function  Minimization]]  presents  background for  the
derived  from  search  heuristics, mathematical  optimization,  and  statistical
learning,  and Chapter\nbsp{}[[Application  of  Function  Minimization Methods  to
Autotuning]]  to autotuning.   Chapter\nbsp{}[[A Design  of Experiments  Methodology]]
describes the DoE  methodology, and its application to  autotuning problems, and
the method  limitations encountered during this  thesis.  Chapter\nbsp{}[[Gaussian
Process  Regression:  A more  Flexible  Method]]  describes the  Gaussian  Process
Regression    method,   and    its   application    to   autotuning    problems.
Chapter\nbsp{}[[Conclusion]]  resumes and  concludes the  high-level discussions  of
this  chapter, in  light of  the  results of  the excursions  performed in  this
thesis.
** The Need for Autotuning
High Performance Computing  has been a cornerstone of  scientific and industrial
progress for at least five decades.  By paying the cost of increased complexity,
software  and  hardware  engineering   advances  continue  to  overcome  several
challenges on the way of  the sustained performance improvements observed during
the  last fifty  years.   A  consequence of  this  mounting  complexity is  that
reaching the theoretical peak hardware  performance for a given program requires
not only expert  knowledge of specific hardware architectures,  but also mastery
of programming models and languages for parallel and distributed computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance, we  can draw  from and  adapt proven  methods from  search,
mathematical optimization, and statistical  learning. The effectiveness of these
adapted methods on performance optimization  problems varies greatly, and hinges
on practical  and mathematical properties  of the problem and  the corresponding
/search space/. The  application of such methods to the  automation of performance
tuning for specific hardware, under a set of /constraints/, is named /autotuning/.

Improving performance  also relies on gathering  application-specific knowledge,
which entails extensive  experimental costs since, with the  exception of linear
algebra  routines,  theoretical  peak  performance is  not  always  a  reachable
comparison  baseline.   When  adapting  methods  for  autotuning  we  must  face
challenges emerging from practical properties,  such as restricted time and cost
budgets,  constraints  on  feasible  parameter  values,  and  the  need  to  mix
/categorical/, /continuous/, and /discrete/ parameters.   To achieve useful results we
must also  choose methods  that make hypotheses  compatible with  problem search
spaces,  such  as  the  existence  of /discoverable/,  or  at  least  /exploitable/,
relationships between parameters and performance.  Choosing an autotuning method
requires balancing  the exploration of a  problem, that is, seeking  to discover
and  explain   relationships  between   parameters  and  performance,   and  the
exploitation of known or discovered relationships, seeking only to find the best
possible performance.

Search  algorithms  based  on  machine  learning heuristics  are  not  the  best
candidates for  autotuning domains  where measurements  are lengthy  and costly,
such as compiling industrial-level FPGA  programs, because these algorithms rely
on the  availability of a  large number of  measurements. They also  assume good
optimizations  are  reachable from  a  starting  position, and  that  tendencies
observed locally in the search space are exploitable.  These assumptions are not
usually true in  common autotuning domains, as  shown in the work  of Seymour /et
al./\nbsp\cite{seymour2008comparison}.

Autotuning search spaces also usually  have non-linear constraints and undefined
regions, which are  also expected to decrease the effectiveness  of search based
on heuristics and  machine learning.  An additional downside  to heuristics- and
machine learning-based search  is that, usually, optimization  choices cannot be
explained, and knowledge  gained during optimization is not  reusable.  The main
contribution of this  thesis is to study  how to overcome the  reliance on these
assumptions about search spaces, and the lack of explainable optimizations, from
the point  of view  of a  /Design of Experiments/  (DoE), or  /Experimental Design/,
methodology to autotuning.

One of  the first detailed  descriptions and  mathematical treatment of  DoE was
presented  by Ronald  Fisher\nbsp\cite{fisher1937design}  in his  1937 book  /The
Design of Experiments/,  where he discussed principles  of experimentation, latin
square  sampling and  factorial  designs.  Later  books such  as  the ones  from
Jain\nbsp\cite{bukh1992art}, Montgomery\nbsp\cite{montgomery2017design}  and Box
/et   al./\nbsp\cite{box2005statistics}   present   comprehensive   and   detailed
foundations.   Techniques  based on  DoE  are  /parsimonious/ because  they  allow
decreasing   the  number   of   measurements  required   to  determine   certain
relationships  between  parameters  and  metrics, and  are  /transparent/  because
parameter  selections and  configurations can  be  justified by  the results  of
statistical tests.

In DoE terminology, a  /design/ is a plan for executing  a series of measurements,
or /experiments/, whose objective is to identify relationships between /factors/ and
/responses/.  While factors and responses can refer to different concrete entities
in  other  domains,  in  computer   experiments  factors  can  be  configuration
parameters for algorithms  and compilers, for example, and responses  can be the
execution time or memory consumption of a program.

Designs  can  serve diverse  purposes,  from  identifying the  most  significant
factors  for  performance, to  fitting  analytical  performance models  for  the
response.  The  field of DoE  encompasses the mathematical formalization  of the
construction of experimental designs.  More practical works in the field present
algorithms to generate designs with different objectives and restrictions.

The contributions of  this thesis are strategies to apply  to program autotuning
the         DoE         methodology,        and         /Gaussian         Process
Regression/\nbsp{}\cite{williams2006gaussian}.   This thesis  presents background
and  a high-level  view  of  the theoretical  foundations  of  each method,  and
detailed  discussions of  the challenges  involved in  specializing the  general
definitions of search  heuristics and statistical learning  methods to different
autotuning problems,  as well as what  can be /learned/ about  specific autotuning
search spaces,  and how  that acquired  knowledge can  be leveraged  for further
optimization.

This  chapter aims  to substantiate  the claim  that autotuning  methods have  a
fundamental  role to  play on  the future  of program  performance optimization,
arguing  that the  value and  the difficulty  of the  efforts to  carefully tune
software became more apparent ever since advances in hardware stopped leading to
effortless performance improvements, at least from the programmer's perspective.
The following sections discuss the historical  context for the changes in trends
on  computer  architecture,  and  characterize  the  search  spaces  found  when
optimizing performance on different domains.

*** Historical Hardware Design Trends
The physical  constraints imposed  by technological  advances on  circuit design
were evident since  the first vacuum tube computers that  already spanned entire
floors,  such   as  the  ENIAC  in   1945\nbsp{}\cite{ceruzzi2003history}.   The
practical and  economical need to fit  more computing power into  real estate is
one  force for  innovation in  hardware design  that spans  its history,  and is
echoed in  modern supercomputers,  such as  the /Summit/  from /Oak  Ridge National
Laboratory/\nbsp{}\cite{olcf2020summit}, which spans an entire room.

#+NAME: fig:trends
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: 49 years of microprocessor data, highlighting the sustained exponential
#+CAPTION: increases and reductions on transistor counts and fabrication processes, the
#+CAPTION: stagnation of frequency scaling around 2005, and one solution found for it,
#+CAPTION: the simultaneous exponential increase on logical core count.
#+CAPTION: Data from Wikipedia\nbsp{}\cite{wiki2020transistor,wiki2020chronology}
[[file:img/49_years_processor_data.pdf]]

Figure\nbsp{}[[fig:trends]] highlights the unrelenting and so far successful pursuit
of smaller transistor fabrication processes, and the resulting capability to fit
more computing power on  a fixed chip area.  This trend  was already observed in
integrated      circuits      by      Gordon      Moore      /et      al./      in
1965\nbsp{}\cite{moore1965cramming},  who also  postulated its  continuity.  The
performance improvements produced by the design efforts to make Moore's forecast
a self-fulfilling  prophecy were  boosted until around  2005 by  the performance
gained from increases in circuit frequency.

Robert  Dennard  /et  al./ remarked  in  1974\nbsp{}\cite{dennard1974design}  that
smaller  transistors, in  part  because they  generate  shorter circuit  delays,
decrease  the energy  required to  power  a circuit  and enable  an increase  in
operation  frequency without  breaking  power usage  constraints.  This  scaling
effect,  named /Dennard's  scaling/,  is hindered  primarily  by leakage  current,
caused    by     quantum    tunneling    effects    in     small    transistors.
Figure\nbsp{}[[fig:trends]] shows  a marked  stagnation on frequency  increase after
around 2005,  as transistors crossed  the 10^{2}nm fabrication process.   It was
expected that leakage  due to tunneling would limit  frequency scaling strongly,
even     before      the     transistor     fabrication      process     reached
10nm\nbsp{}\cite{frank2001device}.

Current hardware is now past the  effects of Dennard's scaling.  The increase in
logical cores around  2015 can be interpreted as preparation  for and mitigation
of the  end of frequency  scaling, and ushered in  an age of  multicore scaling.
Still, in order to meet power consumption constraints, up to half of a multicore
processor could have to be powered down, at all times.  This phenomenon is named
/Dark   Silicon/\nbsp{}\cite{esmaeilzadeh2011dark},   and   presents   significant
challenges        to        current         hardware        designers        and
programmers\nbsp{}\cite{venkataramani2015approximate,cheng2015core,henkel2015new}.

The   /Top500/\nbsp{}\cite{top5002020list}   list    gathers   information   about
commercially  available supercomputers,  and ranks  them by  performance on  the
/LINPACK/ benchmark\nbsp{}\cite{dongarra2003linpack}.  Figure\nbsp{}[[fig:rmax-rpeak]]
shows  the peak  theoretical performance  $RPeak$, and  the maximum  performance
achieved  on the  LINPACK benchmark  $RMax$, in  $Tflops/s$, for  the top-ranked
supercomputers on TOP500.   Despite the smaller performance  gains from hardware
design  that are  to  be  expected for  post-Dennard's  scaling processors,  the
increase in computer  performance has sustained an  exponential climb, sustained
mostly by software improvements.

Although  /hardware accelerators/  such as  GPUs and  FPGAs, have  also helped  to
support exponential performance  increases, their use is not an  escape from the
fundamental  scaling  constraints  imposed   by  current  semiconductor  design.
Figure\nbsp{}[[fig:acc-cores]] shows the increase  in processor and accelerator core
count  on the  top-ranked  supercomputers  on Top500.   Half  of the  top-ranked
supercomputers in the  last decade had accelerator cores and,  of those, all had
around ten times more accelerator than processor cores.  The apparent stagnation
of  core  count in  top-ranked  supercomputers,  even considering  accelerators,
highlights the crucial impact software optimization has on performance.

#+NAME: fig:rmax-rpeak
#+CAPTION: Sustained exponential increase of theoretical \textit{RPeak} and
#+CAPTION: achieved \textit{RMax} performance for
#+CAPTION: the supercomputer ranked 1^{st} on TOP500\nbsp{}\cite{top5002020list}
#+ATTR_LATEX: :width \textwidth :placement [t]
[[file:./img/top500_rmax_rpeak.pdf]]

#+NAME: fig:acc-cores
#+CAPTION: Processor and accelerator core count
#+CAPTION: in supercomputers ranked 1^{st} on
#+CAPTION: TOP500\nbsp{}\cite{top5002020list}.
#+CAPTION: Core  count trends for supercomputers are not
#+CAPTION: necessarily bound to processor trends observed on
#+CAPTION: Figure\nbsp{}\ref{fig:trends}.
#+ATTR_LATEX: :width \textwidth :placement [h!]
[[file:./img/top500_accelerator_cores.pdf]]

Advances in hardware  design are currently not capable  of providing performance
improvements  via frequency  scaling  without dissipating  more  power than  the
processor was  designed to support,  which violates power constraints  and risks
damaging the circuit.  From the programmer's perspective, effortless performance
improvements from hardware  have not been expected for quite  some time, and the
key  to  sustaining  historical  trends  in  performance  scaling  has  lied  in
accelerators, parallel and distributed programming libraries, and fine tuning of
several stages of  the software stack, from instruction selection  to the layout
of neural networks.

The problem of optimizing software  for performance presents its own challenges.
The search  spaces that emerge from  autotuning problems grow quickly  to a size
for which  it would  take a  prohibitive amount  of time  to determine  the best
configuration by exhaustively evaluating  all possibilities. Although this means
we must  seek to decrease  the amount  of possibilities, by  restricting allowed
parameter values, or dropping parameters completely,  it is often unclear how to
decide  which parameters  should be  restricted or  dropped.  The  next sections
introduce a simple  autotuning problem, present an overview of  the magnitude of
the dimension  of autotuning  search spaces, and  briefly introduce  the methods
commonly used to explore search spaces, some of which are discussed in detail in
Chapter\nbsp{}[[Optimization Methods for Autotuning]].
*** Consequences for Code Optimization: Autotuning Loop Nest Optimization
Algorithms for linear  algebra problems are fundamental  to scientific computing
and statistics. Therefore,  decreasing the execution time of  algorithms such as
general  matrix multiplication  (GEMM)\nbsp{}\cite{dongarra1990set}, and  others
from the original BLAS\nbsp{}\cite{lawson1979basic},  is an interesting and well
motivated example, that we will use to introduce the autotuning problem.

One way to improve the performance of such linear algebra programs is to exploit
cache locality by  reordering and organizing loop iterations,  using source code
transformation methods such as loop /tiling/, or /blocking/, and /unrolling/.  We will
now  briefly describe  loop tiling  and unrolling  for a  simple linear  algebra
problem. After,  we will  discuss an  autotuning search  space for  blocking and
unrolling applied to  GEMM, and how these transformations  generate a relatively
large and complex search space, which we can explore using autotuning methods.

Figure\nbsp{}\ref{fig:transpose-c}  shows  three  versions  of  code  in  the  /C/
language that, given three square matrices $A$,  $B$, and $C$, computes $C = C +
A +  B^{\top}$. The first  optimization we can make  is to preemptively  load to
cache, or /prefetch/, as  many as possible of the elements we  know will be needed
at       any       given       iteration,       as       is       shown       in
Figure\nbsp{}\ref{fig:transpose-regular-access}. The shaded  elements on the top
row of Figure\nbsp{}[[fig:blocking-unrolling]] represent  the elements that could be
prefetched in iterations of Figure\nbsp{}\ref{fig:transpose-regular-access}.

Since /C/ matrices are stored in /row-major order/, each access of an element of $B$
forces loading the next row elements, even if we explicitly prefetch a column of
$B$.  Since  we are accessing  $B$ in a  /column-major order/, the  prefetched row
elements would not be used until we reached the corresponding column. Therefore,
the next column elements will have  to be loaded at each iteration, considerably
slowing down the computation.

# #+NAME: fig:blocking-unrolling
# #+ATTR_LATEX: :width \textwidth :placement [t]
# #+CAPTION: Access patterns on GEMM, for the \textit{destination} matrix, using
# #+CAPTION: loop  nest optimizations. Panel \textit{(a)} shows the access order
# #+CAPTION: of a naive implementation, panel \textit{(b)} shows the effect of
# #+CAPTION: loop \textit{tiling}, or \textit{blocking}, and panel \textit{(c)}
# #+CAPTION: shows the compounded effect of loop \textit{unrolling}
# [[file:./img/blocking_unrolling.pdf]]

#+begin_export latex
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      float A[N][N], B[N][N], C[N][N];
      int i, j;
      // Initialize A, B, C
      for(i = 0; i < N; i++){
        // Load line i of A to fast memory
        for(j = 0; j < N; j++){
          // Load C[i][j] to fast memory
          // Load column j of B to fast memory
          C[i][j] += A[i][j] + B[j][i];
          // Write C[i][j] to main memory
        }
      }
    \end{lstlisting}
    \caption{Regular implementation}
    \label{fig:transpose-regular-access}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, x, y;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          // Load block (i, j) of A to fast memory
          // Load block (j, i) of B to fast memory
          for(x = i; x < min(i + B_size, N); x++){
            for(y = j; y < min(j + B_size, N); y++){
              C[x][y] += A[x][y] + B[y][x];
            }
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Blocked, or tiled}
    \label{fig:transpose-blocked-tiled}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          // Load block (i, j) of A to fast memory
          // Load block (j, i) of B to fast memory
          C[i + 0][j + 0] += A[i + 0][j] * B[i][j + 0];
          C[i + 0][j + 1] += A[i + 0][j] * B[i][j + 1];
          // Unroll the remaining 12 iterations
          C[i + Bsize - 1][j + B_size - 2] += A[i + Bsize - 1][j] * B[i][j + B_size - 2];
          C[i + Bsize - 1][j + B_size - 1] += A[i + Bsize - 1][j] * B[i][j + B_size - 1];
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Tiled and unrolled}
    \label{fig:transpose-unrolling}
  \end{subfigure}
  \caption{Loop nest optimizations for $C = C + A + B^{\top}$, in C}
  \label{fig:transpose-c}
\end{figure}
#+end_export

#+NAME: fig:blocking-unrolling
#+ATTR_LATEX: :width .7\textwidth :placement [h!]
#+CAPTION: Access patterns for matrices in $C = C + A + B^{\top}$, with
#+CAPTION: loop  nest optimizations. Panel \textit{(a)} shows the access order
#+CAPTION: of a regular implementation, and panel \textit{(b)} shows the effect of
#+CAPTION: loop \textit{tiling}, or \textit{blocking}
[[file:./img/access_patterns.pdf]]

We  can  solve this  problem  by  reordering  memory  accesses to  request  only
prefetched elements. It  suffices to adequately split loop  indices into blocks,
as shown in Figure\nbsp{}\ref{fig:transpose-blocked-tiled}. Now, memory accesses
are   be   performed    in   /tiles/,   as   shown   on   the    bottom   row   of
Figure\nbsp{}[[fig:blocking-unrolling]].  If  blocks are  correctly sized to  fit in
cache, we  can improve performance  by explicitly prefetching each  tile.  After
blocking, we can  still improve performance by /unrolling/  loop iterations, which
forces register  usage and helps  the compiler to  identify regions that  can be
/vectorized/.   A  conceptual  implementation  of   loop  unrolling  is  shown  in
Figure\nbsp{}\ref{fig:transpose-unrolling}.

Looking at the  loop nest optimization problem from  the autotuning perspective,
the two  /parameters/ that  emerge from  the implementations  are the  /block size/,
which controls the  stride, and the /unrolling factor/, which  controls the number
of unrolled  iterations.  Larger block sizes  are desirable, because we  want to
avoid extra comparisons,  but blocks should be small enough  to ensure access to
as few as possible out-of-cache elements.  Likewise, the unrolling factor should
be large,  to leverage vectorization and  available registers, but not  so large
that it forces memory to the stack.

The values  of block size  and unrolling  factor that optimize  performance will
depend on the  cache hierarchy, register layout,  and vectorization capabilities
of the  target processor, but  also on the memory  access pattern of  the target
algorithm.   In  addition  to  finding   the  best  values  for  each  parameter
independently, an  autotuner must  ideally aim to  account for  the /interactions/
between parameters, that is, for the fact that the best value for each parameter
might also depend on the value chosen for the other.

The    next    loop    optimization    example    comes    from    Seymour    /et
al./\nbsp{}\cite{seymour2008comparison}, and considers 128 blocking and unrolling
values, in the interval $[0,127]$, for  the GEMM algorithm.  The three panels of
Figure\nbsp{}\ref{fig:gemm-c} show  conceptual implementations of  loop blocking
and unrolling for GEMM in /C/.  A block size of zero results in the implementation
from  Figure\nbsp{}\ref{fig:regular-access}, and  an  unrolling  factor of  zero
performs a single iteration per condition check.

#+begin_export latex
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      float A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i++){
        // Load line i of A to fast memory
        for(j = 0; j < N; j++){
          // Load C[i][j] to fast memory
          // Load column j of B to fast memory
          for(k = 0; k < N; k++){
            C[i][j] += A[i][k] * B[k][j];
          }
          // Write C[i][j] to main memory
        }
      }
    \end{lstlisting}
    \caption{Regular implementation}
    \label{fig:regular-access}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.48\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k, x, y;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          for(k = 0; k < N; k++){
            // Load block (i, k) of A to fast memory
            // Load block (k, y) of B to fast memory
            for(x = i; x < min(i + B_size, N); x++){
              for(y = j; y < min(j + B_size, N); y++){
                C[x][y] += A[x][k] * B[k][y];
              }
            }
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Blocked, or tiled}
    \label{fig:blocked-tiled}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \lstset{language=C,captionpos=b,numbers=none}
    \begin{lstlisting}
      int N = 256;
      int B_size = 4;
      int A[N][N], B[N][N], C[N][N];
      int i, j, k;
      // Initialize A, B, C
      for(i = 0; i < N; i += B_size){
        for(j = 0; j < N; j += B_size){
          // Load block (i, j) of C to fast memory
          for(k = 0; k < N; k++){
            // Load block (i, k) of A to fast memory
            // Load block (k, y) of B to fast memory
            C[i + 0][j + 0] += A[i + 0][k] * B[k][j + 0];
            C[i + 0][j + 1] += A[i + 0][k] * B[k][j + 1];
            // Unroll the remaining 12 iterations
            C[i + Bsize - 1][j + B_size - 2] += A[i + Bsize - 1][k] * B[k][j + B_size - 2];
            C[i + Bsize - 1][j + B_size - 1] += A[i + Bsize - 1][k] * B[k][j + B_size - 1];
          }
          // Write block (i, j) of C to main memory
        }
      }
    \end{lstlisting}
    \caption{Tiled and unrolled}
    \label{fig:unrolling}
  \end{subfigure}
  \caption{Loop nest optimizations for GEMM, in C}
  \label{fig:gemm-c}
\end{figure}
#+end_export

#+NAME: fig:search-space
#+ATTR_LATEX: :width .6\textwidth :placement [h!]
#+CAPTION: An exhaustively measured search space, defined by loop blocking and unrolling
#+CAPTION: parameters, for a sequential GEMM kernel.
#+CAPTION: Reproduced from Seymour \textit{et al.}\nbsp{}\cite{seymour2008comparison}
[[file:img/seymour2008comparison.pdf]]

It  is straightforward  to change  the block  size of  the implementations  from
Figure\nbsp{}\ref{fig:gemm-c},  but the  unrolling factor  is not  exposed as  a
parameter.  To test different unrolling values  we need to generate new versions
of the  source code with  different numbers of  unrolled iterations.  We  can do
that   with   code   generators    or   with   /source-to-source   transformation/
tools\nbsp{}\cite{videau2017boast,hartono2009annotation,ansel2009petabricks}.
It is  often necessary to  modify the  program we wish  to optimize in  order to
provide a configuration  interface and expose its implicit  parameters.  Once we
are able to control  the block size and the loop  unrolling factor, we determine
the target search space by choosing the values to be explored.

In this  example, the search  space is defined by  the $128^2 =  16384$ possible
combinations  of  blocking  and  unrolling  values.   The  performance  of  each
combination    in     the    search     space,    shown    in     /Mflops/s/    in
Figure\nbsp{}[[fig:search-space]],    was   measured    for   a    sequential   GEMM
implementation,        using        square        matrices        of        size
400\nbsp{}\cite{seymour2008comparison}. We can  represent this autotuning search
space as a /3D landscape/, since we  have two configurable parameters and a single
target  performance metric.   In  this setting,  the objective  is  to find  the
/highest/ point, since the objective is to /maximize/ Mflops/s, although usually the
performance metric is transformed so that the objective is its /minimization/.

On a first look, there seems to  be no apparent global search space structure in
the landscape on  Figure\nbsp{}[[fig:search-space]], but local features  jump to the
eyes, such as  the ``valley'' across all block sizes  for low unrolling factors,
the ``ramp'' across all unrolling factors for low block sizes, and the series of
jagged  ``plateaus'' across  the middle  regions, with  ridges for  identical or
divisible block sizes  and unrolling factors.  A careful look  reveals also that
there is  a curvature  along the  unrolling factor  axis.  Also  of note  is the
abundance in  this landscape of  /local minima/,  that is, points  with relatively
good performance, surrounded  by points with worse  performance. By exhaustively
evaluating  all  possibilities, the  original  study  determined that  the  best
performance  on this  program  was achieved  with  a  block size  of  80 and  an
unrolling factor of 2.

In this  conceptual example,  all ${\approx}1.64\times10^4$  configurations were
exhaustively evaluated,  but it is  impossible to do  so in most  settings where
autotuning methods are  useful.  The next section provides a  perspective of the
autotuning  domains  and methods  employed  in  current research,  presenting  a
selection of  search spaces and  discussing the trends  that can be  observed on
search space size, targeted HPC domains, and chosen optimization methods.

*** Autotuning Approaches and Search Space
Autotuning  methods have  been used  to improve  performance in  an increasingly
large  variety of  domains,  from  the earlier  applications  to linear  algebra
subprograms,  to the  now ubiquitous  construction and  configuration of  neural
networks,  to the  configuration  of  the increasingly  relevant  tools for  the
re-configurable hardware  of FPGAs.  In this  setting, it is not  far-fetched to
establish a  link between  the continued increases  in performance  and hardware
complexity, that  we discussed previously in  this chapter, to the  increases in
dimension and size of the autotuning problems that we can now tackle.

Figure\nbsp{}[[fig:search-spaces]] presents search space  dimension, measured as the
number of  parameters involved,  and size,  measured as  the number  of possible
parameter  combinations, for  a selection  of search  spaces from  14 autotuning
domains.   Precise  information about  search  space  characterization is  often
missing from works on autotuning  methods and applications. The characterization
of  most of  the search  spaces in  Figure\nbsp{}[[fig:search-spaces]] was  obtained
directly from the text of the corresponding published paper, but for some it was
necessary to extract  characterizations from the available  source code.  Still,
it was impossible  to obtain detailed descriptions of search  spaces for many of
the published works on autotuning methods  and applications, and in that way the
sample shown in this section is  biased, because it contains only information on
works that provided it.

#+NAME: fig:search-spaces
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Dimension and search space size for autotuning problems from 14 domains
#+CAPTION: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,abdelfattah2016performance,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}
#+CAPTION: The left panel shows a zoomed view of the right panel
[[file:img/search_spaces.pdf]]

The left hand  panel of Figure\nbsp{}[[fig:search-spaces]] shows  search spaces with
up to 60 parameters. The over-representation of search spaces for linear algebra
domains in this sample stands out on the  left hand panel, but the domain is not
present on the remaining  portion of the sample, shown on  the right hand panel.
The  largest  search spaces  for  which  we were  able  to  find information  on
published  work are  defined for  the domains  of neural  network configuration,
High-Level  Synthesis  for  FPGAs,   compiler  parameters,  and  domain-specific
languages.

None of the largest search spaces in  this sample, that is, the ones outside the
zoomed area  of the  left hand  panel, come  from works  earlier than  2009. The
sustained  performance improvements  we  discussed previously  have enabled  and
pushed autotuning research toward progressively  larger problems, which has also
been  done to  most  research  areas.  Increased  computing  power  has made  it
feasible,  or at  least tolerable,  to apply  search heuristics  and statistical
learning methods to find program configurations that improve performance.

It is  straightforward to  produce an extremely  large autotuning  search space.
Compilers have  hundreds of binary flags  that can be considered  for selection,
generating  a large  set of  combinations. Despite  that, regarding  performance
improvements, it is likely that most  configuration parameters will have a small
impact, that is,  that only a handful of parameters  are responsible for changes
in performance.  Search  spaces are often much more restrictive  than the one we
discussed in  Section\nbsp{}[[Consequences for Code Optimization:  Autotuning Loop
Nest Optimization]].  Autotuning problem definitions usually come with /constraints/
on parameter values  and limited /experimental budgets/, and  /runtime/ failures for
some  configurations   are  often  unpredictable.   In   this  context,  finding
configurations  that   improve  performance   and  determining  the   subset  of
/significant parameters/ are considerable challenges.

Search  heuristics, such  as methods  based on  genetic algorithms  and gradient
descent, are  a natural way to  tackle these challenges because  they consist of
procedures for exploiting existing  and unknown relationships between parameters
and  performance  without making  or  requiring  /explicit hypotheses/  about  the
problem.  Despite that,  most commonly used heuristics  make /implicit hypotheses/
about search  spaces which are not  always verified, such as  assuming that good
configurations are /reachable/ from a random starting point.

#+begin_SRC R :results output latex :session *R* :eval no-export :exports results
library(knitr)
library(kableExtra)
library(dplyr)

df <- read.csv("data/search_spaces/search_methods.csv")

df <- df %>%
    mutate(citation_name = paste(name,
                                 "~\\cite{",
                                 gscholar_citation,
                                 "}",
                                 sep = "")) %>%
    select(citation_name,
           problem_domain,
           shorthand_method,
           year) %>%
    arrange(year)

names(df) <- c("System", "Domain", "Method", "Year")

caption <- paste("Autotuning methods used by a sample of systems,",
                 "in different domains, ordered by publishing year.",
                 "Methods were classified as either Search Heuristics (SH),",
                 "Machine Learning (ML), or more precisely",
                 "when the originating work provided detailed information.",
                 "Earlier work favored employing Search Heuristics, which",
                 "are less prominent in recent work, which favors methods",
                 "based on Machine Learning.")

kable(df,
      format = "latex",
      escape = FALSE,
      booktabs = TRUE,
      linesep = "",
      label = "search-methods",
      caption = caption) %>%
    kable_styling(latex_options = c("scale_down", "hold_position"))
#+end_SRC

#+RESULTS:
#+begin_export latex

\begin{table}[!h]

\caption{\label{tab:search-methods}Autotuning methods used by a sample of systems, in different domains, ordered by publishing year. Methods were classified as either Search Heuristics (SH), Machine Learning (ML), or more precisely when the originating work provided detailed information. Earlier work favored employing Search Heuristics, which are less prominent in recent work, which favors methods based on Machine Learning.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lllr}
\toprule
System & Domain & Method & Year\\
\midrule
PhiPAC~\cite{bilmes1997optimizing} & Linear Algebra & SH (Exhaustive) & 1997\\
ATLAS~\cite{dongarra1998automatically} & Linear Algebra & SH (Exhaustive) & 1998\\
FFTW~\cite{frigo1998fftw} & Digital Signal Processing & SH (Exhaustive) & 1998\\
Active Harmony~\cite{tapus2002active} & Domain-Specific Language & SH & 2002\\
OSKI~\cite{vuduc2005oski} & Linear Algebra & SH & 2005\\
Seymour, K. \textit{et al.}~\cite{seymour2008comparison} & Linear Algebra & SH & 2008\\
PRO~\cite{tiwari2009scalable} & Linear Algebra & SH & 2009\\
ParamILS~\cite{hutter2009paramils} & Combinatorial Auctions & SH & 2009\\
PetaBricks~\cite{ansel2009petabricks} & Domain-Specific Language & SH (Genetic Algorithm) & 2009\\
MILEPOST GCC~\cite{fursin2011milepost} & Compiler Parameters & ML & 2011\\
Orio~\cite{balaprakash2012spapt} & Linear Algebra & ML (Decision Trees) & 2012\\
pOSKI~\cite{byun2012autotuning} & Linear Algebra & SH & 2012\\
INSIEME~\cite{jordan2012multi} & Compiler Parameters & SH (Genetic Algorithm) & 2012\\
OpenTuner~\cite{ansel2014opentuner} & Compiler Parameters & SH & 2014\\
Lgen~\cite{spampinato2014basic} & Linear Algebra & SH & 2014\\
OPAL~\cite{audet2014optimization} & Parallel Computing & SH & 2014\\
Mametjanov, A. \textit{et al.}~\cite{mametjanov2015autotuning} & High-Level Synthesis & ML (Decision Trees) & 2015\\
CLTune~\cite{nugteren2015cltune} & Parallel Computing & SH & 2015\\
Guerreirro, J. \textit{et al.}~\cite{guerreiro2015multi} & Parallel Computing & SH & 2015\\
Collective Mind~\cite{fursin2015collective} & Compiler Parameters & ML & 2015\\
Abdelfattah, A. \textit{et al.}~\cite{abdelfattah2016performance} & Linear Algebra & SH (Exhaustive) & 2016\\
TANGRAM~\cite{chang2016efficient} & Domain-Specific Language & SH & 2016\\
MASE-BDI~\cite{coelho2016mase} & Environmental Land Change & SH & 2016\\
Xu, C. \textit{et al.}~\cite{xu2017parallel} & High-Level Synthesis & SH & 2017\\
Apollo~\cite{beckingsale2017apollo} & Parallel Computing & ML (Decision Trees) & 2017\\
DeepHyper~\cite{balaprakash2018deephyper} & Neural Networks & ML (Decision Trees) & 2018\\
Tuzov, I. \textit{et al.}~\cite{tuzov2018tuning} & High-Level Synthesis & Design of Experiments & 2018\\
Periscope~\cite{gerndt2018multi} & Compiler Parameters & SH & 2018\\
SynTunSys~\cite{ziegler2019syntunsys} & High-Level Synthesis & SH & 2019\\
Kwon, J. \textit{et al.}~\cite{kwon2019learning} & High-Level Synthesis & ML & 2019\\
FuncyTuner~\cite{wang2019funcytuner} & Compiler Parameters & SH & 2019\\
Ol’ha, J. \textit{et al.}~\cite{olha2019exploiting} & Parallel Computing & Sensitivity Analysis & 2019\\
Petrovic, F. \textit{et al.}~\cite{petrovivc2020benchmark} & Linear Algebra & SH & 2020\\
Chu, Y. \textit{et al.}~\cite{chu2020improving} & Search/MVWCP & SH & 2020\\
\bottomrule
\end{tabular}}
\end{table}
#+end_export

Autotuning methods that make explicit  hypotheses about the target program, such
as methods  based on Design of  Experiments, require some initial  knowledge, or
willingness to make assumptions, about  underlying relationships, and are harder
to adapt to constrained scenarios, but have the potential to produce /explainable/
optimizations.   In  general, methods  based  on  Machine Learning  have  enough
flexibility to  perform well in complex  search spaces and make  few assumptions
about problems, but usually provide little, if  any, that can be used to explain
optimization choices or derive relationships between parameters and performance.

Table\nbsp{}\ref{tab:search-methods} lists some autotuning systems, their target
domains, and  the employed  method, ordered by  publication date.   Some systems
that  did not  provide  detailed  search space  descriptions  and  could not  be
included in Figure\nbsp{}[[fig:search-spaces]], especially some of the earlier work,
provided enough information to categorize their autotuning methods. In contrast,
many  more  recent  works,  especially  those using  methods  based  on  Machine
Learning, did not provide specific  method information. Earlier work often deals
with  search spaces  small enough  to  exhaustively evaluate,  and using  search
heuristics to optimize linear algebra programs is the most prominent category of
earlier work in this sample.  Later  autotuning work target more varied domains,
with  the  most prominent  domains  in  this  sample being  parallel  computing,
compiler parameters, and  High-Level Synthesis.  Systems using  methods based on
Machine Learning become more common on later work than systems using heuristics.

Chapter\nbsp{}[[]]  provides more  detailed definitions  and discussions  of the
applicability, effectiveness, and explanatory  power of autotuning methods based
on search  heuristics and statistical  learning.  The remainder of  this chapter
details the contributions of this thesis and the structure of this document.
* Optimization Methods for Autotuning
** Notation and Search Spaces
The  following chapters  discuss the  optimization  methods that  we applied  to
autotuning  problems in  different domains.   Each chapter  presents a  group of
methods,  briefly discussing  each  method  in the  group  and their  underlying
hypotheses.   The  objective  of  each  chapter is  to  provide  high-level  and
straightforward   descriptions  of   optimization   methods,  presenting   clear
definitions tied to the autotuning context.

Each chapter concludes  with a discussion of the applicability  of each group of
methods  to  autotuning  problems.   The methods  we  discuss  have  significant
differences  but  employ  the  same  basic  concepts.   We  will  use  the  same
mathematical notation  to discuss all  methods when possible.  The  remainder of
this chapter  presents common basic concepts  and the associated notation  to be
used     in     subsequent     chapters,     which     are     summarized     in
Table\nbsp{}[[tab:notation]]. Completing  this introduction  to optimization  in the
context of autotuning, the chapter ends with a discussion of common search space
properties.

*** Notation
We will  call /optimization/  the minimization  of a  real-valued function  with a
single vector  input.  For  a function $f:  \mathcal{X} \mapsto  \mathbb{R}$, we
wish to  find the input  vector $\mathbf{x}^{\ast} =  [x_1,\dots,x_p]^{\top}$ in
the    /parameter   space/,    or   /search    space/   $\mathcal{X}$    for   which
$f(\mathbf{x}^{\ast})$ is  the smallest, compared  to all other  $\mathbf{x} \in
\mathcal{X}$.   The function  $f$  represents, in  the  autotuning context,  the
/performance  metric/ we  wish to  optimize, such  as the  execution time  of some
application,  and  the  parameter  space $\mathcal{X}$  represents  the  set  of
possible /configurations/ we  can explore, such as compiler  flags.  Therefore, we
define optimization in the autotuning  context as finding the configuration that
minimizes the target  performance metric. For the sake of  simplicity, we assume
we can use the opposite of the performance metrics that should be maximized.

As an example of an autotuning  problem, consider optimizing the choice of flags
for a compiler with $p  = \vert{}\mathbf{x}\vert{}$ flags, where a configuration
$\mathbf{x}  =  [x_1,\dots,x_p]^{\top}$ consists  of  a  vector of  $p$  /boolean/
values,  denoting  whether  each  flag  $x_1,\dots,x_p$ is  turned  on  for  the
compilation of a specific application.   To find the compiler configuration that
generates the binary with the smallest execution  time, we conduct a set of $n =
\vert{}\mathbf{X}\vert{}$  /experiments/,  chosen  according  to  some  criterion,
generating     the     /experimental      design     matrix/     $\mathbf{X}     =
[\mathbf{x}_1,\dots,\mathbf{x}_n]$.  Each  experiment consists of  compiling the
target application using the specified compiler configuration, and measuring the
execution time of the resulting binary.

In  this example,  evaluating $f(\mathbf{x})$  involves generating  the compiler
configuration corresponding to the vector  $\mathbf{x}$ of selected flags.  This
involves writing a shell command or a configuration file, running the configured
compiler, checking for  compilation errors, measuring the execution  time of the
resulting binary, and verifying the correctness of its output.

In   practice,  we   may  never   be  able   to  observe   the  /true   value/  of
$f(\mathbf{x})$. In fact,  empirical tests of this nature are  always subject to
unknown or uncontrollable  effects, and to inherent  imprecision in measurement.
In practice,  we settle for observing  $y = f(\mathbf{x}) +  \varepsilon$, where
$\varepsilon$ encapsulates  all unknown and  uncontrollable effects, as  well as
the measurement error.  Returning to the  compiler flag example, suppose that we
could conduct $n = \vert{}\mathcal{X}\vert{} = 2^{p}$ experiments, measuring the
performance of the binaries generated with all possible flag combinations.  With
such experimental design we would obtain the measurements
#+begin_export latex
\begin{align}
  \mathbf{y} =  [y_i = f(\mathbf{x}_i)  +
    \varepsilon_i, \; i =  1,\dots,2^{p}]^{\top}\text{.}
\end{align}
#+end_export
The  measurement  $y_i$   is  an  /estimate/  of   $f(\mathbf{x}_i)$,  with  error
$\varepsilon_i$.  If  the error is reasonably  small, an estimate of  the /global
optimum/ $\mathbf{x}^{\ast}$ in this example  is the $\mathbf{x}_i$ that produces
the binary with  the smallest estimated execution time  $y^{\ast}$, the smallest
$y_i \in \mathbf{y}$.

Assuming we are capable of cheaply evaluating $f$ for a large set of experiments
$\mathbf{X}$, and that we are not  interested in building statistical models for
the  performance  of  our  application,  we  can  directly  optimize  $f$  using
stochastic descent  methods, or  gradient- and hessian-based  methods if  $f$ is
suitably  convex and  differentiable.  These  function minimization  methods are
discussed in Chapter\nbsp{}[[Methods for Function Minimization]].

If we are not capable of directly measuring $f$, if it is unreasonably expensive
or time-consuming  to do so,  or if constructing statistical  performance models
for our application is of crucial  importance, we can employ the surrogate-based
methods   discussed  in   Chapters\nbsp{}[[Statistical  Learning]],   [[Design  of
Experiments]], and  [[Online Learning]].   These methods  use different  strategies to
construct a /surrogate model/
#+begin_export latex
\begin{align}
  \hat{f}_{\theta}: \mathcal{X} \mapsto \mathbb{R}\text{, with }
  \theta \in \Theta\text{,}
\end{align}
#+end_export
where  $\theta$  is  a  parameter vector  usually  estimated  from  measurements
$(\mathbf{X},\mathbf{y})$.   The  function   $\theta  :  \mathcal{X}^{n}  \times
\mathbb{R}^{n} \mapsto \Theta$ represents the  /estimation process/, that uses the
/observations/  $\mathbf{y}  \in  \mathbb{R}^{n}$   from  an  experimental  design
$\mathbf{X}   =   [\mathbf{x}_1,\dots,\mathbf{x}_n]$.   The   /parameter   vector/
$\theta(\mathbf{X},\mathbf{y})$ in the /parameter space/  $\Theta$ will be used to
compute the estimate.

The constructed surrogate model $\hat{f}_{\theta(\mathbf{X},\mathbf{y})}$ can be
used as a tool to attempt to  describe and optimize the underlying real function
$f$,   provided   we   are   able    to   construct   a   useful   estimate   of
$\theta(\mathbf{X},\mathbf{y})$.  We will  discuss in Chapters\nbsp{}[[Statistical
Learning]] and\nbsp{}[[Design  of Experiments]]  methods that  use the  $m$ individual
parameter        estimates        $\theta(\mathbf{X},       \mathbf{y})        =
[\theta_1,\dots,\theta_m]^{\top}$  to   assess  the  significance   of  specific
/factors/.

If  we use  the  Ordinary Least  Squares  (OLS) estimator  for  $\theta$ in  our
compiler flag  example, the parameter estimates  $\theta_2,\dots,\theta_m$ could
correspond  to each  one of  the $p$  flags.  In  this case,  $\theta_1$ is  the
estimate of  the intercept of the  linear model, and  $m = p +  1$. Optimization
methods  using  the  Bayesian  inference framework,  such  as  Gaussian  Process
Regression, which we discuss  in Chapter\nbsp{}[[Statistical Learning]], associate a
probability  distribution to  $\theta(\mathbf{X},\mathbf{y})$, which  propagates
with $\hat{f}_{\theta}$ and can be exploited in our optimization context.

Chapter\nbsp{}[[Statistical  Learning]]  differentiates between  /parametric/  methods
which  make  the  hypothesis  that  the number  $m  =  \vert{}\Theta\vert{}$  of
estimated  parameters  is finite  and  often  interpretable, and  /non-parametric/
methods, which operate in parameter spaces of infinite dimension.

Table\nbsp{}[[tab:notation]] summarizes the notation  and concepts we have discussed
so  far, tying  those concepts  to  the compiler  flag  example we  used in  the
discussion.   The notation  and the  basic concepts  we have  described in  this
section, although referring to abstract entities, enable a uniform discussion of
different optimization methods in the next chapters.

#+NAME: tab:notation
#+CAPTION: Summary of the notation, concepts, and examples discussed in this chapter,
#+CAPTION: and common to the autotuning methods discussed in further chapters
#+ATTR_LATEX: :align p{.24\columnwidth}p{.195\columnwidth}p{.475\columnwidth} :booktabs t :font \scriptsize
|-------------------------------------------------------------------------------------------------------------+---------------------------------+----------------------------------------------------------------------|
| Symbol                                                                                                      | Concept                         | Example                                                              |
|-------------------------------------------------------------------------------------------------------------+---------------------------------+----------------------------------------------------------------------|
| $\mathcal{X}$                                                                                               | Search space                    | All possible compiler flag assignments                               |
| $\mathbf{x} = [x_1, \dots, x_p]^{\top} \in \mathcal{X}$                                                     | Input variable                  | A specific flag assignment                                           |
| $\mathbf{X} = [\mathbf{x}_1, \dots, \mathbf{x}_n] \subseteq \mathcal{X}$                                    | Experimental design             | A set of flag assignments                                            |
| $p = \vert{}\mathbf{x}\vert{}$                                                                              | Search space dimension          | The number of flags to assign                                        |
| $n = \vert{}\mathbf{X}\vert{}$                                                                              | Number of experiments           | Size of the set of flags to compile and measure                      |
| $f: \mathcal{X} \mapsto \mathbb{R}$                                                                         | Function to minimize            | Performance metric, such as the execution time of a binary           |
| $y_i = f(\mathbf{x}_i) + \varepsilon_i$                                                                     | Observable quantity             | Execution time with flags $\mathbf{x}_i$, with error $\varepsilon_i$ |
| $\mathbf{y} = [y_1, \dots, y_n]^{\top} \in \mathbb{R}^{n}$                                                  | Observations                    | List of execution times for all flags                                |
| $\mathbf{x}^{\ast}: y^{\ast} = f(\mathbf{x}^{\ast}) \leq f(\mathbf{x}), \forall \mathbf{x} \in \mathcal{X}$ | Global optimum                  | Flag assignment with smallest execution time                         |
| $\Theta$                                                                                                    | Parameter space                 | All possible values of the coefficients of a linear model            |
| $m = \vert{}\Theta\vert{}$                                                                                  | Number of parameters            | Number of OLS coefficient estimates                                  |
| $\theta(\mathbf{X}, \mathbf{y}) \in  \Theta$                                                                | Parameter vector                | OLS estimate of the coefficients of a linear model                   |
| $\hat{f}_{\theta}: \mathcal{X} \mapsto \mathbb{R}$                                                          | Surrogate for $f$               | A linear model fit used for /predictions/                              |
| $\hat{y} = \hat{f}(\mathbf{x}, \theta(\mathbf{X, \mathbf{y}}))$                                             | Estimate of $f$ at $\mathbf{x}$ | A prediction of execution time for a specific flag assignment        |
|-------------------------------------------------------------------------------------------------------------+---------------------------------+----------------------------------------------------------------------|

Before moving on to the descriptions of derivative-based and stochastic function
minimization methods and their application to autotuning problems, we discuss in
the next section some  of the properties of search spaces  that are relevant for
both autotuning and mathematical optimization.

*** Search Spaces
Consider a more  abstract optimization problem than the  compiler flag selection
from  the  last  section,  consisting  of finding  the  global  optimum  of  the
paraboloid surface defined by the Booth function,
#+begin_export latex
\begin{align}
y_a = f(\mathbf{x} = [x_1, x_2]^{\top}) = (x + 2y - 7)^{2} + (2x + y - 5)^{2}, \; x_1,x_2 \in [-10, 10]\text{.} \label{eq:f0}
\end{align}
#+end_export
In our notation, the search space for  this example is $\mathcal{X} = (x_1, x_2)
\in \mathbb{R}^{2},  \; x_1,x_2 \in  [-10,10]$, with global optimum  $y^{\ast} =
f(\mathbf{x}^{\ast} = [1, 3]) = 0$.

#+NAME: fig:simple-search-spaces
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Contour plots and slices through the global optimum, marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$, for search spaces
#+CAPTION: defined by variations of the Booth function.
#+CAPTION: Panels (a) and (d) correspond to Equation\nbsp{}\ref{eq:f0}, panels
#+CAPTION: (b) and (e) to Equation\nbsp{}\ref{eq:f1}, and panels (c) and (f)
#+CAPTION: to Equation\nbsp{}\ref{eq:f2}.
[[file:img/simple_search_space.pdf]]

Besides the  search space defined  by the  observations $y_a$, we  will consider
search spaces for two variations
#+begin_export latex
\begin{align}
y_b &= f(\mathbf{x}) + \varepsilon, \; \text{with } \varepsilon \thicksim \mathcal{N}(0, \sigma^2)\text{, and} \label{eq:f1} \\
y_c &= f(\mathbf{x}) + \varepsilon, \; \text{with } \; x_1 \in [3, -6]\text{.} \label{eq:f2}
\end{align}
#+end_export
The underlying objective function in  this example has a closed-form expression,
but in the context of our applications we consider that we can never observe the
true $f(\mathbf{x})$, even in ideal experimental conditions.  In that sense, the
observations $y_b$  closer to  a real application,  and incorporate  the unknown
effects and  measurement errors  to which the  underlying objective  function is
subject, represented by the  normally distributed random variable $\varepsilon$,
with  mean  $0$  and  variance   $\sigma^{2}$.   There  are  often  algorithmic,
theoretical, or practical /constraints/ on  the allowed combinations of parameters
of a given  objective function.  The observations $y_c$  represent this scenario
by incorporating constraints on the parameter $x_1$.

Panels (a),  (b), and (c) of  Figure\nbsp{}[[fig:simple-search-spaces]] show contour
plots   for  our   search  space   variations  in   Equations\nbsp{}\ref{eq:f0},
\ref{eq:f1}, and \ref{eq:f2} respectively. The  global optimum in each variation
is  represented by  a  red cross,  and its  location  changes between  scenarios
because of  $\varepsilon$. Panels (d), (e),  and (f) show slices  of panels (a),
(b),   and   (c),  respectively,   that   pass   through  the   global   optimum
$\mathbf{x}^{\ast}   =   (x_{1}^{\ast},   x_{2}^{\ast})$  for   fixed   $x_1   =
x_{1}^{\ast}$.

#+NAME: fig:optima-convexity
#+ATTR_LATEX: :width .66\textwidth :placement [t]
#+CAPTION: Illustrating the relationship between convexity
#+CAPTION: of a function over a compact set and the presence
#+CAPTION: of local minima. Panels (a) and (b) match the functions
#+CAPTION: on the same panels of Figure\nbsp{}\ref{fig:simple-search-spaces}
[[file:img/optima_convexity_annotated.pdf]]

The noise-free example  shown in panel (a)  has no local optima,  and its smooth
surface can  be quickly navigated  by the derivative-based methods  discussed in
Chapter\nbsp{}[[Methods for Function Minimization]]. Such  methods aim to follow the
direction of /greatest descent/ in the neighborhood of a given point. In a contour
plot, this direction  is always orthogonal to the contour  lines.  The panel (a)
from   Figure\nbsp{}[[fig:optima-convexity]]  illustrates   the  /convexity/   of  our
noise-free function. Informally, a line segment connecting any two points in the
graph of a convex  function will not cross the graph  of the function. Convexity
of  a function  over a  compact set  implies the  existence of  a single  global
optimum, whereas  lack of convexity  implies the  existence of /local  optima/, as
happens  in   our  noisy  functions,  and   is  highlighted  in  panel   (b)  of
Figure\nbsp{}[[fig:optima-convexity]].

Local optima  are by  definition surrounded  by higher  values of  the objective
function,  and can  thus trap  optimization methods  that do  not plan  for such
situations,   such   as   a   naive   implementation   of   a   derivative-based
method. Adapting the step  size of a derivative-based method is  one way to deal
with functions with many local optima, such as the ones on panels (b) and (c) of
Figure\nbsp{}[[fig:simple-search-spaces]].

Objective functions  including constraints can  present much harder  problems to
optimization methods, if  certain conditions are met. For example,  on panel (c)
of Figure\nbsp{}[[fig:simple-search-spaces]],  we have constraints that  cut contour
lines in such a way that prevents  attempts to move inside the feasible space in
the  direction of  greatest descent.   A derivative-based  method would  have to
drastically  decrease its  step size  upon reaching  the constraint  border, and
coast along the border in small steps  until it finds a more appealing direction
of descent.

This  more abstract  and  simple example  aimed  to illustrate  that  it can  be
non-trivial to find the  global optimum of the simplest of  search spaces, if we
consider  the   significant  challenges   introduced  by  unknown   effects  and
measurement error.   The additional  challenges introduced by  the time  cost to
obtain measurements, which are discussed in Chapter\nbsp{}[[Design of Experiments]],
guided the  selection of the optimization  methods studied in this  thesis.  The
next  chapter discusses  derivative-based  and stochastic  methods for  function
minimization.

** Methods for Function Minimization
This  chapter  aims  to  present  the  intuition  guiding  the  construction  of
derivative-based and  stochastic methods for function  minimization.  We discuss
the key hypotheses of these groups of methods, and for which autotuning problems
they can be most effective.

We will put  aside for the moment  the idea of using observations  to estimate a
parameter  vector  and  construct   a  surrogate  function  $\hat{f}(\mathbf{x},
\theta(\mathbf{X},\mathbf{y}))$. This  idea will return in  later chapters.  The
methods discussed  in this chapter  do not  construct a surrogate  function, and
thus attempt  to directly  optimize the  objective function  $f(\mathbf{x})$. In
this sense,  because they need to  know how to evaluate  it during optimization,
these methods  make the hypothesis  that the  objective function is  known.  The
effectiveness of methods based on  derivatives requires additional properties of
$f(\mathbf{x})$ to  be known or  estimable, such as  its first and  second order
derivatives, which imposes additional constraints on objective functions.

Evaluating derivatives  to determine the next  best step or using  heuristics to
explore a search space cannot be  done parsimoniously, because a large number of
function evaluations is  required to estimate derivatives  when closed-forms are
unknown, and to explore a search  space in the expectation of leveraging unknown
structure.  We  will discuss  Design of  Experiments in  Chapter\nbsp{}[[Design of
Experiments]], and present  optimization strategies for situations  where the cost
of evaluating the objective function is prohibitive.

Methods based on  derivatives can be powerful, provided  their strong hypothesis
are  respected. We  now  briefly  define and  discuss  these  methods and  their
application to autotuning.

*** Methods Based on Derivatives
The derivatives  of a function $f$  at a point $\mathbf{x}$  provide information
about  the   values  of  $f$   in  a   neighborhood  of  $\mathbf{x}$.    It  is
straightforward  to   construct  optimization   methods  that  use   this  local
information, although iteratively leveraging it requires closed-form expressions
for  the derivatives  of $f$,  or estimates  obtained by  evaluating $f$  at the
neighborhoods of each point.

This  section  discusses  gradient  descent and  Newton's  method,  optimization
methods using first and second derivatives of $f$ respectively, which for convex
functions quickly  converge to the  global optimum.   We will use  examples with
Booth's function to discuss how noise and uncertainty impact these methods.

**** Gradient Descent
#+NAME: fig:booth-gradient
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Contour plots and direction of greatest descent $-\nabla{}f(\mathbf{x})$,
#+CAPTION: for search spaces  defined by variations of the Booth function.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: The global optimum is marked with a $\color{red}\boldsymbol{\times}$.
#+CAPTION: To aid visualization, vector magnitude was encoded by color intensity,
#+CAPTION: so that darker vectors have larger magnitude. The gradient along the
#+CAPTION: function's basin is near zero.
[[file:img/booth_gradient.pdf]]

The  gradient  $\nabla{}f(\mathbf{x})$ of  a  function  $f: \mathcal{X}  \mapsto
\mathbb{R}$ at point  $\mathbf{x} = [x_1, \dots, x_n]^{\top}$ is  defined as
#+begin_export latex
\begin{align}
  \nabla{}f(\mathbf{x} = [x_1,\dots,x_n]^{\top}) =
        \left[f^{\prime}_{x_1}(\mathbf{x}),
          \dots,
          f^{\prime}_{x_n}(\mathbf{x})\right]^{\top}\text{,}
\end{align}
#+end_export
where  $f^{\prime}_{x_i}(\mathbf{x})$  is  the  partial  derivative  of  $f$  at
$\mathbf{x}$,    with     respect    to    variable    $x_i$.      The    vector
$\nabla{}f(\mathbf{x}_i)$ points to  the direction of /greatest  ascent/ in which,
from the perspective of $f(\mathbf{x}_i)$, the value of $f$ increases the most.

The gradient descent  method is one of the simplest  ways to leverage derivative
information  for  optimization.   It  consists  in  moving  iteratively  in  the
direction  of /greatest  descent/, opposite  the gradient,  from a  starting point
$\mathbf{x}_1$.  If we follow the opposite of the gradient at $\mathbf{x}_1$ for
additional points $\mathbf{x}_2,\dots,\mathbf{x}_n$, each iteration is written
#+begin_export latex
\begin{align}
  \mathbf{x}_{k} = \mathbf{x}_{k - 1} - \alpha_{k}\nabla{}f(\mathbf{x}_{k - 1}), \; k=2,\dots,n\text{,}
\end{align}
#+end_export
where $\alpha_k$  is the  step size  at iteration  $k$. The  step size  for each
iteration can  be a parameter  fixed at the  beginning of optimization,  but the
best $\alpha_k$ can alternatively be determined by searching along the direction
of greatest descent.

Figure\nbsp{}[[fig:booth-gradient]]  shows the  opposites  of the  gradients of  the
three variations of Booth's  function, described by Equations\nbsp{}\ref{eq:f0},
\ref{eq:f1},   and  \ref{eq:f2},   in  panels   (a),  (b),   (c),  respectively.
Figure\nbsp{}[[fig:booth-gradient-descent-path]]  shows, in  equally marked  panels,
the paths  taken by a gradient  descent algorithm where $\alpha_k$  is chosen at
each step, according to the values of  $f$ in the neighborhood $x_{k - 1}$.

The gradient descent method iterates along  the direction of greatest descent at
each point  and easily  reaches the optimum  on the search  space of  panel (a),
unless we make an unlucky choice of $\alpha_k$.  The descent paths on panels
(b)  and  (c)  are  not  so straightforward  since  the  several  local  minima,
represented by the crests and loops on  the contour lines, trap the descent path
if $\alpha_k$ is not carefully chosen.

The  situation   is  thornier  in  panel   (c),  where  an  unlucky   choice  of
$\mathbf{x}_1$ or $\alpha_k$ throws the  descent path against the top constraint
border,  forcing the  method to  zigzag along  the border  in short  steps. This
happens  in  this  particular  situation  in  panel  (c)  because  all  gradient
information  guides the  descent across  the constraint  border, but  the method
cannot cross it. Gradient  descent has a harder time on panels  (b) and (c) even
upon reaching  the basin  where the optimum  lies, because  gradient information
there is  also conflicting  due to noise  $\varepsilon$.  Gradient  descent gets
stuck in our example paths, but restarting strategies picking new $\mathbf{x}_1$
or $\alpha_k$ could help escaping the local minima along the basin.

#+NAME: fig:booth-gradient-descent-path
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Representation of paths taken by the gradient descent method,
#+CAPTION: with adaptive choice of $\alpha_k$,
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Contour plots and direction of greatest descent $-\nabla{}f(\mathbf{x})$
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_gradient_descent_path.pdf]]

**** Newton's Method
With Newton's method  we can improve upon the intuition  of descending along the
opposite  of  the gradient  of  $f$  by  using  the second  partial  derivatives
$f_{\mathbf{x}_{k -  1}}^{\prime\prime}$ to approximate $f$  in the neighborhood
of $\mathbf{x}_{k - 1}$ using its second Taylor polynomial
#+begin_export latex
\begin{align}
  f(\mathbf{x}) \approx f(\mathbf{x}_{k - 1}) \; + \;
    \nabla{}f(\mathbf{x}_{k - 1})^{\top}(\mathbf{x} - \mathbf{x}_{k - 1}) \; + \;
    \frac{1}{2}(\mathbf{x} - \mathbf{x}_{k - 1})^{\top}\mathbf{H}f(\mathbf{x}_{k - 1})
    (\mathbf{x} - \mathbf{x}_{k - 1})\text{,}
\end{align}
#+end_export
where  $\mathbf{x}$  is  in  the  neighborhood  of  $\mathbf{x}_{k  -  1}$,  and
$\mathbf{H}f(\mathbf{x}_{k - 1})$ denotes the Hessian of $f$, a square matrix of
second derivatives of $f$, with elements
#+begin_export latex
\begin{align}
  \left(\mathbf{H}f(\mathbf{x}_{k - 1})\right)_{i,j} =
  f^{\prime\prime}_{x_i,x_j}(\mathbf{x}_{k - 1})\text{.}
\end{align}
#+end_export
We are  not going  to consider  the approximation  of $f$  by the  second Taylor
polynomial to be an estimation process in the statistical sense, because it does
not involve dealing with measurement or modeling error.

The second Taylor  polynomial uses information about the  partial derivatives of
$f$  at $\mathbf{x}_{k  - 1}$  to  produce an  approximation of  $f$ for  points
$\mathbf{x}$ around  $\mathbf{x}_{k - 1}$.  If  we compute the gradient  of this
approximation  polynomial  and  set  it  to  zero,  we  obtain  the  next  point
$\mathbf{x}_k$, as well  as the iterative step of Newton's  method.  Starting at
$\mathbf{x}_1$, for points $\mathbf{x}_2, \dots, \mathbf{x}_n$, we have
#+begin_export latex
\begin{align}
\mathbf{x}_k = \mathbf{x}_{k - 1} -
\mathbf{H}f(\mathbf{x}_{k - 1})\nabla{}f(\mathbf{x}_{k - 1})\text{.} \label{eq:newton}
\end{align}
#+end_export

Provided the strong hypotheses of convexity and differentiability are respected,
derivative-based  methods  are  extremely effective.   In  particular,  Newton's
method converges to the global optimum in  a single step if $f$ is quadratic and
$\mathbf{H}f$ is positive definite.  This  happens in the following example with
Booth's function
#+begin_export latex
\begin{align}
y = f(\mathbf{x} = [x_1, x_2]^{\top}) = (x + 2y - 7)^{2} + (2x + y - 5)^{2}, \; x_1,x_2 \in [-10, 10]\text{,}
\end{align}
#+end_export
adapted          from          the         book          Algorithms          for
Optimization\nbsp{}\cite{kochenderfer2019algorithms}.

If we start with $\mathbf{x}_1 = [9, 8]^{\top}$ and plug
#+begin_export latex
\begin{align}
  \nabla{}f(\mathbf{x}_1 = [9, 8]^{\top}) = [10 \cdot 9 + 8 \cdot 8 - 34,
    8 \cdot 9 + 10 \cdot 8 - 38]^{\top} = [120, 144]^{\top}\text{,}
\end{align}
#+end_export
and
#+begin_export latex
\begin{align}
  \mathbf{H}f(\mathbf{x}_1 = [9, 8]^{\top}) =
  \begin{bmatrix}
    10 & 8 \\
    8 & 10
  \end{bmatrix}
\end{align}
#+end_export
into the Newton's method update step in Equation\nbsp{}\ref{eq:newton},
we reach $\mathbf{x}^{\ast} = [1, 3]^{\top}$ in the next step
#+begin_export latex
\begin{align}
  \mathbf{x}_2 = \begin{bmatrix}
    9 \\
    8
  \end{bmatrix} -
  \begin{bmatrix}
    10 & 8 \\
    8 & 10
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    120 \\
    144
  \end{bmatrix} =
  \begin{bmatrix}
    1 \\
    3
  \end{bmatrix} = \mathbf{x}^{\ast}\text{.}
\end{align}
#+end_export

Function minimization methods work very  well if the objective function respects
their  strong hypotheses  of  convexity  and differentiability.   Unfortunately,
autotuning problems seldom fulfill the  conditions necessary for the application
of  such  methods.

#+NAME: fig:search-space-2
#+ATTR_LATEX: :width .6\textwidth :placement [h!]
#+CAPTION: An exhaustively measured search space, defined by loop blocking and unrolling
#+CAPTION: parameters, for a sequential GEMM kernel.
#+CAPTION: Reproduced from Seymour \textit{et al.}\nbsp{}\cite{seymour2008comparison}
[[file:img/seymour2008comparison.pdf]]

Typical  autotuning   search  spaces,  such   as  the  one  we   encountered  in
Chapter\nbsp{}[[The    Need   for    Autotuning]],    which    is   reproduced    in
Figure\nbsp{}[[fig:search-space-2]],  clearly  present  considerable  challenges  to
derivative-based methods,  due to  the abundance of  local minima,  valleys, and
ridges.   Since  we  have  closed-form   expression  for  the  search  space  in
Figure\nbsp{}[[fig:search-space-2]], we would have  to perform a considerable number
of  evaluations  of $f$  in  order  to estimate  its  derivative  at each  step.
Although gradient  descent and other  derivative-based methods can  be effective
and  are  historically  important,  measuring  $f$  is  usually  not  cheap  for
autotuning search spaces.  Therefore, minimizing  experimentation cost is also a
strong concern.

Before    discussing    methods    to     construct    surrogate    models    in
Chapter\nbsp{}[[Statistical Learning]], and  how to use such  surrogates to minimize
experimental  cost on  Chapter\nbsp{}[[Design of  Experiments]], we  will relax  the
convexity and differentiability requirements  on objective functions and discuss
stochastic  methods  for  function  minimization,  and  their  applicability  to
autotuning.

*** Stochastic Methods
In this  section we  discuss some stochastic  methods for  function minimization
that  drop the  convexity and  differentiability requirements  of gradient-based
methods, becoming applicable to a wider range of autotuning problems at the cost
of providing no  convergence guarantees. In fact, these methods  have no clearly
stated hypotheses and  are based on search space  exploration heuristics.  Often
the best  possible understanding  of how  these heuristics  work comes  from the
intuition  and motivation  behind their  definition,  and from  the analysis  of
empirical tests.

There  are multiple  ways to  categorize heuristics.  Our choice  was to  make a
distinction  between single-state  and  population-based  methods.  In  summary,
single-state methods have update rules  mapping a single point $\mathbf{x}_k$ to
a point  $\mathbf{x}_{k + 1}$,  while rules  for population-based methods  map a
population  $\mathbf{P}_k   =  [\mathbf{x}_1,\dots,\mathbf{x}_n]^{\top}$   to  a
population $\mathbf{P}_{k + 1}$, which  may retain, combine, and modify elements
from $\mathbf{P}$.

We will first  discuss single-state methods, building up  to Simulated Annealing
from  Random  Walk,  then  we  discuss Genetic  Algorithms  and  Particle  Swarm
Optimization, representing widely used population-based methods.

**** Single-State Methods: Random Walk and Simulated Annealing
Random Sampling  is arguably the  simplest exploration heuristic,  consisting of
picking uncorrelated samples  from a uniform distribution over  the search space
$\mathcal{X}$.  There is no guarantee of finding local or global minima, but the
chances of  improving over a starting  point increase if the  objective function
has  many  local  minima.   Despite  its  simplicity,  Random  Sampling  can  be
surprisingly effective.

A simple way to derive a single-state  heuristic from Random Sampling is to take
correlated  samples,  so  that  each  sample  lies  in  a  neighborhood  of  its
predecessor, which is called Random Walk.  The neighborhood $N(\mathbf{x})$ of a
point is  the set of  points within distance $d$  from $\mathbf{x}$. One  way to
define it is
#+begin_export latex
\begin{align}
  N(\mathbf{x}) = \{\mathbf{x}_i \in \mathcal{X}: \mathbf{x}_i \neq \mathbf{x},
    \; \lVert{}\mathbf{x}_i - \mathbf{x}\rVert{}^{2} \leq d\}\text{.}
\end{align}
#+end_export
As  long  as  it is  possible  to  compute  distances  between the  elements  of
$\mathcal{X}$, we  will be  able to  construct the neighborhood  of a  point and
employ the stochastic methods we discuss in this section.

A random walk of length $n$ starting at $\mathbf{x}_1$ produces a sequence where
each point $\mathbf{x}_{k  > 1}$ is a random variable  with uniform distribution
over $N(\mathbf{x}_{k  - 1})$. There  is no guarantee that  we will ever  find a
better point  with respect  to the  objective function than  the one  we started
with.  A straightforward extension  of Random Walk is to pick at  each step $k >
1$ the first point $\mathbf{x}_k$ we  come across in $N(\mathbf{x}_{k - 1})$ for
which $f(\mathbf{x}_k)  < f(\mathbf{x}_{k  - 1})$.   This greedy  strategy would
require   measuring  the   value  of   $f$   for  possibly   many  elements   of
$N(\mathbf{x}_{k - 1})$  but it ensures that  we will only move  toward a better
point.

If are willing to  pay the cost to measure all the  points in $N(\mathbf{x}_{k -
1})$ we can choose the $\mathbf{x}^{\ast}_{k}$ that brings the best improvement,
for  which $f(\mathbf{x}^{\ast}_{k})  < f(\mathbf{x})$  for all  $\mathbf{x} \in
N(\mathbf{x}_{k - 1})$. This best improvement  strategy always moves to the best
point  in a  neighborhood, but  it can  still get  stuck in  lock minima  if the
current point is already the best one in its neighborhood. Adapting the distance
that  defines  a  neighborhood  can  help escape  local  minima,  but  the  best
improvement  strategy still  requires  measuring the  entire  neighborhood of  a
point.

Simulated Annealing  is a  probabilistic improvement  heuristic inspired  by the
process of annealing, where temperature is carefully controlled to first agitate
a material's crystalline  structure with higher temperature, and  then settle it
into  more  desirable  configurations.   Adapting  this  idea  to  optimization,
Simulated Annealing makes a compromise  between a greedy approach to exploration
and a  random walk.  At  each step $k$, we  pick a random  uniformly distributed
$\mathbf{x}_{k} \in N(\mathbf{x}_{k - 1})$ and  move to it if $f(\mathbf{x}_k) <
f(\mathbf{x}_{k  - 1})$.   In contrast  to a  greedy approach,  we also  move if
$f(\mathbf{x}_k) \geq f(\mathbf{x}_{k - 1})$ with probability $p$.

In analogy to the real annealing  process, $p$ starts high to enable exploration
of  the  search  space  and  then  decreases  to  force  a  descent  towards  an
optimum. All through  the process, a nonzero value of  $p$ permits the heuristic
to leave a local  minimum.  The probability $p_k$ of moving to  a worse point at
iteration $k$ is
#+begin_export latex
\begin{align}
  p_k = exp\left(-{\dfrac{f(\mathbf{x}_{k}) -
      f(\mathbf{x}_{k - 1})}{t_k}}\right)\text{,}
\end{align}
#+end_export
where  $t_k$ is  the  temperature at  iteration $k$,  and  can follow  different
decaying  rates. For  a  starting temperature  $t_1$  the logarithmic  annealing
schedule is
#+begin_export latex
\begin{align}
  t_k = \dfrac{t_1}{log(k + 1)}\text{.}
\end{align}
#+end_export

#+NAME: fig:booth-sima-path
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Representation of paths taken by the Simulated Annealing method
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Contour plots and direction of greatest descent
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_simulated_annealing_descent_path.pdf]]

Figure\nbsp{}[[fig:booth-sima-path]] shows the paths taken by Simulated Annealing on
search spaces defined by variations of Booth's function. In panel (a) we can see
the effects  of the  annealing schedule,  which enables  large detours  to worse
points early on, but forces descent on later iterations. Since this search space
has no local  minima, the descent eventually reaches the  optimum.  In panel (b)
the higher initial temperature also allows  escaping the many local minima found
during exploration, but as the path approaches the global optimum with the lower
temperature of later  iterations it gets trapped by one  the local minima around
the global optimum. Likewise, encountering a constraint border early on in panel
(c) is not a challenge for Simulated  Annealing, since it can bounce back toward
worse points, but on later iterations the  method is forced to wander around the
border like gradient descent, also getting trapped at local minima.

Single-state methods for optimization provide  heuristics for exploring a search
space, usually  based on the  neighborhoods of the  points that compose  a path.
The cost of  measuring the objective function $f$ for  an entire neighborhood is
prohibitive in  large dimensional search  spaces, but methods such  as Simulated
Annealing  provide  strategies  for  escaping local  minima  without  completely
evaluating  a neighborhood.   Despite  that, the  local  nature of  single-state
methods means that the final results  are heavily reliant on the starting point.
Restarting and performing  parallel searches are among the  strategies to reduce
dependence on initial choices,  as is the idea of using  a population of points,
which we discuss next.

**** Population-Based Methods: Genetic Algorithms and Particle Swarm Optimization

#+NAME: fig:booth-genetic-path
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Representation of paths taken by a Genetic Algorithm
#+CAPTION: on the search spaces  defined by variations of the Booth function.
#+CAPTION: Hollow points and dashed lines mark members of previous populations
#+CAPTION: and the regions they covered, while filled points and complete
#+CAPTION: lines mark the final population.
#+CAPTION: Panels (a), (b), and (c) correspond to Equations\nbsp{}\ref{eq:f0},
#+CAPTION: \ref{eq:f1}, and \ref{eq:f2} respectively.
#+CAPTION: Contour plots and direction of greatest descent
#+CAPTION: are also shown, and the global optimum is marked with a
#+CAPTION: $\color{red}\boldsymbol{\times}$.
[[file:img/booth_genetic_algorithm_descent_path.pdf]]
*** Summary
** Statistical Learning
*** Introduction
- Optimization with Surrogate Models
- The Bias-Variance Trade-Off
- Inference and prediction
- Supervised and Unsupervised Learning
- Parametric and Nonparametric Statistics
- Model Assessment:
  - Train and Test Sets
  - Train and Test Mean Squared Error
  - Cross Validation, Bootstrapping
  - Information Criteria for Model Assessment
    - Mallow's C_p and AIC
    - BIC
*** Linear Regression
- Ordinary Least Squares: Gauss-Markov (BLUE)
- Transformations of $X$
- ANOVA
*** Gaussian Process Regression
- Sampling Functions from Multidimensional Gaussian Distributions
- Nonparametric Modeling with Covariance Kernels
- Sensitivity Analysis with Sobol Indices
- Other Nonparametric Methods
*** Summary
** Design of Experiments
*** Introduction
- Reducing Experimental Cost
- Improving Model Inference and Prediction
*** 2-Level Factorial Designs
*** Screening
*** Optimal Design
**** Mixing Categorical and Numerical Factors
**** Optimality Criteria
**** Exchange Algorithms for Optimal Design Construction
- KL-Exchange
*** Space-Filling Designs
*** Summary
** Online Learning
** Summary
The  effectiveness   of  stochastic  descent   methods  on  autotuning   can  be
limited\nbsp{}\cite{seymour2008comparison,balaprakash2011can,balaprakash2012experimental},
between other factors, by underlying hypotheses  about the search space, such as
the  reachability of  the  global optimum  and the  smoothness  of search  space
surfaces, which  are frequently not  respected. The derivation  of relationships
between  parameters  and  performance  from search  heuristic  optimizations  is
greatly hindered,  if not rendered impossible,  by the biased way  these methods
explore  parameters.   Some  parametric  learning methods,  such  as  Design  of
Experiments,  are  not widely  applied  to  autotuning.  These  methods  perform
structured  parameter  exploration,  and  can  be used  to  build  and  validate
performance     models,     generating    transparent     and     cost-effective
optimizations\nbsp{}\cite{mametjanov2015autotuning,bruel2019autotuning}.   Other
methods  from  the parametric  family  are  more  widely  used, such  as  Bandit
Algorithms\nbsp{}\cite{xu2017parallel}.  Nonparametric learning methods, such as
Decision   Trees\nbsp{}\cite{balaprakash2016automomml}   and  Gaussian   Process
Regression\nbsp{}\cite{parsa2019pabo}, are able to reduce model bias greatly, at
the  expense  of  increased  prediction  variance.   Figure\nbsp{}\ref{fig:tree}
categorizes some autotuning methods according to  some of the key hypotheses and
branching questions underlying each method.

#+NAME: fig:tree
#+ATTR_LATEX: :width \textwidth :placement [t]
#+CAPTION: Some autotuning methods
[[file:img/tree.pdf]]

** Applications to Autotuning                                     :noexport:
*** Function Minimization
**** OpenTuner: Leveraging Ensembles of Heuristics
***** The Multi-Armed Bandit, Area Under the Curve Approach
*** Learning
**** Software for Autotuning with Statistical Learning
***** The /R/ Language
**** Results using Statistical Learning
***** Linear Regression and Design of Experiments
***** Methods based on Machine Learning
*** Design of Experiments
**** Inference for Autotuning under a Tight Budget
***** Fractional Factorial Designs for Chip Design on FPGAs
*** Gaussian Process Regression
* Applications
** Research Methodology: Efforts for Reproducible Science
*** Introduction
*** Computational Documents with Emacs and Org mode
*** Versioning for Code, Text, and Data
** Application 1: CUDA Compiler
** Application 2: GPU Laplacian Kernel
** Application 3: High-Level Synthesis for FPGAs
** Application 4: CPU Kernels from the SPAPT Benchmark
** Application 5: Bit Quantization for Neural Networks
* Conclusion
#+begin_export latex
\bibliographystyle{IEEEtran}
\bibliography{bibliography/references}
#+end_export
