#+STARTUP: beamer overview indent inlineimages logdrawer
#+TITLE: @@latex: Toward Transparent and Parsimonious
#+TITLE: Methods \\ for Automatic Performance Tuning@@
#+AUTHOR:    \footnotesize Pedro Bruel \newline \scriptsize \emph{phrb@ime.usp.br}
#+DATE:      \scriptsize July 9 2021
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:nil @:t \n:nil ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   tex:t latex:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:

* LaTeX Setup                                      :B_ignoreheading:noexport:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

See [[Emacs Setup]] below for local buffer variables

** LaTeX Configuration
:latex_header:
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [10pt, compress, aspectratio=169, xcolor={table,usenames,dvipsnames}]
#+LATEX_HEADER: \mode<beamer>{\usetheme[numbering=fraction, progressbar=none, titleformat frame=regular, titleformat title=regular, sectionpage=progressbar]{metropolis}}

#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)

#+LATEX_HEADER: \usepackage{sourcecodepro}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}

#+LATEX_HEADER: \definecolor{Base}{HTML}{191F26}

# #+LATEX_HEADER: \definecolor{Accent}{HTML}{b10000}
# #+LATEX_HEADER: \colorlet{Accent}{PineGreen}
#+LATEX_HEADER: \colorlet{Accent}{OliveGreen!85!Black}

#+LATEX_HEADER: \colorlet{Highlight}{Accent!18}

#+LATEX_HEADER: \setbeamercolor{alerted text}{fg=Accent}
#+LATEX_HEADER: \setbeamercolor{frametitle}{fg=Accent,bg=normal text.bg}
#+LATEX_HEADER: \setbeamercolor{normal text}{bg=black!2,fg=Base}

#+LATEX_HEADER: \usefonttheme{professionalfonts}
#+LATEX_HEADER: \usepackage{newpxtext}
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: \setmonofont{DejaVuSansMono}
#+LATEX_HEADER: \usepackage{newpxmath}

#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
#+LATEX_HEADER:   commentstyle=\color{Accent},
# #+LATEX_HEADER:   escapeinside={\%*}{*)},
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   keywordstyle=\color{Accent},
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller[2]\relax}
#+LATEX_HEADER: \addtobeamertemplate{block begin}{}{\justifying}

#+LATEX_HEADER: \captionsetup[figure]{labelformat=empty}

#+LATEX_HEADER: \hypersetup{
#+LATEX_HEADER:     colorlinks=true,
#+LATEX_HEADER:     linkcolor={Accent},
#+LATEX_HEADER:     citecolor={Accent},
#+LATEX_HEADER:     urlcolor={Accent}
#+LATEX_HEADER: }

#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \setlength{\metropolis@titleseparator@linewidth}{1pt}
#+LATEX_HEADER: \setlength{\metropolis@progressonsectionpage@linewidth}{2.5pt}
# #+LATEX_HEADER: \setlength{\metropolis@progressinheadfoot@linewidth}{2pt}
#+LATEX_HEADER: \makeatother
:end:


* Introduction (5 min)
** Trends on Hardware Design
- Hardware has ceased to provide "effortless" performance gains
- Performance continues to increase
- Accelerators are important, but suffer from the same scaling limits
- Code optimization is crucial for performance, and will continue to be

** An Example of Autotuning: Loop Tiling and Unrolling
- How  to  restructure  memory  accesses  in loops  to  increase  throughput  by
  leveraging cache locality?
- Size and shape of the resulting search space
- Introduce notation: $f: \mathcal{X} \to \mathbb{R}$

** Autotuning Problems in Other Domains
- Number of parameters and combinations growing over time
- Earlier application to optimize BLAS routines
- Autotuning for specific domains and Neural Networks

** Common Approaches to Autotuning
- Function minimization methods
  - Online learning?
- Surrogate-based optimization
  - Linear Models
  - Gaussian Processes
- Design of Experiments

** Contributions of this Thesis
- Striving to develop and apply transparent and parsimonious autotuning methods
- Applications in this thesis:

#+begin_export latex
\begin{table}[]
  \renewcommand{\arraystretch}{1.5}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}ll@{}}
      \textbf{Domain} & \textbf{Method}         \\ \midrule
      CUDA compiler  parameters & \multirow{2}{*}{Function  minimization methods
        with Online Learning} \\
      FPGA compiler parameters &  \\
      OpenCL  Laplacian  Kernel  &  Function  minimization  methods, Linear Models, Gaussian Process Regression \\
      SPAPT Kernels & Linear Models, Gaussian Process Regression \\
      CNN Mixed-Precision Quantization & Gaussian Process Regression
    \end{tabular}%
  }
\end{table}
#+end_export

* Methods for Function Minimization (10 minutes)
** Overview
- We know information about $f$
- Derivative-based
- Other heuristics
** Search Heuristics with Multi-Armed Bandit
- OpenTuner
- Ensemble of search heuristics
  - Coordinated by a MAB algorithm (online learning)
** Application: High-Level Synthesis for FPGAs
** Search Space and Performance Metrics
** Results
** Discussion
- Curse of dimensionality
- It is often unclear:
  - if there is something to find, and when to stop exploring
- It is impossible to:
  - interpret optimizations
* Design of Experiments (15 minutes)
** Linear Models
- Learning: Building surrogates, used for optimization
- Introduce notation:
  - $\hat{f}_{\theta}: \mathcal{X} \to \mathbb{R}$
  - Model of $f$: $f(\mathbf{x}) = \mathbf{x}^{\transp}\theta + \varepsilon$
  - Surrogate $\hat{f}_{\theta}(\mathbf{x}) = \mathbf{x}^{\transp}\hat{\theta}$.
- Best Linear Unbiased Estimator
- Learning methods assume the design $\mathbf{X}$ is given
** Design of Experiments
- Statistical methods  to choose the  design $\mathbf{X}$ to  minimize surrogate
  model variance
- Notation:
  - Factors, levels, design
- Simple linear model example for 2-factor designs
- Factorial designs , screening
** Optimal Design
- Distributing points  according to initial modeling  hypotheses decreases model
  matrix determinant (associated with variance)
- Good for exploiting known search space structure, or verifying existing hypotheses
** Space-filling Designs
- Curse of dimensionality for sampling:
  - Most sampled points will be on the "shell"
- LHS: Partition and then sample, need to optimize later
- Low-discrepancy: deterministic space-filling sequences
** Interpreting Significance
- ANOVA for Linear Models
  - Isolate "significant" factors
- Sobol indices
  - expensive computation
** A Transparent and Parsimonious Approach to Autotuning
- Explain paper diagram
** Application: GPU Laplacian
** Search Space and Performance Metrics
** Results
- Comparison with multiple methods
- Leave GPR for later
** Interpreting the Optimization
** Application: SPAPT kernels
- Pick one?
** Search Spaces and Performance Metrics
** Results
- Is there anything to find?
- Leave GPR for later
** Interpreting the Optimization
** Discussion
- Motivating results in the Laplacian kernel
- It is possible to interpret results, guide optimization
  - sometimes simpler models give better results

- For SPAPT kernels, it is still unclear:
  - if there is something to find, and when to stop exploring
  - is there a global optimum, is it "hidden"?
    - how to find it, if so? (can learning do it?)

- Random Sampling has good performance
  - Abundance of local optima?

- What is the most effective level of abstraction for optimizing a program?
  - Compiler, kernel, machine, model, dependencies?

* Gaussian Process Regression (10 minutes)
** More Flexibility with Gaussian Process Regression
- Introduce notation:
  - Model of $f$: $f(\mathbf{x}) \sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$
  - Surrogate $\hat{f}_{\theta}(\mathbf{x}) \sim f(\mathbf{x}) \; \vert{} \; \mathbf{X}, \mathbf{y}$

** Expected Improvement: Balancing Exploitation and Exploration
- How to decide where to measure next?
** Application: GPU Laplacian and SPAPT
- GPR was applied to these problems too
** Results: GPU Laplacian
- GPR is good too, but the simpler model is more consistent
** Results: SPAPT
- GPR still can't find better configurations
** Application: Quantization for Convolutional Neural Networks
** Search Space, Constraints, and Performance Metrics
- Comparing with a Reinforcement Learning approach in the original paper
- ImageNet
** Results
** Interpreting the Optimization
- Sobol indices, inconclusive
** Discussion
- Low-discrepancy sampling in high dimension
- Constraints complicate exploration
- Multi-objective optimization
* Conclusion (5 min)
** Curse of Dimensionality for Autotuning Problems
- Implications for Sampling and Learning
- Space-filling helps, but does not solve
- Constraints
** Which method to use?
- Design  of  Experiments  for  transparency and  parsimony  when  building  and
  interpreting statistical models
- Linear models for simpler spaces and problems
- Gaussian Process Surrogates for more complex situations
** It is often unclear if there is something to find
- Abundance of local optima
- Is there a global optimum, is it "hidden"?
  - How to find it, if so? (can learning do it?)
- What is the most effective level of abstraction for optimizing a program?
  - Compiler, kernel, machine, model, dependencies?
- When to stop?
** Reproducibility of Performance Tuning Experiments
- Redoing all the work for different problems
- Complementary approaches:
  - Completely evaluate small sets of a search space
  - Collaborative optimizing for different architectures, problems
* Ending Title :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \maketitle

* Emacs Setup                                      :noexport:B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
See [[LaTeX Setup]] above for the beamer configuration

** Use XeLaTeX
If you  accept this definition  when loading the  buffer, this variable  will be
modified  locally to  the buffer.  This allows  using XeLaTeX  for exporting  to
beamer pdf.

# Local Variables:
# eval: (setq-local org-latex-pdf-process (list "latexmk -xelatex %f"))
# End:
