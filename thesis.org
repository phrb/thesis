#+STARTUP: overview indent inlineimages logdrawer
#+TITLE:  Autotuning Methods
#+AUTHOR:      Pedro Bruel
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: ExportableReports(E)
#+TAGS: FAPESP(f)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS: partless-book
#+LATEX_CLASS_OPTIONS: [11pt,twoside,a4paper]
#+LATEX_HEADER: \usepackage[a4paper]{geometry}
# #+LATEX_HEADER: \geometry{
# #+LATEX_HEADER:   %top=32mm,
# #+LATEX_HEADER:   %bottom=28mm,
# #+LATEX_HEADER:   %left=24mm,
# #+LATEX_HEADER:   %right=34mm,
# #+LATEX_HEADER:   textwidth=152mm, % 210-24-34
# #+LATEX_HEADER:   textheight=237mm, % 297-32-28
# #+LATEX_HEADER:   vmarginratio=8:7, % 32:28
# #+LATEX_HEADER:   hmarginratio=12:17, % 24:34
# #+LATEX_HEADER:   % Com geometry, esta medida não é tão relevante; basta garantir que ela
# #+LATEX_HEADER:   % seja menor que "top" e que o texto do cabeçalho caiba nela.
# #+LATEX_HEADER:   headheight=25.4mm,
# #+LATEX_HEADER:   % distância entre o início do texto principal e a base do cabeçalho;
# #+LATEX_HEADER:   % ou seja, o cabeçalho "invade" a margem superior nessa medida. Essa
# #+LATEX_HEADER:   % é a medida que determina a posição do cabeçalho
# #+LATEX_HEADER:   headsep=11mm,
# #+LATEX_HEADER:   footskip=10mm,
# #+LATEX_HEADER:   marginpar=20mm,
# #+LATEX_HEADER:   marginparsep=5mm,
# #+LATEX_HEADER: }
#+LATEX:HEADER: \usepackage[inline]{enumitem}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amssymb,amsthm}
#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{sourcecodepro}
# #+LATEX_HEADER: \usepackage{mathpazo}
#+LATEX_HEADER: \usepackage{newpxtext}
#+LATEX_HEADER: \usepackage{newpxmath}
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \usepackage{titling}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{tikz-qtree}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstdefinelanguage{Julia}%
#+LATEX_HEADER:   {morekeywords={abstract,struct,break,case,catch,const,continue,do,else,elseif,%
#+LATEX_HEADER:       end,export,false,for,function,immutable,mutable,using,import,importall,if,in,%
#+LATEX_HEADER:       macro,module,quote,return,switch,true,try,catch,type,typealias,%
#+LATEX_HEADER:       while,<:,+,-,::,/},%
#+LATEX_HEADER:    sensitive=true,%
#+LATEX_HEADER:    alsoother={$},%
#+LATEX_HEADER:    morecomment=[l]\#,%
#+LATEX_HEADER:    morecomment=[n]{\#=}{=\#},%
#+LATEX_HEADER:    morestring=[s]{"}{"},%
#+LATEX_HEADER:    morestring=[m]{'}{'},%
#+LATEX_HEADER: }[keywords,comments,strings]%
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
# #+LATEX_HEADER:   escapeinside={\%*}{*)},
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   language=R,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \setlength{\parskip}{0.6em}
#+LATEX_HEADER: \usepackage[pagestyles]{titlesec}
#+LATEX_HEADER: \titleformat{\chapter}[display]{\normalfont\bfseries}{}{0pt}{\Huge}
#+LATEX_HEADER: \newpagestyle{mystyle}
#+LATEX_HEADER: {\sethead[\thepage][][\chaptertitle]{}{}{\thepage}}
#+LATEX_HEADER: \pagestyle{mystyle}

* Thesis Drafts                                                    :noexport:
** Structure Drafts
1. Introduction
   1. Autotuning
      1. Algorithm Selection Problem?
   2. Overview of Autotuning Methods (taxonomy/decision tree)
   3. Search Heuristics
      - Introduction
      - OpenTuner
      - Autotuning GPU compiler parameters
      - Autotuning High Level Synthesis for FPGAs
   4. Statistical Learning
      - Parametric, nonparametric
   5. Related Work
      - Literature Review
2. Design of Experiments
   1. Introduction
      1. Linear Regression
   2. Screening
      1. Main effects
      2. Example with CUDA flags
   3. Factorial Designs
      1. Example?
   4. Optimal Design
      1. Properties of the BLUE, Information Matrix
      2. Variance-optimizing criteria
      3. Example on Laplacian GPU
   5. Autotuning SPAPT Kernels
      - Mixing factor types
      - Sampling with Constraints
      - Heteroscedasticity
3. Gaussian Process Regression
   1. Introduction
      1. Bayesian Linear Model (Rasmussen's Book)
      2. EGO
   2. Revisiting SPAPT kernels
   3. Quantization for Deep Neural Networks
4. Conclusion
   - Expressing structure with kernels? (Duvenaud's thesis)
   - Performance of the Federov Algorithm for D-Optimal design construction?
*** Structure Draft
- Course on performance optimization for HPC, and why it's hard
- Difficulty to optimize programs comes from complexity in:
  - Computer architecture
    - Pursuit of doubling performance, fitting more transistors,
      (Moore's Law), and the end of frequency and power
      scaling (Dennard's),
      mean that we need parallel architectures, which are more complex
  - Software
    - Parallel architectures are harder to program efficiently
** Underlying Hypotheses of Autotuning Methods
:PROPERTIES:
:EXPORT_FILE_NAME: hipotheses.pdf
:END:
*** Introduction                                                 :noexport:
Given  a program  with $X  \in \mathcal{X}$  configurable parameters,  we want  to
choose the best parameter values according  to a performance metric given by the
function  $f(X)$.   Autotuning methods  attempt  find  the $X_{*}$  that  minimizes
$f(\cdot)$.   Despite  their different  approaches,  autotuning  methods share  some
common hypotheses:

- There is no knowledge about the global optimal configuration
- There could be some problem-specific knowledge to exploit
- Measuring the effects of a choice of parameter values is possible but costly

Each  autotuning method  has  assumptions that  justify  its implementation  and
usage. Some of  these hypotheses are explicit,  such as the ones  that come from
the  linear model.   Others are  implicit,  such as  the ones  that support  the
implementation and the justification of optimization heuristics.
*** Overview of Autotuning Methods
:PROPERTIES:
:EXPORT_TITLE:
:EXPORT_FILE_NAME: tree.pdf
:END:
#+begin_export latex
\begin{sidewaysfigure}[t]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      for tree={%
        anchor = north,
        align = center,
        l sep+=1em
      },
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
        draw,
        [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
          draw,
          color = NavyBlue
          [{Search Heuristics},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{No}}
            [{\textbf{Random} \textbf{Sampling}}, draw]
            [{Reachable Optima},
              draw,
              color = BurntOrange
              [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$},
                draw,
                color = BurntOrange
                [{Strong $corr(f(X),d(X,X_{*}))$?},
                  draw,
                  color = NavyBlue
                  [{More Global},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{Introduce a \textit{population} of $X$\\\textbf{Genetic} \textbf{Algorithms}}, draw]
                    [, phantom]]
                  [{More Local},
                    draw,
                    color = BurntOrange,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{High local optima density?},
                      draw,
                      color = NavyBlue
                      [{Exploit Steepest Descent},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                        [{In a neighbourhood:\\\textbf{Greedy} \textbf{Search}}, draw]
                        [{Estimate $f^{\prime}(X)$\\\textbf{Gradient} \textbf{Descent}}, draw]]
                      [{Allows\\exploration},
                        draw,
                        color = BurntOrange,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                        [{Allow worse $f(X)$\\\textbf{Simulated} \textbf{Annealing}}, draw]
                        [{Avoid recent $X$\\\textbf{Tabu}\textbf{Search}}, draw]]]]]
                [,phantom]]
              [,phantom]]]
          [{Statistical Learning},
            draw,
            color = BurntOrange,
            edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
            [{Parametric Learning},
              draw,
              color = BurntOrange
              [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                draw,
                color = BurntOrange
                [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                [, phantom]]
              [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                draw,
                color = BurntOrange
                [, phantom]
                [{Check for model adequacy?},
                  draw,
                  alias = adequacy,
                  color = NavyBlue
                  [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                    draw,
                    alias = interactions,
                    color = NavyBlue,
                    edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                    [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                      draw
                      [, phantom]
                      [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                        edge = {-stealth, ForestGreen, semithick},
                        edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                        draw,
                        alias = estimate,
                        color = ForestGreen]]
                    [{\textbf{Optimal} \textbf{Design}},
                      draw,
                      alias = optimal,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [, phantom]
                  [{\textbf{Space-filling} \textbf{Designs}},
                    draw,
                    edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                    [, phantom]
                    [{Model selection},
                      edge = {-stealth, ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      alias = selection,
                      color = ForestGreen]]]]]
            [{Nonparametric Learning},
              draw,
              color = BurntOrange
              [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
              [{\textbf{Gaussian} \textbf{Process Regression}},
                alias = gaussian,
                draw]
              [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
      \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
      \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
      \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
      \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
    \end{forest}
  }
  \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
    denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
    denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
    denote algorithm choices, and \textbf{bold} boxes denote methods.}
\end{sidewaysfigure}
#+end_export

*** Previous Attempts                                            :noexport:
#+begin_export latex
\forestset{linebreaks/.style={for tree={align = center}}}
\begin{sidewaysfigure}
  \resizebox{\textwidth}{!}{%
    \begin{forest}
      linebreaks
      [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\ $Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$}
        [{Does not construct\\estimate $Y = \hat{f}(\cdot, \theta{}(X))$}
          [{Reachable\\optima}
            [{Strong $corr(f(X),f(X^{\prime}))$,\\for close $X,X^{\prime}$}
              [{Strong\\$corr(f(X),d(X,X_{*}))$}
                [{Low local\\optima density}
                  [{\textbf{Greedy}\\\textbf{Search}}, draw]
                  [{Estimate $f^{\prime}(X)$}
                    [{\textbf{Gradient}\\\textbf{Descent}}, draw]]]
                [{Introduce a ``population''\\$\mathbf{X} = (X_1,\dots,X_n)$}
                  [{Combination, mutation,\\within $\mathbf{X}$}
                    [{\textbf{Genetic}\\\textbf{Algorithms}}, draw]]
                  [{\textbf{Ant}\\\textbf{Colony}}, draw]]]
              [{Weaker\\$corr(f(X),d(X,X_{*}))$}
                [{Accept\\worst $f(X)$}
                  [{\textbf{Simulated}\\\textbf{Annealing}}, draw]]
                [{Avoid\\recent $X$}
                  [{\textbf{Tabu}\\\textbf{Search}}, draw]]]]]
          [{\textbf{Random}\\\textbf{Sampling}}, draw]]
        [{Constructs surrogate\\estimate $\hat{f}(\cdot, \theta(X))$}
          [{Parametric\\Learning}
            [{$\hat{f}(X) \approx f_1(X_1) + \dots + f_k(X_k)$}
              [{\textbf{Independent}\\\textbf{Bandit}}, draw]]
            [{$\hat{f}(X) = \mathcal{B}(logit(\mathcal{M}(X)\theta(X) + \varepsilon))$}
              [{\textbf{Logistic}\\\textbf{Regression}}, draw]]
            [{$\hat{f}(X) = \mathcal{M}(X)\theta(X) + \varepsilon$}
              [{\textbf{Linear}\\\textbf{Regression}}, draw]
              [{Measure\\properties of $X$}
                [{Independance\\of effects}
                  [{\textbf{Screening}}, draw]]
                [{Homoscedasticity of $\varepsilon$}
                  [{\textbf{Optimal}\\\textbf{Design}}, draw]]]]]
          [{Nonparametric\\Learning}
            [{Splitting\\rules on $X$}
              [{\textbf{Decision}\\\textbf{Trees}}, draw]]
            [{$\hat{f} = \mathcal{GP}(X; \mathcal{K})$}
              [{\textbf{Gaussian}\\\textbf{Process Regression}}, draw]]
            [{\textbf{Neural}\\\textbf{Networks}}, draw]
            [{\textbf{Multi-armed}\\\textbf{Bandit (?)}}, draw]]]]
    \end{forest}
  }
  \caption{Some hypothesis of some autotuning methods}
\end{sidewaysfigure}

#+end_export

#+begin_export latex
\newcommand{\tabitem}{~~\llap{\textbullet}~~}

\begin{table}[ht]
  \center
  \begin{tabular}{@{}p{0.3\textwidth}p{0.5\textwidth}@{}}
    \toprule
    Method &  Hypotheses \\ \midrule
    Metaheuristics & \tabitem There are similarities between natural fenomena and the target problem \\
    & \tabitem Gradual changes in configurations produce gradual changes in performance \\
    & \tabitem The optimal configuration is ``reachable'', by small changes, from non-optimal configurations  \\
    \addlinespace \\
    Machine Learning & \tabitem As more samples are obtained, decreases in ``out-of-sample error'' imply decreases ``in-sample error'' \\
    & \tabitem \textbf{TODO} What are the classes of models? \\
    \addlinespace \\
    Design of Experiments & \tabitem There is ``exploitable search space structure''\\
    & \tabitem Linear model: Response $\bm{Y}$ is an ``unobservable function'' of parameters $\bm{X}$: \\
    & \hspace{0.15\textwidth} $f(\bm{X}) = \bm{Y} = \bm{X\beta} + \bm{\varepsilon}$ \\
    & \tabitem Optimal Design: Variance of estimator $\hat{\bm{\beta}}$ is proportional to $\bm{X}$: \\
    & \hspace{0.15\textwidth} $\bm{\hat{\beta}} = \left(\bm{X}^{\intercal}\bm{X}\right)^{-1}\bm{X}^{\intercal}\bm{Y}$ \\
    \addlinespace \\
    Gaussian Process Regression & \tabitem Response $\bm{Y}$ is a sample from a multidimensional Gaussian distribution, with mean $m(\bf{X})$ and variance $k(\bm{X}, \bm{X}^{\intercal})$: \\
    & \hspace{0.1\textwidth} $\bm{Y} = f(\bm{X}) \sim \mathcal{N}(m(\bm{X}), k(\bm{X}, \bm{X}^{\intercal}))$ \\
    & \tabitem Predictions $\bm{Y_{*}}$ can be made conditioning distribution to observed data\\ \bottomrule
  \end{tabular}%
\end{table}
#+end_export

#+begin_export latex
\resizebox{!}{\textheight}{%
  \begin{tikzpicture}[rotate = -90]
    \begin{scope}
      \tikzset{every tree node/.style = {align = center}}
      \tikzset{level 1+/.style={level distance = 40pt}}
      \Tree [.\node(n0){Minimize $f: X \mapsto \mathbb{R}$ \\ $f(X) = f^{*}(X) + \varepsilon = m$};
        [.{Does not construct \\ estimate $\hat{f}(X; \theta)$}
          [.{Reachability of \\ optima}
            [.{\textbf{Greedy} \\ \textbf{Search}} ]
            [.{$d(x_i, x_j) \to 0$ $\implies$ \\ $d(f(x_i), f(x_j)) \to 0$}
              [.{Abundance of \\ local optima}
                [.{\textbf{Simulated} \\ \textbf{Annealing}} ]]
              [.{Closeness of a \\ ``population'' of $X$}
                [.{\textbf{Genetic} \\ \textbf{Algorithms}} ]]]]
          [.{\textbf{Random} \\ \textbf{Sampling}} ] ]
        [.\node(r1){Constructs surrogate \\ estimate $\hat{f}(X; \theta)$};
          [.{Explicit, variable \\ models of $\theta$}
            [.{$\hat{f} = M(X)\theta + \varepsilon$}
              [.{Independance \\ of effects}
                [.{\textbf{Screening}} ] ]
              [.{Homoscedasticity}
                [.{\textbf{Optimal} \\ \textbf{Design}} ] ] ] ]
          [.{Implicit, fixed \\ models of $\theta$}
            [.{\textbf{Neural Networks}} ] ]
          [.{Samples \\ functions}
            [.{$\hat{f} = \mathcal{GP}(X; \theta, \mathcal{K})$}
              [.{\textbf{Gaussian Process} \\ \textbf{Regression}} ] ] ] ] ]
    \end{scope}
    % \begin{scope}[thick]
    %   \draw [color = orange] (n0) to [bend left = 2] (r1);
    %   \draw [color = green] (n0) to [bend right = 2] (r1);
    % \end{scope}
  \end{tikzpicture}
}
#+end_export
** Application to the Microsoft Latin America PhD Award
*** Search Heuristics and Statistical Learning methods for Autotuning HPC Programs
:PROPERTIES:
:EXPORT_DATE:
:EXPORT_TITLE: @@latex: Search Heuristics and Statistical Learning \\ Methods for Program Autotuning@@
:EXPORT_FILE_NAME: application.pdf
:EXPORT_AUTHOR: Pedro Bruel
:END:

#+latex: \vspace{-4em}

High Performance Computing  has been a cornerstone of  collective scientific and
industrial progress  for at least  five decades.   Paying the cost  of increased
complexity,  software and  hardware  engineering advances  continue to  overcome
several challenges on the way of the sustained performance improvements observed
during the last  fifty years.  This mounting complexity means  that reaching the
advertised hardware  performance for  a given program  requires not  only expert
knowledge  of a  given hardware  architecture, but  also mastery  of programming
models  and  languages for  parallel  and  distributed  computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance,  we  can  draw   and  adapt  proven  methods  from  search,
mathematical  optimization and  statistics. The  effectiveness of  these adapted
methods  on autotuning  problems varies  greatly,  and hinges  on practical  and
mathematical properties of the problem and the corresponding /search space/.

When  adapting methods  for autotuning,  we must  face challenges  emerging from
practical properties  such as restricted  time and cost budgets,  constraints on
feasible  parameter values,  and the  need to  mix /categorical/,  /continuous/, and
/discrete/ parameters. To achieve useful results, we must also choose methods that
make hypotheses compatible with problem search  spaces, such as the existence of
discoverable,  or at  least  exploitable, relationships  between parameters  and
performance.   Choosing  an autotuning  method  requires  determining a  balance
between the exploration of a problem, when we would seek to discover and explain
relationships between  parameters and performance,  and the exploitation  of the
best optimizations we can find, when we would seek only to minimize performance.

The    effectiveness   of    search    heuristics   on    autotuning   can    be
limited\nbsp{}\cite{seymour2008comparison,balaprakash2011can,balaprakash2012experimental},
between other factors, by underlying hypotheses  about the search space, such as
the  reachability of  the  global optimum  and the  smoothness  of search  space
surfaces, which  are frequently not  respected. The derivation  of relationships
between  parameters  and  performance  from search  heuristic  optimizations  is
greatly hindered,  if not rendered impossible,  by the biased way  these methods
explore  parameters.   Some  parametric  learning methods,  such  as  Design  of
Experiments,  are  not widely  applied  to  autotuning.  These  methods  perform
structured  parameter  exploration,  and  can  be used  to  build  and  validate
performance     models,     generating    transparent     and     cost-effective
optimizations\nbsp{}\cite{mametjanov2015autotuning,bruel2019autotuning}. Other methods
from   the  parametric   family   are   more  widely   used,   such  as   Bandit
Algorithms\nbsp{}\cite{xu2017parallel}.   Nonparametric  learning   methods,  such  as
Decision    Trees\nbsp{}\cite{balaprakash2016automomml}     and    Gaussian    Process
Regression\nbsp{}\cite{parsa2019pabo}, are able  to reduce model bias  greatly, at the
expense of increased prediction variance. Figure\nbsp{}\ref{fig:tree} categorizes some
autotuning  methods  according to  some  of  the  key hypotheses  and  branching
questions underlying each method.

During this  thesis I have  adapted and  studied the effectiveness  of different
search heuristics and statistical learning  methods on optimizing performance on
several autotuning domains.  During the beginning of my PhD at the University of
São Paulo (USP), I have published a paper on optimizing the configuration of the
CUDA compiler\nbsp{}\cite{bruel2017autotuning},  where we have  reached up to  4 times
performance improvement  in comparison with a  high-level compiler optimization.
In collaboration with researchers from  Hewlett Packard Enterprise (HPE) in Palo
Alto, I wrote a  paper on the autotuning of a  compiler for High-Level Synthesis
for FPGAs\nbsp{}\cite{bruel2017autotuninghls}, where we  have reached, on average, 25%
improvements on  performance, size, and  complexity of  designs.

At the  end of 2017,  I joined  the /cotutelle/ PhD  program at the  University of
Grenoble Alpes  (UGA) and  became a member  of the POLARIS  Inria team,  where I
applied  Design  of   Experiments  to  the  autotuning   of  a  source-to-source
transformation  compiler\nbsp{}\cite{bruel2019autotuning},  where  we  showed  we  can
achieve significant speedup by exploiting  search space structure using a strict
budget.   I also  have  collaborated with  HPE on  another  paper, providing  an
analysis  of the  applicability  of autotuning  methods  to a  Hardware-Software
Co-design  problem\nbsp{}\cite{bruel2017generalize}.   During  my  Teaching  Assistant
internships,  I  have  published one  paper\nbsp{}\cite{bruel2017openmp}  on  parallel
programming  teaching, and  collaborated on  another\nbsp{}\cite{goncalves2016openmp},
where we showed that teaching lower level programming models, despite being more
challenging at first, provides a stronger core understanding.

I continue to collaborate with HPE  researchers on the application of autotuning
methods to  optimize Neural Networks,  hardware accelerators for  Deep Learning,
and  algorithms  for dealing  with  network  congestion.   With my  advisors,  I
currently manage  1 undergraduate and 4  masters students, who are  applying the
statistical  learning autotuning  methods  I studied  and  adapted to  different
domains  in the  context of  a joint  USP/HPE research  project.  I  am strongly
motivated to continue pursuing a career  on Computer Science research, aiming to
produce  rigorous and  value-adding  contributions. I  hereby  submit my  thesis
proposal and application to the Microsoft Latin America PhD Award.
#+begin_export latex
\begin{center}
  \begin{figure}[t]
    \resizebox{.9\textwidth}{!}{%
      \begin{forest}
        for tree={%
          anchor = north,
          align = center,
          l sep+=1em
        },
        [{Minimize $f: \mathcal{X} \mapsto \mathbb{R}$,\\$Y = f(X = (x_1,\dots,x_k) \in \mathcal{X}) + \varepsilon$},
          draw,
          [{Constructs surrogate estimate $\hat{f}(\cdot, \theta(X))$?},
            draw,
            color = NavyBlue
            [{Search Heuristics},
              draw,
              color = BurntOrange,
              edge label = {node[midway, fill=white, font = \scriptsize]{No}}
              [{\textbf{Random} \textbf{Sampling}}, draw]
              [{Reachable Optima},
                draw,
                color = BurntOrange
                [, phantom]
                [{Underlying Hypotheses \\ \textbf{Heuristics}}, draw]]]
            [{Statistical Learning},
              draw,
              color = BurntOrange,
              edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
              [{Parametric Learning},
                draw,
                color = BurntOrange
                [{$\forall{}i: x_i \in X$ is discrete\\$\hat{f}(X) \approx f_1(x_1) + \dots + f_k(x_k)$},
                  draw,
                  color = BurntOrange
                  [{\textbf{Independent Bandits}\\for each $x_i$:\textbf{UCB},\textbf{EXP3},$\dots$}, draw]
                  [, phantom]]
                [{Linear Model\\$\hat{f} = \mathcal{M}(X)\theta{}(X) + \varepsilon$},
                  draw,
                  color = BurntOrange
                  [, phantom]
                  [{Check for model adequacy?},
                    draw,
                    alias = adequacy,
                    color = NavyBlue
                    [{Consider interactions?\\{$\exists x_i \neq x_j:\; \theta(x_ix_j) \neq 0$}},
                      draw,
                      alias = interactions,
                      color = NavyBlue,
                      edge label = {node[midway, fill=white, font = \scriptsize]{No}}
                      [{$\forall x_i \in X: x_i \in \{-1, 1\}$\\\textbf{Screening} \textbf{Designs}},
                        edge label = {node[midway, fill=white, font = \scriptsize]{No}},
                        draw
                        [, phantom]
                        [{Select $\hat{X}_{*}$, reduce dimension of $\mathcal{X}$},
                          edge = {-stealth, ForestGreen, semithick},
                          edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                          draw,
                          alias = estimate,
                          color = ForestGreen]]
                      [{\textbf{Optimal} \textbf{Design}},
                        draw,
                        alias = optimal,
                        edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}]]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [, phantom]
                    [{\textbf{Space-filling} \textbf{Designs}},
                      draw,
                      edge label = {node[midway, fill=white, font = \scriptsize]{Yes}}
                      [, phantom]
                      [{Model selection},
                        edge = {-stealth, ForestGreen, semithick},
                        edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                        draw,
                        alias = selection,
                        color = ForestGreen]]]]]
              [{Nonparametric Learning},
                draw,
                color = BurntOrange
                [{Splitting rules on X\\\textbf{Decision} \textbf{Trees}},
                  draw
                  [, phantom]
                  [{Estimate $\hat{f}(\cdot)$ and $uncertainty(\hat{f}(\cdot))$},
                    edge = {-stealth, ForestGreen, semithick},
                    draw,
                    alias = uncertainty,
                    color = ForestGreen
                    [{Minimize $uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Explore}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X)$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit}},
                      draw,
                      color = ForestGreen]
                    [{Minimize $\hat{f}(X) - uncertainty(\hat{f}(X))$},
                      edge = {ForestGreen, semithick},
                      edge label = {node[midway, fill=white, font = \scriptsize]{Exploit$+$Explore}},
                      draw,
                      color = ForestGreen]]]
                [{\textbf{Gaussian} \textbf{Process Regression}},
                  alias = gaussian,
                  draw]
                [{\textbf{Neural} \textbf{Networks}}, draw]]]]]
        \draw [-stealth, semithick, ForestGreen](selection) to [bend left=27] node[near start, fill=white, font = \scriptsize] {Exploit} (adequacy.south);
        \draw [-stealth, semithick, ForestGreen](estimate.east) to [bend right=37] node[near start, fill=white, font = \scriptsize] {Explore} (adequacy.south) ;
        \draw [-stealth, semithick, ForestGreen](gaussian) to (uncertainty);
        \draw [-stealth, semithick, ForestGreen](optimal) to node[midway, fill=white, font = \scriptsize] {Exploit} (estimate) ;
      \end{forest}
    }
    \caption{A high-level view of autotuning methods, where \textcolor{NavyBlue}{\textbf{blue}} boxes
      denote branching questions, \textcolor{BurntOrange}{\textbf{orange}} boxes
      denote key hypotheses, \textcolor{ForestGreen}{\textbf{green}} boxes
      highlight exploration and exploitation choices, and \textbf{bold} boxes denote methods.}
    \label{fig:tree}
  \end{figure}
\end{center}
#+end_export

#+latex: \newpage

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
*** (Short) Search Heuristics and Statistical Learning methods for Autotuning HPC Programs
:PROPERTIES:
:EXPORT_DATE:
:EXPORT_TITLE: @@latex: Search Heuristics and Statistical Learning \\ Methods for Program Autotuning@@
:EXPORT_FILE_NAME: short-application.pdf
:EXPORT_AUTHOR: Pedro Bruel
:END:

#+latex: \vspace{-3em}

High Performance Computing  has been a cornerstone of  collective scientific and
industrial progress  for at least  five decades.   Paying the cost  of increased
complexity,  software and  hardware  engineering advances  continue to  overcome
several challenges on the way of the sustained performance improvements observed
during the last  fifty years.  This mounting complexity means  that reaching the
advertised hardware  performance for  a given program  requires not  only expert
knowledge  of a  given hardware  architecture, but  also mastery  of programming
models  and  languages for  parallel  and  distributed  computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance,  we  can  draw   and  adapt  proven  methods  from  search,
mathematical  optimization and  statistics. The  effectiveness of  these adapted
methods  on autotuning  problems varies  greatly,  and hinges  on practical  and
mathematical properties of the problem and the corresponding /search space/.

When  adapting methods  for autotuning,  we must  face challenges  emerging from
practical properties  such as restricted  time and cost budgets,  constraints on
feasible  parameter values,  and the  need to  mix /categorical/,  /continuous/, and
/discrete/ parameters. To achieve useful results, we must also choose methods that
make hypotheses compatible with problem search  spaces, such as the existence of
discoverable,  or at  least  exploitable, relationships  between parameters  and
performance.   Choosing  an autotuning  method  requires  determining a  balance
between the exploration of a problem, when we would seek to discover and explain
relationships between  parameters and performance,  and the exploitation  of the
best optimizations we can find, when we would seek only to minimize performance.

During this  thesis I have  adapted and  studied the effectiveness  of different
search heuristics and statistical learning  methods on optimizing performance on
several autotuning domains.  During the beginning of my PhD at the University of
São Paulo (USP), I have published a paper on optimizing the configuration of the
CUDA compiler\nbsp{}\cite{bruel2017autotuning},  where we have  reached up to  4 times
performance improvement  in comparison with a  high-level compiler optimization.
In collaboration with researchers from  Hewlett Packard Enterprise (HPE) in Palo
Alto, I wrote a  paper on the autotuning of a  compiler for High-Level Synthesis
for FPGAs\nbsp{}\cite{bruel2017autotuninghls}, where we  have reached, on average, 25%
improvements on  performance, size, and  complexity of  designs.

At the  end of 2017,  I joined  the /cotutelle/ PhD  program at the  University of
Grenoble Alpes  (UGA) and  became a member  of the POLARIS  Inria team,  where I
applied  Design  of   Experiments  to  the  autotuning   of  a  source-to-source
transformation  compiler\nbsp{}\cite{bruel2019autotuning},  where  we  showed  we  can
achieve significant speedup by exploiting  search space structure using a strict
budget.   I also  have  collaborated with  HPE on  another  paper, providing  an
analysis  of the  applicability  of autotuning  methods  to a  Hardware-Software
Co-design  problem\nbsp{}\cite{bruel2017generalize}.

I continue to collaborate with HPE  researchers on the application of autotuning
methods to  optimize Neural Networks,  hardware accelerators for  Deep Learning,
and  algorithms  for dealing  with  network  congestion.   With my  advisors,  I
currently manage  1 undergraduate and 4  masters students, who are  applying the
statistical  learning autotuning  methods  I studied  and  adapted to  different
domains  in the  context of  a joint  USP/HPE research  project.  I  am strongly
motivated to continue pursuing a career  on Computer Science research, aiming to
produce  rigorous and  value-adding  contributions. I  hereby  submit my  thesis
proposal and application to the Microsoft Latin America PhD Award.

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
* Generating Figures                                               :noexport:
** Chapter 1
*** 49 Years of Processor Data
**** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(dplyr)
df_freq <- read.csv("data/wiki_data/frequency.csv", header = TRUE)
df_transistor <- read.csv("data/wiki_data/transistor_count.csv", header = TRUE)
#+end_SRC

#+RESULTS:

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_freq)
#+end_SRC

#+RESULTS:
#+begin_example
'data.frame':	199 obs. of  12 variables:
 $ date               : int  1971 1972 1972 1972 1972 1973 1973 1973 1974 1974 ...
 $ name               : chr  "4004" "PPS-25" "μPD700" "8008" ...
 $ designer           : chr  "Intel" "Fairchild" "NEC" "Intel" ...
 $ max_clock_khz      : int  740 400 NA 500 200 NA NA NA 715 NA ...
 $ max_clock_mhz      : num  NA NA NA NA NA 2 1 1 NA 2 ...
 $ max_clock_ghz      : num  NA NA NA NA NA NA NA NA NA NA ...
 $ process_micro_m    : num  10 NA NA 10 NA 7.5 6 NA NA 6 ...
 $ process_nm         : int  NA NA NA NA NA NA NA NA NA NA ...
 $ chips              : int  1 2 1 1 1 1 1 1 3 1 ...
 $ transistor_count   : int  2250 NA NA 3500 NA 2500 2800 NA NA 6000 ...
 $ transistor_millions: num  NA NA NA NA NA NA NA NA NA NA ...
 $ logical_cores      : int  1 1 1 1 1 1 1 1 1 1 ...
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_transistor)
#+end_SRC

#+RESULTS:
: 'data.frame':	151 obs. of  6 variables:
:  $ name            : chr  "Intel 4004 " "Intel 8008 " "Toshiba TLCS-12 " "Intel 4040 " ...
:  $ transistor_count: num  2250 3500 11000 3000 4100 ...
:  $ date            : int  1971 1972 1973 1974 1974 1974 1974 1975 1976 1976 ...
:  $ designer        : chr  "Intel" "Intel" "Toshiba" "Intel" ...
:  $ process_nm      : int  10000 10000 6000 10000 6000 6000 8000 8000 5000 4000 ...
:  $ area_mm         : num  12 14 32 12 16 20 11 21 27 18 ...

**** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/49_years_processor_data.pdf" :width 10 :height 5 :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")


point_alpha = 0.9
point_size = 2

shapes = c(0, 1, 2, 5)

ggplot() +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_nm,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = process_micro_m * 1e3,
                   shape = "Process (nanometers)",
                   color = "Process (nanometers)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = logical_cores,
                   shape = "Logical Cores (Count)",
                   color = "Logical Cores (Count)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_khz * 1e-3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_mhz,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = max_clock_ghz * 1e3,
                   shape = "Frequency (MHz)",
                   color = "Frequency (MHz)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_freq,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_millions * 1e3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    geom_point(data = df_transistor,
               alpha = point_alpha,
               size = point_size,
               aes(x = date,
                   y = transistor_count * 1e-3,
                   shape = "Transistors (Thousands)",
                   color = "Transistors (Thousands)")) +
    xlab("Year") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 18) +
    theme(axis.title.y = element_blank(),
          legend.position = c(0.14, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 14),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/49_years_processor_data.pdf]]
*** TOP500 Rmax/Rpeak and Power
**** Loading Data and Packages
Load the /csv/:

#+begin_SRC R :results output :session *R* :exports code :eval no-export
library(dplyr)
library(tidyr)
library(ggplot2)

df <- read.csv("./data/top500/TOP500_history.csv")
#+end_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example
**** Looking at Data
***** Column Names
We  have many  columns  filled with  `NA`s,  due to  how  metrics were  measured
differently over the years. There's data from 1993 to 2019!

#+begin_SRC R :results output :session *R* :exports both :eval no-export
names(df)
#+end_SRC

#+RESULTS:
#+begin_example
 [1] "Year"                            "Month"
 [3] "Day"                             "Rank"
 [5] "Site"                            "Manufacturer"
 [7] "Computer"                        "Country"
 [9] "Processors"                      "RMax"
[11] "RPeak"                           "Nmax"
[13] "Nhalf"                           "Processor.Family"
[15] "Processor"                       "Processor.Speed..MHz."
[17] "System.Family"                   "Operating.System"
[19] "Architecture"                    "Segment"
[21] "Application.Area"                "Interconnect.Family"
[23] "Interconnect"                    "Region"
[25] "Continent"                       "Power"
[27] "System.Model"                    "Total.Cores"
[29] "Measured.Size"                   "Processor.Cores"
[31] "Accelerator"                     "Name"
[33] "Accelerator.Cores"               "Efficiency...."
[35] "Mflops.Watt"                     "Processor.Technology"
[37] "OS.Family"                       "Cores.per.Socket"
[39] "Processor.Generation"            "Previous.Rank"
[41] "First.Appearance"                "First.Rank"
[43] "Accelerator.Co.Processor.Cores"  "Accelerator.Co.Processor"
[45] "Power.Source"                    "Rmax..TFlop.s."
[47] "Rpeak..TFlop.s."                 "HPCG..TFlop.s."
[49] "Power..kW."                      "Power.Effeciency..GFlops.Watts."
[51] "Site.ID"                         "System.ID"
#+end_example

***** Achieved / Max Performance
#+begin_SRC R :results graphics output :session *R* :file "./img/top500_rmax_rpeak.pdf" :width 10 :height 5 :exports both :eval no-export
library(ggplot2)
library(extrafont)
library(scales)

loadfonts(device = "postscript")

point_size = 2.8
shapes = c(0, 1, 2, 5)

plot_df <- df %>%
    filter(Rank <= 1) %>%
    mutate(RMaxT = coalesce(RMax / 1e3, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak / 1e3, Rpeak..TFlop.s.),
           Power = coalesce(Power, Power..kW.)) %>%
    select(Rank,
           Year,
           Power,
           RMaxT,
           RPeakT) %>%
    distinct(Rank, Year, .keep_all = TRUE) %>%
    mutate(Ratio = RMaxT / RPeakT) %>%
    filter(is.finite(Ratio) & Ratio <= 1.0)

ggplot() +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RMaxT,
                   shape = "RMax",
                   color = "RMax")) +
    geom_point(data = plot_df,
               size = point_size,
               aes(x = Year,
                   y = RPeakT,
                   shape = "RPeak",
                   color = "RPeak")) +
    # geom_point(data = plot_df,
    #            size = point_size,
    #            aes(x = Year,
    #                y = Power,
    #                shape = "Power (kW)",
    #                color = "Power (kW)")) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    ylab("TFlops/s") +
    scale_color_brewer(name = element_blank(), palette = "Set1", direction = 1) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_log10(breaks = trans_breaks(trans = "log10",
                                        inv = function(x) 10 ^ x,
                                        n = 7),
                  labels = trans_format("log10",
                                        math_format(10 ^ .x))) +
    theme_bw(base_size = 20) +
    theme(legend.position = c(0.07, 0.86),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 16),
          text = element_text(family = "Liberation Sans")) +
    guides(color = guide_legend(nrow = 4,
                                override.aes = list(alpha = 1.0,
                                                    size = 2)))
#+end_SRC

#+RESULTS:
[[file:./img/top500_rmax_rpeak.pdf]]

***** Other Plots
****** Processor Clock
Supercomputer  clock  explosion  and  range  broadening.  Even  top-tier  clocks
stagnate after 2008.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_processors_clock.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Processor.Speed..MHz. / 1000,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
                                        #scale_y_log10() +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Processor Clock (GHz)") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_processors_clock.pdf]]

****** Processors
Core count sustained  exponential increase.  Although top-tier  core count still
increases, range  broadening around  2012 can be  explained by  introduction and
ubiquity of accelerator cores on all tiers.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_total_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)
library(tidyr)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores) - replace_na(Accelerator.Co.Processor.Cores, 0)) %>%
    select(Rank, Year, AllCores, Accelerator.Co.Processor.Cores) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("AllCores",
                                    "Accelerator.Co.Processor.Cores"),
                         labels = c("Processor Cores",
                                    "Accelerator Cores"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Core Count") +
    scale_y_log10() +
    # annotation_logticks(sides = "l") +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.67, 0.08),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 4)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_total_cores.pdf]]
****** RPeak and RMax
Sustained increase of theoretical peak and  achieved max performance on HPL and,
most recently,  on the  HPCG benchmark.  RPeak does not  guarantee rank  on some
cases.

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rpeak.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(RMax = RMax / 1e3,
           RPeak = RPeak / 1e3,
           RMaxT = coalesce(RMax, Rmax..TFlop.s.),
           RPeakT = coalesce(RPeak, Rpeak..TFlop.s.)) %>%
    select(Rank,
           Year,
           RMaxT,
           RPeakT,
           HPCG..TFlop.s.) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakT",
                                    "RMaxT",
                                    "HPCG..TFlop.s."),
                         labels = c("RPeak (HPL)",
                                    "RMax (HPL)",
                                    "RMax (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance (TFlops/s)") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.83, 0.09),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 3)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rpeak.pdf]]
****** RMax / Cores
Ratio of performance and core count, for HPL and HPCG. Is this sustained increase due only to accelerator cores, or are there other engineering and software advances?
#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_rmax_cores.pdf" :width 17.5 :height 7 :exports both :eval no-export
library(ggplot2)

plot_df <- df %>%
    mutate(AllCores = coalesce(Processors, Total.Cores)) %>%
    mutate(RMax = (RMax / 1e3) / AllCores,
           RPeak = (RPeak / 1e3) / AllCores,
           Rmax..TFlop.s. = Rmax..TFlop.s. / AllCores,
           Rpeak..TFlop.s. = Rpeak..TFlop.s. / AllCores,
           RMaxC = coalesce(RMax, Rmax..TFlop.s.),
           RPeakC = coalesce(RPeak, Rpeak..TFlop.s.),
           HPCGC = HPCG..TFlop.s. / AllCores) %>%
    select(Rank,
           Year,
           RMaxC,
           RPeakC,
           HPCGC) %>%
    gather(-Rank, -Year, key = "Type", value = "Count") %>%
    mutate(Type = factor(Type,
                         levels = c("RPeakC",
                                    "RMaxC",
                                    "HPCGC"),
                         labels = c("RPeak / Cores (HPL)",
                                    "RMax / Cores (HPL)",
                                    "RMax / Cores (HPCG)"))) %>%
    filter(is.finite(Count))

ggplot() +
    geom_jitter(data = plot_df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Count,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  6) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Performance / Core Count") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.85, 0.1),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          strip.text.x = element_text(size = 28),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4))) +
    facet_wrap(. ~ Type, ncol = 5)
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_rmax_cores.pdf]]

****** NMax
Exponential increase of problem size to reach max performance. Why is there
range broadening after 2011?

#+begin_SRC R :results graphics output :session *R* :file "../res/top500_graphs/top500_nmax.pdf" :width 10 :height 10 :exports both :eval no-export
library(ggplot2)

ggplot() +
    geom_jitter(data = df,
                alpha = 0.5,
                height = 0.0,
                size = 1.5,
                aes(x = Year,
                    y = Nmax,
                    color = cut(Rank,
                                breaks = c(1, 167, 334, 500),
                                include.lowest = TRUE))) +
    scale_x_continuous(breaks = function(x) { seq(floor(min(x)),
                                                  ceiling(max(x)),
                                                  4) }) +
    scale_color_brewer(name = "TOP500 Rank", palette = "Set1") +
    ylab("Problem Size to Reach RMax") +
    scale_y_log10() +
    theme_bw(base_size = 27) +
    theme(legend.position = c(0.25, 0.95),
          legend.direction = "horizontal",
          legend.background = element_rect(fill = "transparent", colour = NA),
          legend.text = element_text(size = 15),
          legend.title = element_text(size = 15),
          axis.text.y = element_text(angle = 90, hjust = 0.5)) +
    guides(color = guide_legend(nrow = 3, override.aes = list(alpha = 1.0, size = 4)))
#+end_SRC

#+RESULTS:
[[file:../res/top500_graphs/top500_nmax.pdf]]

*** Search Spaces
**** Load Data
#+begin_SRC R :results output :session *R* :eval no-export :exports results
library(extrafont)

df_search_spaces <- read.csv("data/search_spaces/search_spaces.csv")

loadfonts(device = "postscript")
#+end_SRC

#+RESULTS:
#+begin_example

Akaash already registered with postscriptFonts().
AkrutiMal1 already registered with postscriptFonts().
AkrutiMal2 already registered with postscriptFonts().
AkrutiTml1 already registered with postscriptFonts().
AkrutiTml2 already registered with postscriptFonts().
Anonymice Powerline already registered with postscriptFonts().
Arimo for Powerline already registered with postscriptFonts().
Bitstream Vera Sans already registered with postscriptFonts().
Bitstream Vera Sans Mono already registered with postscriptFonts().
Bitstream Vera Serif already registered with postscriptFonts().
Cousine for Powerline already registered with postscriptFonts().
IBM 3270 already registered with postscriptFonts().
IBM 3270 Narrow already registered with postscriptFonts().
IBM 3270 Semi-Narrow already registered with postscriptFonts().
DejaVu Math TeX Gyre already registered with postscriptFonts().
DejaVu Sans already registered with postscriptFonts().
DejaVu Sans Light already registered with postscriptFonts().
DejaVu Sans Condensed already registered with postscriptFonts().
DejaVu Sans Mono already registered with postscriptFonts().
DejaVu Sans Mono for Powerline already registered with postscriptFonts().
DejaVu Serif already registered with postscriptFonts().
DejaVu Serif Condensed already registered with postscriptFonts().
Droid Arabic Kufi already registered with postscriptFonts().
Droid Arabic Naskh already registered with postscriptFonts().
Droid Naskh Shift Alt already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans. Skipping setup for this font.
Droid Sans Arabic already registered with postscriptFonts().
Droid Sans Armenian already registered with postscriptFonts().
Droid Sans Devanagari already registered with postscriptFonts().
Droid Sans Ethiopic already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Fallback. Skipping setup for this font.
Droid Sans Georgian already registered with postscriptFonts().
Droid Sans Hebrew already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Sans Mono. Skipping setup for this font.
Droid Sans Mono Dotted for Powerline already registered with postscriptFonts().
Droid Sans Mono Slashed for Powerline already registered with postscriptFonts().
Droid Sans Tamil already registered with postscriptFonts().
Droid Sans Thai already registered with postscriptFonts().
More than one version of regular/bold/italic found for Droid Serif. Skipping setup for this font.
Font Awesome 5 Brands Regular already registered with postscriptFonts().
Font Awesome 5 Free Regular already registered with postscriptFonts().
Font Awesome 5 Free Solid already registered with postscriptFonts().
Gargi-1.2b already registered with postscriptFonts().
Goha-Tibeb Zemen already registered with postscriptFonts().
Go Mono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for GurbaniBoliLite. Skipping setup for this font.
Hack already registered with postscriptFonts().
Inconsolata Black already registered with postscriptFonts().
Inconsolata already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Inconsolata for Powerline. Skipping setup for this font.
Inconsolata Condensed already registered with postscriptFonts().
Inconsolata Condensed Black already registered with postscriptFonts().
Inconsolata Condensed Bold already registered with postscriptFonts().
Inconsolata Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Condensed Light already registered with postscriptFonts().
Inconsolata Condensed Medium already registered with postscriptFonts().
Inconsolata Condensed SemiBold already registered with postscriptFonts().
Inconsolata Expanded already registered with postscriptFonts().
Inconsolata Expanded Black already registered with postscriptFonts().
Inconsolata Expanded Bold already registered with postscriptFonts().
Inconsolata Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Expanded Light already registered with postscriptFonts().
Inconsolata Expanded Medium already registered with postscriptFonts().
Inconsolata Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed already registered with postscriptFonts().
Inconsolata Extra Condensed Black already registered with postscriptFonts().
Inconsolata Extra Condensed Bold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Extra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Extra Condensed Light already registered with postscriptFonts().
Inconsolata Extra Condensed Medium already registered with postscriptFonts().
Inconsolata Extra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Extra Expanded already registered with postscriptFonts().
Inconsolata Extra Expanded Black already registered with postscriptFonts().
Inconsolata Extra Expanded Bold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Extra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Extra Expanded Light already registered with postscriptFonts().
Inconsolata Extra Expanded Medium already registered with postscriptFonts().
Inconsolata Extra Expanded SemiBold already registered with postscriptFonts().
Inconsolata ExtraLight already registered with postscriptFonts().
Inconsolata Light already registered with postscriptFonts().
Inconsolata Medium already registered with postscriptFonts().
Inconsolata SemiBold already registered with postscriptFonts().
Inconsolata Semi Condensed already registered with postscriptFonts().
Inconsolata Semi Condensed Black already registered with postscriptFonts().
Inconsolata Semi Condensed Bold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Semi Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Semi Condensed Light already registered with postscriptFonts().
Inconsolata Semi Condensed Medium already registered with postscriptFonts().
Inconsolata Semi Condensed SemiBold already registered with postscriptFonts().
Inconsolata Semi Expanded already registered with postscriptFonts().
Inconsolata Semi Expanded Black already registered with postscriptFonts().
Inconsolata Semi Expanded Bold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Semi Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Semi Expanded Light already registered with postscriptFonts().
Inconsolata Semi Expanded Medium already registered with postscriptFonts().
Inconsolata Semi Expanded SemiBold already registered with postscriptFonts().
Inconsolata Ultra Condensed already registered with postscriptFonts().
Inconsolata Ultra Condensed Black already registered with postscriptFonts().
Inconsolata Ultra Condensed Bold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Condensed ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Condensed Light already registered with postscriptFonts().
Inconsolata Ultra Condensed Medium already registered with postscriptFonts().
Inconsolata Ultra Condensed SemiBold already registered with postscriptFonts().
Inconsolata Ultra Expanded already registered with postscriptFonts().
Inconsolata Ultra Expanded Black already registered with postscriptFonts().
Inconsolata Ultra Expanded Bold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraBold already registered with postscriptFonts().
Inconsolata Ultra Expanded ExtraLight already registered with postscriptFonts().
Inconsolata Ultra Expanded Light already registered with postscriptFonts().
Inconsolata Ultra Expanded Medium already registered with postscriptFonts().
Inconsolata Ultra Expanded SemiBold already registered with postscriptFonts().
Liberation Mono already registered with postscriptFonts().
Liberation Sans already registered with postscriptFonts().
Liberation Serif already registered with postscriptFonts().
Ligconsolata already registered with postscriptFonts().
Likhan already registered with postscriptFonts().
Literation Mono Powerline already registered with postscriptFonts().
malayalam already registered with postscriptFonts().
MalOtf already registered with postscriptFonts().
Meslo LG L DZ for Powerline already registered with postscriptFonts().
Meslo LG L for Powerline already registered with postscriptFonts().
Meslo LG M DZ for Powerline already registered with postscriptFonts().
Meslo LG M for Powerline already registered with postscriptFonts().
Meslo LG S DZ for Powerline already registered with postscriptFonts().
Meslo LG S for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for monofur for Powerline. Skipping setup for this font.
More than one version of regular/bold/italic found for Mukti Narrow. Skipping setup for this font.
Noto Kufi Arabic already registered with postscriptFonts().
Noto Kufi Arabic Medium already registered with postscriptFonts().
Noto Kufi Arabic Semi bold already registered with postscriptFonts().
Noto Mono for Powerline already registered with postscriptFonts().
Noto Music already registered with postscriptFonts().
Noto Naskh Arabic already registered with postscriptFonts().
Noto Naskh Arabic UI already registered with postscriptFonts().
Noto Nastaliq Urdu already registered with postscriptFonts().
Noto Sans Black already registered with postscriptFonts().
Noto Sans already registered with postscriptFonts().
Noto Sans Light already registered with postscriptFonts().
Noto Sans Medium already registered with postscriptFonts().
Noto Sans Thin already registered with postscriptFonts().
Noto Sans Adlam already registered with postscriptFonts().
Noto Sans Adlam Unjoined already registered with postscriptFonts().
Noto Sans AnatoHiero already registered with postscriptFonts().
Noto Sans Arabic Blk already registered with postscriptFonts().
Noto Sans Arabic already registered with postscriptFonts().
Noto Sans Arabic Light already registered with postscriptFonts().
Noto Sans Arabic Med already registered with postscriptFonts().
Noto Sans Arabic Thin already registered with postscriptFonts().
Noto Sans Arabic UI Bk already registered with postscriptFonts().
Noto Sans Arabic UI already registered with postscriptFonts().
Noto Sans Arabic UI Lt already registered with postscriptFonts().
Noto Sans Arabic UI Md already registered with postscriptFonts().
Noto Sans Arabic UI Th already registered with postscriptFonts().
Noto Sans Armenian Blk already registered with postscriptFonts().
Noto Sans Armenian already registered with postscriptFonts().
Noto Sans Armenian Light already registered with postscriptFonts().
Noto Sans Armenian Med already registered with postscriptFonts().
Noto Sans Armenian Thin already registered with postscriptFonts().
Noto Sans Avestan already registered with postscriptFonts().
Noto Sans Bamum already registered with postscriptFonts().
Noto Sans Bassa Vah already registered with postscriptFonts().
Noto Sans Batak already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Blk. Skipping setup for this font.
Noto Sans Bengali already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Bengali Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Bengali Thin. Skipping setup for this font.
Noto Sans Bengali UI already registered with postscriptFonts().
Noto Sans Bhaiksuki already registered with postscriptFonts().
Noto Sans Brahmi already registered with postscriptFonts().
Noto Sans Buginese already registered with postscriptFonts().
Noto Sans Buhid already registered with postscriptFonts().
Noto Sans CanAborig Bk already registered with postscriptFonts().
Noto Sans CanAborig already registered with postscriptFonts().
Noto Sans CanAborig Lt already registered with postscriptFonts().
Noto Sans CanAborig Md already registered with postscriptFonts().
Noto Sans CanAborig Th already registered with postscriptFonts().
Noto Sans Carian already registered with postscriptFonts().
Noto Sans CaucAlban already registered with postscriptFonts().
Noto Sans Chakma already registered with postscriptFonts().
Noto Sans Cham Blk already registered with postscriptFonts().
Noto Sans Cham already registered with postscriptFonts().
Noto Sans Cham Light already registered with postscriptFonts().
Noto Sans Cham Med already registered with postscriptFonts().
Noto Sans Cham Thin already registered with postscriptFonts().
Noto Sans Cherokee Blk already registered with postscriptFonts().
Noto Sans Cherokee already registered with postscriptFonts().
Noto Sans Cherokee Light already registered with postscriptFonts().
Noto Sans Cherokee Med already registered with postscriptFonts().
Noto Sans Cherokee Thin already registered with postscriptFonts().
Noto Sans Coptic already registered with postscriptFonts().
Noto Sans Cuneiform already registered with postscriptFonts().
Noto Sans Cypriot already registered with postscriptFonts().
Noto Sans Deseret already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Bk. Skipping setup for this font.
Noto Sans Devanagari already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Devanagari Lt. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Md. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Devanagari Th. Skipping setup for this font.
Noto Sans Devanagari UI already registered with postscriptFonts().
Noto Sans Display Black already registered with postscriptFonts().
Noto Sans Display already registered with postscriptFonts().
Noto Sans Display Light already registered with postscriptFonts().
Noto Sans Display Medium already registered with postscriptFonts().
Noto Sans Display Thin already registered with postscriptFonts().
Noto Sans Duployan already registered with postscriptFonts().
Noto Sans EgyptHiero already registered with postscriptFonts().
Noto Sans Elbasan already registered with postscriptFonts().
Noto Sans Ethiopic Blk already registered with postscriptFonts().
Noto Sans Ethiopic already registered with postscriptFonts().
Noto Sans Ethiopic Light already registered with postscriptFonts().
Noto Sans Ethiopic Med already registered with postscriptFonts().
Noto Sans Ethiopic Thin already registered with postscriptFonts().
Noto Sans Georgian Blk already registered with postscriptFonts().
Noto Sans Georgian already registered with postscriptFonts().
Noto Sans Georgian Light already registered with postscriptFonts().
Noto Sans Georgian Med already registered with postscriptFonts().
Noto Sans Georgian Thin already registered with postscriptFonts().
Noto Sans Glagolitic already registered with postscriptFonts().
Noto Sans Gothic already registered with postscriptFonts().
Noto Sans Grantha already registered with postscriptFonts().
Noto Sans Gujarati already registered with postscriptFonts().
Noto Sans Gujarati UI already registered with postscriptFonts().
Noto Sans Gurmukhi Black already registered with postscriptFonts().
Noto Sans Gurmukhi already registered with postscriptFonts().
Noto Sans Gurmukhi Light already registered with postscriptFonts().
Noto Sans Gurmukhi Medium already registered with postscriptFonts().
Noto Sans Gurmukhi Thin already registered with postscriptFonts().
Noto Sans Gurmukhi UI Black already registered with postscriptFonts().
Noto Sans Gurmukhi UI already registered with postscriptFonts().
Noto Sans Gurmukhi UI Light already registered with postscriptFonts().
Noto Sans Gurmukhi UI Medium already registered with postscriptFonts().
Noto Sans Gurmukhi UI Thin already registered with postscriptFonts().
Noto Sans HanifiRohg already registered with postscriptFonts().
Noto Sans Hanunoo already registered with postscriptFonts().
Noto Sans Hatran already registered with postscriptFonts().
Noto Sans Hebrew Blk already registered with postscriptFonts().
Noto Sans Hebrew already registered with postscriptFonts().
Noto Sans Hebrew Light already registered with postscriptFonts().
Noto Sans Hebrew Med already registered with postscriptFonts().
Noto Sans Hebrew Thin already registered with postscriptFonts().
Noto Sans ImpAramaic already registered with postscriptFonts().
Noto Sans Indic Siyaq Numbers already registered with postscriptFonts().
Noto Sans InsPahlavi already registered with postscriptFonts().
Noto Sans InsParthi already registered with postscriptFonts().
Noto Sans Javanese already registered with postscriptFonts().
Noto Sans Kaithi already registered with postscriptFonts().
Noto Sans Kannada Black already registered with postscriptFonts().
Noto Sans Kannada already registered with postscriptFonts().
Noto Sans Kannada Light already registered with postscriptFonts().
Noto Sans Kannada Medium already registered with postscriptFonts().
Noto Sans Kannada Thin already registered with postscriptFonts().
Noto Sans Kannada UI Black already registered with postscriptFonts().
Noto Sans Kannada UI already registered with postscriptFonts().
Noto Sans Kannada UI Light already registered with postscriptFonts().
Noto Sans Kannada UI Medium already registered with postscriptFonts().
Noto Sans Kannada UI Thin already registered with postscriptFonts().
Noto Sans Kayah Li already registered with postscriptFonts().
Noto Sans Kharoshthi already registered with postscriptFonts().
Noto Sans Khmer Black already registered with postscriptFonts().
Noto Sans Khmer already registered with postscriptFonts().
Noto Sans Khmer Light already registered with postscriptFonts().
Noto Sans Khmer Medium already registered with postscriptFonts().
Noto Sans Khmer Thin already registered with postscriptFonts().
Noto Sans Khmer UI Black already registered with postscriptFonts().
Noto Sans Khmer UI already registered with postscriptFonts().
Noto Sans Khmer UI Light already registered with postscriptFonts().
Noto Sans Khmer UI Medium already registered with postscriptFonts().
Noto Sans Khmer UI Thin already registered with postscriptFonts().
Noto Sans Khojki already registered with postscriptFonts().
Noto Sans Khudawadi already registered with postscriptFonts().
Noto Sans Lao Blk already registered with postscriptFonts().
Noto Sans Lao already registered with postscriptFonts().
Noto Sans Lao Light already registered with postscriptFonts().
Noto Sans Lao Med already registered with postscriptFonts().
Noto Sans Lao Thin already registered with postscriptFonts().
Noto Sans Lao UI Blk already registered with postscriptFonts().
Noto Sans Lao UI already registered with postscriptFonts().
Noto Sans Lao UI Light already registered with postscriptFonts().
Noto Sans Lao UI Med already registered with postscriptFonts().
Noto Sans Lao UI Thin already registered with postscriptFonts().
Noto Sans Lepcha already registered with postscriptFonts().
Noto Sans Limbu already registered with postscriptFonts().
Noto Sans Linear A already registered with postscriptFonts().
Noto Sans Linear B already registered with postscriptFonts().
Noto Sans Lisu already registered with postscriptFonts().
Noto Sans Lycian already registered with postscriptFonts().
Noto Sans Lydian already registered with postscriptFonts().
Noto Sans Mahajani already registered with postscriptFonts().
Noto Sans Malayalam Black already registered with postscriptFonts().
Noto Sans Malayalam already registered with postscriptFonts().
Noto Sans Malayalam Light already registered with postscriptFonts().
Noto Sans Malayalam Medium already registered with postscriptFonts().
Noto Sans Malayalam Thin already registered with postscriptFonts().
Noto Sans Malayalam UI Black already registered with postscriptFonts().
Noto Sans Malayalam UI already registered with postscriptFonts().
Noto Sans Malayalam UI Light already registered with postscriptFonts().
Noto Sans Malayalam UI Medium already registered with postscriptFonts().
Noto Sans Malayalam UI Thin already registered with postscriptFonts().
Noto Sans Mandaic already registered with postscriptFonts().
Noto Sans Manichaean already registered with postscriptFonts().
Noto Sans Marchen already registered with postscriptFonts().
Noto Sans Math already registered with postscriptFonts().
Noto Sans Mayan Numerals already registered with postscriptFonts().
Noto Sans MeeteiMayek already registered with postscriptFonts().
Noto Sans Mende Kikakui already registered with postscriptFonts().
Noto Sans Meroitic already registered with postscriptFonts().
Noto Sans Miao already registered with postscriptFonts().
Noto Sans Modi already registered with postscriptFonts().
Noto Sans Mongolian already registered with postscriptFonts().
Noto Sans Mono Black already registered with postscriptFonts().
Noto Sans Mono already registered with postscriptFonts().
Noto Sans Mono Light already registered with postscriptFonts().
Noto Sans Mono Medium already registered with postscriptFonts().
Noto Sans Mono Thin already registered with postscriptFonts().
Noto Sans Mro already registered with postscriptFonts().
Noto Sans Multani already registered with postscriptFonts().
Noto Sans Myanmar Blk already registered with postscriptFonts().
Noto Sans Myanmar already registered with postscriptFonts().
Noto Sans Myanmar Light already registered with postscriptFonts().
Noto Sans Myanmar Med already registered with postscriptFonts().
Noto Sans Myanmar Thin already registered with postscriptFonts().
Noto Sans Myanmar UI Black already registered with postscriptFonts().
Noto Sans Myanmar UI already registered with postscriptFonts().
Noto Sans Myanmar UI Light already registered with postscriptFonts().
Noto Sans Myanmar UI Medium already registered with postscriptFonts().
Noto Sans Myanmar UI Thin already registered with postscriptFonts().
Noto Sans Nabataean already registered with postscriptFonts().
Noto Sans Newa already registered with postscriptFonts().
Noto Sans NewTaiLue already registered with postscriptFonts().
Noto Sans N'Ko already registered with postscriptFonts().
Noto Sans Ogham already registered with postscriptFonts().
Noto Sans Ol Chiki already registered with postscriptFonts().
Noto Sans OldHung already registered with postscriptFonts().
No regular (non-bold, non-italic) version of Noto Sans Old Italic. Skipping setup for this font.
Noto Sans OldNorArab already registered with postscriptFonts().
Noto Sans Old Permic already registered with postscriptFonts().
Noto Sans OldPersian already registered with postscriptFonts().
Noto Sans OldSogdian already registered with postscriptFonts().
Noto Sans OldSouArab already registered with postscriptFonts().
Noto Sans Old Turkic already registered with postscriptFonts().
Noto Sans Oriya already registered with postscriptFonts().
Noto Sans Oriya UI already registered with postscriptFonts().
Noto Sans Osage already registered with postscriptFonts().
Noto Sans Osmanya already registered with postscriptFonts().
Noto Sans Pahawh Hmong already registered with postscriptFonts().
Noto Sans Palmyrene already registered with postscriptFonts().
Noto Sans PauCinHau already registered with postscriptFonts().
Noto Sans PhagsPa already registered with postscriptFonts().
Noto Sans Phoenician already registered with postscriptFonts().
Noto Sans PsaPahlavi already registered with postscriptFonts().
Noto Sans Rejang already registered with postscriptFonts().
Noto Sans Runic already registered with postscriptFonts().
Noto Sans Samaritan already registered with postscriptFonts().
Noto Sans Saurashtra already registered with postscriptFonts().
Noto Sans Sharada already registered with postscriptFonts().
Noto Sans Shavian already registered with postscriptFonts().
Noto Sans Siddham already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Blk. Skipping setup for this font.
Noto Sans Sinhala already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Sinhala Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Sinhala Thin. Skipping setup for this font.
Noto Sans Sinhala UI already registered with postscriptFonts().
Noto Sans SoraSomp already registered with postscriptFonts().
Noto Sans Sundanese already registered with postscriptFonts().
Noto Sans Syloti Nagri already registered with postscriptFonts().
Noto Sans Symbols Blk already registered with postscriptFonts().
Noto Sans Symbols already registered with postscriptFonts().
Noto Sans Symbols Light already registered with postscriptFonts().
Noto Sans Symbols Med already registered with postscriptFonts().
Noto Sans Symbols Thin already registered with postscriptFonts().
Noto Sans Symbols2 already registered with postscriptFonts().
Noto Sans Syriac Black already registered with postscriptFonts().
Noto Sans Syriac already registered with postscriptFonts().
Noto Sans Syriac Thin already registered with postscriptFonts().
Noto Sans Tagalog already registered with postscriptFonts().
Noto Sans Tagbanwa already registered with postscriptFonts().
Noto Sans Tai Le already registered with postscriptFonts().
Noto Sans Tai Tham already registered with postscriptFonts().
Noto Sans Tai Viet already registered with postscriptFonts().
Noto Sans Takri already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Blk. Skipping setup for this font.
Noto Sans Tamil already registered with postscriptFonts().
More than one version of regular/bold/italic found for Noto Sans Tamil Light. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Med. Skipping setup for this font.
More than one version of regular/bold/italic found for Noto Sans Tamil Thin. Skipping setup for this font.
Noto Sans Tamil Supplement already registered with postscriptFonts().
Noto Sans Tamil UI already registered with postscriptFonts().
Noto Sans Telugu Black already registered with postscriptFonts().
Noto Sans Telugu already registered with postscriptFonts().
Noto Sans Telugu Light already registered with postscriptFonts().
Noto Sans Telugu Medium already registered with postscriptFonts().
Noto Sans Telugu Thin already registered with postscriptFonts().
Noto Sans Telugu UI Black already registered with postscriptFonts().
Noto Sans Telugu UI already registered with postscriptFonts().
Noto Sans Telugu UI Light already registered with postscriptFonts().
Noto Sans Telugu UI Medium already registered with postscriptFonts().
Noto Sans Telugu UI Thin already registered with postscriptFonts().
Noto Sans Thaana Black already registered with postscriptFonts().
Noto Sans Thaana already registered with postscriptFonts().
Noto Sans Thaana Light already registered with postscriptFonts().
Noto Sans Thaana Medium already registered with postscriptFonts().
Noto Sans Thaana Thin already registered with postscriptFonts().
Noto Sans Thai Blk already registered with postscriptFonts().
Noto Sans Thai already registered with postscriptFonts().
Noto Sans Thai Light already registered with postscriptFonts().
Noto Sans Thai Med already registered with postscriptFonts().
Noto Sans Thai Thin already registered with postscriptFonts().
Noto Sans Thai UI Blk already registered with postscriptFonts().
Noto Sans Thai UI already registered with postscriptFonts().
Noto Sans Thai UI Light already registered with postscriptFonts().
Noto Sans Thai UI Med already registered with postscriptFonts().
Noto Sans Thai UI Thin already registered with postscriptFonts().
Noto Sans Tibetan already registered with postscriptFonts().
Noto Sans Tifinagh already registered with postscriptFonts().
Noto Sans Tirhuta already registered with postscriptFonts().
Noto Sans Ugaritic already registered with postscriptFonts().
Noto Sans Vai already registered with postscriptFonts().
Noto Sans WarangCiti already registered with postscriptFonts().
Noto Sans Yi already registered with postscriptFonts().
Noto Serif Black already registered with postscriptFonts().
Noto Serif already registered with postscriptFonts().
Noto Serif Light already registered with postscriptFonts().
Noto Serif Medium already registered with postscriptFonts().
Noto Serif Thin already registered with postscriptFonts().
Noto Serif Ahom already registered with postscriptFonts().
Noto Serif Armenian Bk already registered with postscriptFonts().
Noto Serif Armenian already registered with postscriptFonts().
Noto Serif Armenian Lt already registered with postscriptFonts().
Noto Serif Armenian Md already registered with postscriptFonts().
Noto Serif Armenian Th already registered with postscriptFonts().
Noto Serif Balinese already registered with postscriptFonts().
Noto Serif Bengali Black already registered with postscriptFonts().
Noto Serif Bengali already registered with postscriptFonts().
Noto Serif Bengali Light already registered with postscriptFonts().
Noto Serif Bengali Medium already registered with postscriptFonts().
Noto Serif Bengali Thin already registered with postscriptFonts().
Noto Serif Devanagari Black already registered with postscriptFonts().
Noto Serif Devanagari already registered with postscriptFonts().
Noto Serif Devanagari Light already registered with postscriptFonts().
Noto Serif Devanagari Medium already registered with postscriptFonts().
Noto Serif Devanagari Thin already registered with postscriptFonts().
Noto Serif Display Black already registered with postscriptFonts().
Noto Serif Display already registered with postscriptFonts().
Noto Serif Display Light already registered with postscriptFonts().
Noto Serif Display Medium already registered with postscriptFonts().
Noto Serif Display Thin already registered with postscriptFonts().
Noto Serif Dogra already registered with postscriptFonts().
Noto Serif Ethiopic Bk already registered with postscriptFonts().
Noto Serif Ethiopic already registered with postscriptFonts().
Noto Serif Ethiopic Lt already registered with postscriptFonts().
Noto Serif Ethiopic Md already registered with postscriptFonts().
Noto Serif Ethiopic Th already registered with postscriptFonts().
Noto Serif Georgian Bk already registered with postscriptFonts().
Noto Serif Georgian already registered with postscriptFonts().
Noto Serif Georgian Lt already registered with postscriptFonts().
Noto Serif Georgian Md already registered with postscriptFonts().
Noto Serif Georgian Th already registered with postscriptFonts().
Noto Serif Gujarati Black already registered with postscriptFonts().
Noto Serif Gujarati already registered with postscriptFonts().
Noto Serif Gujarati Light already registered with postscriptFonts().
Noto Serif Gujarati Medium already registered with postscriptFonts().
Noto Serif Gujarati Thin already registered with postscriptFonts().
Noto Serif Gurmukhi Black already registered with postscriptFonts().
Noto Serif Gurmukhi already registered with postscriptFonts().
Noto Serif Gurmukhi Light already registered with postscriptFonts().
Noto Serif Gurmukhi Medium already registered with postscriptFonts().
Noto Serif Gurmukhi Thin already registered with postscriptFonts().
Noto Serif Hebrew Blk already registered with postscriptFonts().
Noto Serif Hebrew already registered with postscriptFonts().
Noto Serif Hebrew Light already registered with postscriptFonts().
Noto Serif Hebrew Med already registered with postscriptFonts().
Noto Serif Hebrew Thin already registered with postscriptFonts().
Noto Serif Kannada Black already registered with postscriptFonts().
Noto Serif Kannada already registered with postscriptFonts().
Noto Serif Kannada Light already registered with postscriptFonts().
Noto Serif Kannada Medium already registered with postscriptFonts().
Noto Serif Kannada Thin already registered with postscriptFonts().
Noto Serif Khmer Black already registered with postscriptFonts().
Noto Serif Khmer already registered with postscriptFonts().
Noto Serif Khmer Light already registered with postscriptFonts().
Noto Serif Khmer Medium already registered with postscriptFonts().
Noto Serif Khmer Thin already registered with postscriptFonts().
Noto Serif Lao Blk already registered with postscriptFonts().
Noto Serif Lao already registered with postscriptFonts().
Noto Serif Lao Light already registered with postscriptFonts().
Noto Serif Lao Med already registered with postscriptFonts().
Noto Serif Lao Thin already registered with postscriptFonts().
Noto Serif Malayalam Black already registered with postscriptFonts().
Noto Serif Malayalam already registered with postscriptFonts().
Noto Serif Malayalam Light already registered with postscriptFonts().
Noto Serif Malayalam Medium already registered with postscriptFonts().
Noto Serif Malayalam Thin already registered with postscriptFonts().
Noto Serif Myanmar Blk already registered with postscriptFonts().
Noto Serif Myanmar already registered with postscriptFonts().
Noto Serif Myanmar Light already registered with postscriptFonts().
Noto Serif Myanmar Med already registered with postscriptFonts().
Noto Serif Myanmar Thin already registered with postscriptFonts().
Noto Serif Sinhala Black already registered with postscriptFonts().
Noto Serif Sinhala already registered with postscriptFonts().
Noto Serif Sinhala Light already registered with postscriptFonts().
Noto Serif Sinhala Medium already registered with postscriptFonts().
Noto Serif Sinhala Thin already registered with postscriptFonts().
Noto Serif Tamil Blk already registered with postscriptFonts().
Noto Serif Tamil already registered with postscriptFonts().
Noto Serif Tamil Light already registered with postscriptFonts().
Noto Serif Tamil Med already registered with postscriptFonts().
Noto Serif Tamil Thin already registered with postscriptFonts().
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Black. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Light. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Medium. Skipping setup for this font.
No regular (non-bold, non-italic) version of NotoSerifTamilSlanted Thin. Skipping setup for this font.
Noto Serif Tangut already registered with postscriptFonts().
Noto Serif Telugu Black already registered with postscriptFonts().
Noto Serif Telugu already registered with postscriptFonts().
Noto Serif Telugu Light already registered with postscriptFonts().
Noto Serif Telugu Medium already registered with postscriptFonts().
Noto Serif Telugu Thin already registered with postscriptFonts().
Noto Serif Thai Blk already registered with postscriptFonts().
Noto Serif Thai already registered with postscriptFonts().
Noto Serif Thai Light already registered with postscriptFonts().
Noto Serif Thai Med already registered with postscriptFonts().
Noto Serif Thai Thin already registered with postscriptFonts().
Noto Serif Tibetan Black already registered with postscriptFonts().
Noto Serif Tibetan already registered with postscriptFonts().
Noto Serif Tibetan Light already registered with postscriptFonts().
Noto Serif Tibetan Medium already registered with postscriptFonts().
Noto Serif Tibetan Thin already registered with postscriptFonts().
NovaMono for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Nunito. Skipping setup for this font.
orya already registered with postscriptFonts().
More than one version of regular/bold/italic found for padmaa. Skipping setup for this font.
Pothana2000 already registered with postscriptFonts().
ProFont for Powerline already registered with postscriptFonts().
More than one version of regular/bold/italic found for Roboto. Skipping setup for this font.
More than one version of regular/bold/italic found for Roboto Condensed. Skipping setup for this font.
Roboto Mono for Powerline already registered with postscriptFonts().
Roboto Mono Light for Powerline already registered with postscriptFonts().
Roboto Mono Medium for Powerline already registered with postscriptFonts().
Roboto Mono Thin for Powerline already registered with postscriptFonts().
Sagar already registered with postscriptFonts().
Space Mono already registered with postscriptFonts().
Space Mono for Powerline already registered with postscriptFonts().
Symbol Neu for Powerline already registered with postscriptFonts().
TAMu_Kadambri already registered with postscriptFonts().
TAMu_Kalyani already registered with postscriptFonts().
TAMu_Maduram already registered with postscriptFonts().
Tinos for Powerline already registered with postscriptFonts().
TSCu_Comic already registered with postscriptFonts().
TSCu_Paranar already registered with postscriptFonts().
TSCu_Times already registered with postscriptFonts().
Ubuntu already registered with postscriptFonts().
Ubuntu Light already registered with postscriptFonts().
Ubuntu Condensed already registered with postscriptFonts().
Ubuntu Mono already registered with postscriptFonts().
Ubuntu Mono derivative Powerline already registered with postscriptFonts().
#+end_example

#+begin_SRC R :results output :session *R* :eval no-export :exports results
str(df_search_spaces)
#+end_SRC

#+RESULTS:
: 'data.frame':	69 obs. of  8 variables:
:  $ name                   : chr  "atax" "dgemv3" "fdtd4d2d" "gemver" ...
:  $ year                   : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...
:  $ dimension              : int  19 49 30 24 11 15 14 12 20 25 ...
:  $ search_space_size      : num  1.65e+14 2.73e+30 7.06e+24 7.26e+17 1.56e+08 ...
:  $ log10_search_space_size: int  14 30 24 17 8 8 12 8 16 19 ...
:  $ domain                 : chr  "Linear Algebra" "Linear Algebra" "Linear Algebra" "Linear Algebra" ...
:  $ author                 : chr  "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" "Balaprakash, P. et al. (2012)" ...
:  $ gscholar_citation      : chr  "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" "balaprakash2012spapt" ...
**** Generate Caption
#+begin_SRC R :results output :session *R* :eval no-export :exports results
citations <- unique(df_search_spaces$gscholar_citation)
citations <- paste(citations[citations != ""], collapse = ",")
cat(paste("\\nbsp{}\\cite{", citations, "}", sep = ""))
#+end_SRC

#+RESULTS:
:
: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}

**** Plots
#+begin_SRC R :results graphics output :session *R* :file "./img/search_spaces.pdf" :width 18 :height 8.7 :eval no-export
library(ggplot2)
library(dplyr)
library(scales)
library(RColorBrewer)
library(ggrepel)
library(patchwork)

point_alpha = 1.0
point_size = 3
label_size = 6

shapes = c(15, 16, 17, 18, 6, 7, 9, 0, 3, 5, 12, 14, 13, 11)

legend_rows = length(unique(df_search_spaces$domain)) / 2
legend_position = c(0.66, 0.12)

base_size = 25
font_family = "Liberation Sans"

color_palette = colorRampPalette(brewer.pal(9,
                                            "Set1"))(
                                                length(
                                                    unique(
                                                        df_search_spaces$domain)))

x_text = element_text(size = 26)
y_text = element_text(size = 26)

x_label = element_text(size = 28)
y_label = element_text(size = 28)

scientific_10 <- function(x) {
    print(x)
    result <- parse(text = gsub("(.*)",
                                "10^\\1",
                                format(x)))
    print(result)
    return(result)
}

p1 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_point(alpha = point_alpha,
               size = point_size,
               show.legend = FALSE) +
    geom_text_repel(data = filter(df_search_spaces,
                                  dimension > 40 &
                                  log10_search_space_size >= 30),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    xlim = c(110, 120),
                    size = label_size,
                    show.legend = FALSE) +
    xlab("Dimension") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(text = element_text(family = font_family),
          axis.text.x = x_text,
          axis.text.y = y_text,
          axis.title.y = element_blank())

p2 <- ggplot(data = df_search_spaces,
             aes(x = dimension,
                 y = log10_search_space_size,
                 color = domain,
                 shape = domain)) +
    geom_point(alpha = point_alpha,
               size = point_size) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 40 &
                               dimension < 60 &
                               log10_search_space_size >= 30,
                               log10_search_space_size < 50),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(30, 50),
                    xlim = c(NA, 45),
                    nudge_x = -3,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension < 20 &
                               log10_search_space_size > 19),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(20, 50),
                    xlim = c(0, 25),
                    nudge_y = 1.6,
                    size = label_size,
                    show.legend = FALSE) +
    geom_text_repel(data = df_search_spaces %>%
                        filter(dimension > 30 &
                               log10_search_space_size < 25),
                    aes(x = dimension,
                        y = log10_search_space_size,
                        label = paste(author, name),
                        color = domain),
                    ylim = c(15, 50),
                    xlim = c(30, NA),
                    nudge_y = 2,
                    size = label_size,
                    show.legend = FALSE) +
    xlim(0, 60) +
    xlab("Dimension") +
    ylab("Search Space Size") +
    scale_color_manual(name = element_blank(),
                       values = color_palette) +
    scale_shape_manual(name = element_blank(),
                       values = shapes) +
    scale_y_continuous(limits = c(1, 50), label = scientific_10) +
    theme_bw(base_size = base_size) +
    theme(axis.text.x = x_text,
          axis.text.y = y_text,
          legend.position = legend_position,
          legend.direction = "horizontal",
          legend.spacing.x = unit(0.0, 'cm'),
          legend.spacing.y = unit(0.0, 'cm'),
          legend.background = element_rect(fill = "transparent", color = NA),
          legend.text = element_text(size = 15),
          text = element_text(family = font_family)) +
    guides(color = guide_legend(nrow = legend_rows,
                                override.aes = list(alpha = 1.0,
                                                    size = 3)))

p2 * p1
#+end_SRC

#+RESULTS:
[[file:./img/search_spaces.pdf]]
* The Need for Autotuning in High Performance Computing
High Performance Computing  has been a cornerstone of  scientific and industrial
progress for at least five decades.  By paying the cost of increased complexity,
software  and  hardware  engineering   advances  continue  to  overcome  several
challenges on the way of  the sustained performance improvements observed during
the  last fifty  years.   A  consequence of  this  mounting  complexity is  that
reaching the theoretical peak hardware  performance for a given program requires
not only expert  knowledge of specific hardware architectures,  but also mastery
of programming models and languages for parallel and distributed computing.

If we state performance optimization problems as /search/ or /learning/ problems, by
converting implementation  and configuration  choices to /parameters/  which might
affect  performance,  we  can  draw   and  adapt  proven  methods  from  search,
mathematical optimization, and /statistical  learning/. The effectiveness of these
adapted methods on performance optimization  problems varies greatly, and hinges
on practical  and mathematical properties  of the problem and  the corresponding
/search  space/. The  application of  such methods  to the  automation of  program
optimization  for  specific hardware,  under  a  set  of /constraints/,  is  named
/autotuning/.

Improving performance  also relies on gathering  application-specific knowledge,
which requires extensive experimental costs  since, with the exception of linear
algebra  routines,  theoretical  peak  performance is  not  always  a  reachable
comparison  baseline.  When  adapting  methods  for  autotuning,  we  must  face
challenges emerging from  practical properties such as restricted  time and cost
budgets,  constraints  on  feasible  parameter  values,  and  the  need  to  mix
/categorical/, /continuous/, and /discrete/ parameters.  To achieve useful results, we
must also  choose methods  that make hypotheses  compatible with  problem search
spaces,  such  as  the  existence  of /discoverable/,  or  at  least  /exploitable/,
relationships between parameters and performance.  Choosing an autotuning method
requires determining  a balance between  the exploration  of a problem,  when we
would  seek  to  discover  and  explain  relationships  between  parameters  and
performance, and the exploitation of the best optimizations we can find, when we
would seek only to minimize performance.

The contributions of  this thesis are strategies to apply  to program autotuning
the       statistical       learning       methods      of       /Design       of
Experiments/\nbsp{}\cite{montgomery2017design},   or  /Experimental   Design/,  and
/Gaussian  Process   Regression/\nbsp{}\cite{williams2006gaussian}.   This  thesis
presents background and a high-level view of the theoretical foundations of each
method, and detailed discussions of  the challenges involved in specializing the
general definitions  of search  heuristics and  statistical learning  methods to
different autotuning  problems, as well  as what  can be /learned/  about specific
autotuning search spaces,  and how that acquired knowledge can  be leveraged for
further optimization.

This  chapter aims  to substantiate  the claim  that autotuning  methods have  a
fundamental  role to  play on  the future  of program  performance optimization,
arguing  that the  value and  the difficulty  of the  efforts to  carefully tune
software became more apparent ever since advances in hardware stopped leading to
effortless,   at   least   from  the   programmer's   perspective,   performance
improvements.  The  following sections  discuss the  historical context  for the
changes in trends on computer  architecture, and characterizes the search spaces
found when optimizing performance on different domains.

** Historical Trends in Hardware Design
The physical  constraints imposed  by technological  advances on  circuit design
were evident since  the first vacuum tube computers that  already spanned entire
floors,  such   as  the  ENIAC  in   1945\nbsp{}\cite{ceruzzi2003history}.   The
practical and  economical need to fit  more computing power into  real estate is
one  force for  innovation in  hardware design  that spans  its history,  and is
echoed in  modern supercomputers,  such as  the /Summit/  from /Oak  Ridge National
Laboratory/\nbsp{}\cite{olcf2020summit}, which also spans entire floors.

Figure\nbsp{}[[fig:trends]] highlights the unrelenting and so far successful pursuit
of smaller transistor fabrication processes, and the resulting capability to fit
more computing power on  a fixed chip area.  This trend  was already observed in
integrated      circuits      by      Gordon      Moore      /et      al./      in
1965\nbsp{}\cite{moore1965cramming},  who also  postulated its  continuity.  The
performance improvements produced by the design efforts to make Moore's forecast
a self-fulfilling  prophecy were  boosted until around  2005 by  the performance
gained from increases in circuit frequency.

#+NAME: fig:trends
#+ATTR_LATEX: :width \textwidth
#+CAPTION: 49 years of microprocessor data, highlighting the sustained exponential
#+CAPTION: increases and reductions on transistor counts and fabrication processes, the
#+CAPTION: stagnation of frequency scaling around 2005, and one solution found for it,
#+CAPTION: the simultaneous exponential increase on logical core count.
#+CAPTION: Data from Wikipedia\nbsp{}\cite{wiki2020transistor,wiki2020chronology}
[[file:img/49_years_processor_data.pdf]]

Robert  Dennard  /et  al./ remarked  in  1974\nbsp{}\cite{dennard1974design}  that
smaller  transistors, in  part  because they  generate  shorter circuit  delays,
decrease  the energy  required to  power  a circuit  and enable  an increase  in
operation  frequency without  breaking  power usage  constraints.  This  scaling
effect,  named /Dennard's  scaling/,  is hindered  primarily  by leakage  current,
caused    by     quantum    tunneling    effects    in     small    transistors.
Figure\nbsp{}[[fig:trends]] shows  a marked  stagnation on frequency  increase after
around 2005,  as transistors crossed  the 10^{2}nm fabrication process.   It was
expected that leakage  due to tunneling would limit  frequency scaling strongly,
even     before      the     transistor     fabrication      process     reached
10nm\nbsp{}\cite{frank2001device}.

Current hardware is now past the  effects of Dennard's scaling.  The increase in
logical cores,  still around  2015, can  be interpreted  as preparation  for and
mitigation of the end  of frequency scaling, and ushered in  an age of multicore
scaling.  Still, in order to meet power consumption constraints, up to half of a
multicore processor could have to be  powered down at all times. This phenomenon
is  named  /Dark  Silicon/\nbsp{}\cite{esmaeilzadeh2011dark}, and  still  presents
significant       challenges        to       hardware        designers       and
programmers\nbsp{}\cite{venkataramani2015approximate,cheng2015core,henkel2015new}.

#+NAME: fig:rmax-rpeak
#+CAPTION: Theoretical and achieved performance for the top-ranked supercomputer
#+CAPTION: on the TOP500 list (ref)
#+ATTR_LATEX: :width \textwidth
[[file:./img/top500_rmax_rpeak.pdf]]

#+latex: \todo[inline]{Citation and discussion for RMax/RPeak figure}

Advances in hardware  design are currently not capable  of providing performance
improvements  via frequency  scaling  without dissipating  more  power than  the
processor  was  designed to  support,  risking  damages. From  the  programmer's
perspective,  effortless  performance  improvements  over  time  have  not  been
expected for  quite some time,  and the key  to sustaining historical  trends in
performance  scaling lies  in  hardware  accelerators such  as  GPUs and  FPGAs,
parallel  and distributed  programming  libraries, and  fine  tuning of  several
stages of the software stack, from instruction selection to the layout of neural
networks.

#+latex: \todo[inline]{Hook paragraph for next section}

** Characterizing Search Spaces from Autotuning Problems

#+ATTR_LATEX: :width \textwidth
#+CAPTION: Dimension and search space size for autotuning problems from different domains
#+CAPTION: \nbsp{}\cite{balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison}.
#+CAPTION: The left panel shows a zoomed view of the right panel
[[file:img/search_spaces.pdf]]

#+ATTR_LATEX: :width .7\textwidth
#+CAPTION: A search space defined for loop blocking and unrolling
#+CAPTION: parameters, in a matrix multiplication kernel\nbsp{}\cite{seymour2008comparison}
[[file:img/seymour2008comparison.pdf]]

** Thesis Contributions

** Text Structure

Chapter\nbsp{}[[Search and Statistical Learning for Autotuning]] presents background
for  the application  of methods  derived from  search heuristics,  mathematical
optimization, and statistical learning to autotuning.
** Backup Text                                                    :noexport:
The    effectiveness   of    search    heuristics   on    autotuning   can    be
limited\nbsp{}\cite{seymour2008comparison,balaprakash2011can,balaprakash2012experimental},
between other factors, by underlying hypotheses  about the search space, such as
the  reachability of  the  global optimum  and the  smoothness  of search  space
surfaces, which  are frequently not  respected. The derivation  of relationships
between  parameters  and  performance  from search  heuristic  optimizations  is
greatly hindered,  if not rendered impossible,  by the biased way  these methods
explore  parameters.   Some  parametric  learning methods,  such  as  Design  of
Experiments,  are  not widely  applied  to  autotuning.  These  methods  perform
structured  parameter  exploration,  and  can  be used  to  build  and  validate
performance     models,     generating    transparent     and     cost-effective
optimizations\nbsp{}\cite{mametjanov2015autotuning,bruel2019autotuning}. Other methods
from   the  parametric   family   are   more  widely   used,   such  as   Bandit
Algorithms\nbsp{}\cite{xu2017parallel}.   Nonparametric  learning   methods,  such  as
Decision    Trees\nbsp{}\cite{balaprakash2016automomml}     and    Gaussian    Process
Regression\nbsp{}\cite{parsa2019pabo}, are able  to reduce model bias  greatly, at the
expense of increased prediction variance. Figure\nbsp{}\ref{fig:tree} categorizes some
autotuning  methods  according to  some  of  the  key hypotheses  and  branching
questions underlying each method.

* Reproducible Science
* Search and Statistical Learning for Autotuning
* Design of Experiments
* Gaussian Process Regression
** References
#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
