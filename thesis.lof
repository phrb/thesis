\babel@toc {english}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces 49 years of microprocessor data, highlighting the sustained exponential increases and reductions on transistor counts and fabrication processes, the stagnation of frequency scaling around 2005, and one solution found for it, the simultaneous exponential increase on logical core count. Data from Wikipedia~\cite {wiki2020transistor,wiki2020chronology}\relax }}{23}{figure.caption.5}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Sustained exponential increase of theoretical \textit {RPeak} and achieved \textit {RMax} performance for the supercomputer ranked 1\textsuperscript {st} on TOP500~\cite {top5002020list}\relax }}{25}{figure.caption.6}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Processor and accelerator core count in supercomputers ranked 1\textsuperscript {st} on TOP500~\cite {top5002020list}. Core count trends for supercomputers are not necessarily bound to processor trends observed on Figure~\ref {fig:trends}.\relax }}{25}{figure.caption.7}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Loop nest optimizations for $C = C + A + B^{\top }$, in C\relax }}{27}{figure.caption.8}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Access patterns for matrices in \(C = C + A + B^{\top }\), with loop nest optimizations. Panel \textit {(a)} shows the access order of a regular implementation, and panel \textit {(b)} shows the effect of loop \textit {tiling}, or \textit {blocking}\relax }}{27}{figure.caption.9}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Loop nest optimizations for GEMM, in C\relax }}{28}{figure.caption.10}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces An exhaustively measured search space, defined by loop blocking and unrolling parameters, for a sequential GEMM kernel. Reproduced from Seymour \textit {et al.}~\cite {seymour2008comparison}\relax }}{29}{figure.caption.11}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Dimension and search space size for autotuning problems from 14 domains ~\cite {balaprakash2012spapt,ansel2014opentuner,byun2012autotuning,petrovivc2020benchmark,balaprakash2018deephyper,bruel2019autotuning,bruel2015autotuning,bruel2017autotuning,mametjanov2015autotuning,abdelfattah2016performance,xu2017parallel,tiwari2009scalable,hutter2009paramils,chu2020improving,tuzov2018tuning,ziegler2019syntunsys,gerndt2018multi,kwon2019learning,wang2019funcytuner,olha2019exploiting,seymour2008comparison} The left panel shows a zoomed view of the right panel\relax }}{31}{figure.caption.12}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Contour plots and slices through the global optimum, marked with a \(\color {red}\bm {\times }\), for search spaces defined by variations of the Booth function. Panels (a) and (d) correspond to Equation~\ref {eq:f0}, panels (b) and (e) to Equation~\ref {eq:f1}, and panels (c) and (f) to Equation~\ref {eq:f2}.\relax }}{38}{figure.caption.16}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Illustrating the relationship between convexity of a function over a compact set and the presence of local minima. Panels (a) and (b) match the functions on the same panels of Figure~\ref {fig:simple-search-spaces}\relax }}{39}{figure.caption.17}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Contour plots and direction of greatest descent \(-\nabla {}f(\mathbf {x})\), for search spaces defined by variations of the Booth function. Panels (a), (b), and (c) correspond to Equations~\ref {eq:f0}, \ref {eq:f1}, and \ref {eq:f2} respectively. The global optimum is marked with a \(\color {red}\bm {\times }\). To aid visualization, vector magnitude was encoded by color intensity, so that darker vectors have larger magnitude. The gradient along the function's basin is near zero.\relax }}{42}{figure.caption.18}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Representation of paths taken by the gradient descent method, with adaptive choice of \(\alpha _k\), on the search spaces defined by variations of the Booth function. Panels (a), (b), and (c) correspond to Equations~\ref {eq:f0}, \ref {eq:f1}, and \ref {eq:f2} respectively. Contour plots and direction of greatest descent \(-\nabla {}f(\mathbf {x})\) are also shown, and the global optimum is marked with a \(\color {red}\bm {\times }\).\relax }}{43}{figure.caption.19}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Representation of paths taken by the gradient descent method with 4 restarts, with adaptive choice of \(\alpha _k\), on the search spaces defined by variations of the Booth function. Panels (a), (b), and (c) correspond to Equations~\ref {eq:f0}, \ref {eq:f1}, and \ref {eq:f2} respectively. Contour plots and direction of greatest descent \(-\nabla {}f(\mathbf {x})\) are also shown, and the global optimum is marked with a \(\color {red}\bm {\times }\).\relax }}{44}{figure.caption.20}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces An exhaustively measured search space, defined by loop blocking and unrolling parameters, for a sequential GEMM kernel. Reproduced from Seymour \textit {et al.}~\cite {seymour2008comparison}\relax }}{45}{figure.caption.21}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Representation of paths taken by the Simulated Annealing method on the search spaces defined by variations of the Booth function. Panels (a), (b), and (c) correspond to Equations~\ref {eq:f0}, \ref {eq:f1}, and \ref {eq:f2} respectively. Contour plots and direction of greatest descent are also shown, and the global optimum is marked with a \(\color {red}\bm {\times }\).\relax }}{48}{figure.caption.22}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Some ways of producing offspring from two parents with binary chromosomes. Crossover splits parent chromosomes and combine the resulting pieces. In general, pieces from multiple splits can be combined. Mutations are introduced randomly and correspond to flipping bits on binary chromosomes.\relax }}{49}{figure.caption.23}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Color-coded generation snapshots of a Genetic Algorithm on the search spaces defined by variations of the Booth function. Hollow points and dashed lines mark members of previous populations and the regions they covered, while filled points and complete lines mark the final population. Panels (a), (b), and (c) correspond to Equations~\ref {eq:f0}, \ref {eq:f1}, and \ref {eq:f2} respectively. Contour plots and direction of greatest descent are also shown, and the global optimum is marked with a \(\color {red}\bm {\times }\).\relax }}{50}{figure.caption.24}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Representation of paths taken by the gradient descent method, with adaptive choice of \(\alpha _k\), on the search spaces defined by variations of the Booth function. Panel groups (a,d,g), (b,e,h), and (c,f,i) correspond to Equations~\ref {eq:f0}, \ref {eq:f1}, and \ref {eq:f2} respectively. Panel groups (a,b,c), (d,e,f), and (g,h,i) correspond to Gradient Descent with restarts, Simulated Annealing, and a Genetic Algorithm, respectively. Contour plots and direction of greatest descent \(-\nabla {}f(\mathbf {x})\) are also shown, and the global optimum is marked with a \(\color {red}\bm {\times }\).\relax }}{52}{figure.caption.25}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces In Linear Regression, the prediction vector \(\hat {\mathbf {y}}\) is the orthogonal projection of the observations vector \(\mathbf {y}\) into the vector space spanned by the columns of \(\mathbf {X}\).\relax }}{55}{figure.caption.26}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Three linear model surrogates for the Booth function, with a \(\color {red}\bm {\times }\) marking the global optimum and best surrogate predictions. The fixed experimental design \(\mathbf {X}\) used to fit all surrogates is marked by \(\color {blue}\bm {\times }\) s. Panel (a) shows noisy measurements of Booth's function, panels (b), (c), and (d) show surrogate predictions for models fit with basis functions sets from Equations~\ref {eq:hb}, \ref {eq:hc}, and \ref {eq:hd}, respectively\relax }}{57}{figure.caption.27}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Effects of three covariance matrices on a multivariate normal distribution with mean vector \(\bm {\mu } = [0\tmspace +\thickmuskip {.2777em} 0]^{\top }\) and covariance matrix \(\bm {\Sigma }\) as shown on the upper right corner of each plot\relax }}{60}{figure.caption.28}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Reinterpreting the unrolled dimensions of 100 samples of a 20 dimension multivariate normal, on the left panel, to obtain 100 samples of functions evaluated on 20 different input points, on the right panel\relax }}{61}{figure.caption.29}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Covariance of points \((\mathbf {x}, \mathbf {x}^{\prime })\) according to four covariance functions based on the distance \(\lVert {}\mathbf {x} - \mathbf {x}^{\prime }\rVert {}\). Expressions for each kernel are shown in Table~\ref {tab:kernel-expressions}, and Matérn kernels use parameters \(v_1,v_2 = \{\frac {3}{2},\frac {5}{2}\}\)\relax }}{63}{figure.caption.31}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Fitting a Gaussian Process to three noise-free observations. The left panel shows 300 samples from a Gaussian prior, using the Matérn kernel to compute the covariance matrix. The center and right panels show 300 samples from the posterior distributions conditioned by one, then two more, successive noise-free observations\relax }}{64}{figure.caption.32}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Fitting a Gaussian Process to three noisy observations, in the same conditions and with the same panel structure in Figure~\ref {fig:matern52-50d-fitting}\relax }}{64}{figure.caption.33}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Gaussian Process surrogates using three different model trends, fit to six noise-free observations of a single-input objective function, marked with black circles\relax }}{66}{figure.caption.34}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Three Gaussian Process Regression surrogates with noisy fits for the Booth function, with a \(\color {red}\bm {\times }\) marking the global optimum and best surrogate predictions. The fixed experimental design \(\mathbf {X}\) used to fit all surrogates is marked by \(\color {blue}\bm {\times }\) s. Panel (a) shows noisy measurements of Booth's function, panels (b), (c), and (d) show surrogate predictions for models fit with linear, quadratic, and quadratic plus interactions trends, respectively\relax }}{67}{figure.caption.35}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Exploration of the search space, using a fixed budget of 50 points. The red ``\(+\)'' represents the best point found by each strategy, and ``\(\times \)''s denote neighborhood exploration\relax }}{77}{figure.caption.51}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {8.1}{\ignorespaces Some autotuning methods\relax }}{80}{figure.caption.55}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {10.1}{\ignorespaces Autotuner representation and time scale of the experiments\relax }}{84}{figure.caption.56}%
\contentsline {figure}{\numberline {10.2}{\ignorespaces Simplified view of NVCC compilation\relax }}{86}{figure.caption.57}%
\contentsline {figure}{\numberline {10.3}{\ignorespaces Mean speedup over 30 repetitions of the tuned solutions for Rodinia kernels versus the \textit {opt-level} \(=2\) baseline\relax }}{89}{figure.caption.61}%
\contentsline {figure}{\numberline {10.4}{\ignorespaces Mean speedup over 30 repetitions of the tuned solutions for kernels we implemented versus the \textit {opt-level} \(=2\) baseline\relax }}{90}{figure.caption.62}%
\contentsline {figure}{\numberline {10.5}{\ignorespaces Mean speedup over 30 repetitions of the tuned solutions for Rodinia kernels versus the \textit {opt-level} \(=2\) baseline, across two hours of tuning. Notice the difference in the \textit {y}-axis scales for each panel\relax }}{91}{figure.caption.63}%
\contentsline {figure}{\numberline {10.6}{\ignorespaces Mean speedup over 30 repetitions of the tuned solutions for the kernels we implemented versus the \textit {opt-level} \(=2\) baseline, across two hours of tuning. Notice the difference in the \textit {y}-axis scales for each panel\relax }}{91}{figure.caption.64}%
\contentsline {figure}{\numberline {10.7}{\ignorespaces Main effects estimates and 95\% confidence intervals for the \textit {high level} of the 15 factors in the Plackett-Burman design, for target kernels measured on the Titan X and Quadro M1200 GPUs\relax }}{94}{figure.caption.68}%
\contentsline {figure}{\numberline {10.8}{\ignorespaces Execution time of baseline and model-predicted NVCC flag configurations for three Rodinia kernels on the Titan X and Quadro M1200 GPUs. Circles mark each of the 20 measurements of each flag configuration, filled dots and whiskers mark the mean and 95\% confidence intervals\relax }}{95}{figure.caption.69}%
\contentsline {figure}{\numberline {10.9}{\ignorespaces Mean speedups found using the predictions of the screening model, for three Rodinia kernels on the Titan X and Quadro M1200 GPUs. The horizontal dashed lines mark the PTX-stage \texttt {opt-level=3} comparison baseline\relax }}{96}{figure.caption.70}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {11.1}{\ignorespaces Autotuner representation and time scale of the experiments\relax }}{98}{figure.caption.71}%
\contentsline {figure}{\numberline {11.2}{\ignorespaces Autotuner representation and time scale of more complex FPGA applications\relax }}{98}{figure.caption.72}%
\contentsline {figure}{\numberline {11.3}{\ignorespaces High-Level Synthesis compilation process. The autotuner search space at the HLS stage is highlighted in blue\relax }}{100}{figure.caption.73}%
\contentsline {figure}{\numberline {11.4}{\ignorespaces Autotuner Setup\relax }}{100}{figure.caption.74}%
\contentsline {figure}{\numberline {11.5}{\ignorespaces Comparison of the absolute values for Random and Default starting points in the Balanced scenario\relax }}{104}{figure.caption.78}%
\contentsline {figure}{\numberline {11.6}{\ignorespaces Relative improvement for all metrics in the \textit {Balanced} scenario\relax }}{105}{figure.caption.79}%
\contentsline {figure}{\numberline {11.7}{\ignorespaces Relative improvement for all metrics in the \textit {Area} scenario\relax }}{105}{figure.caption.79}%
\contentsline {figure}{\numberline {11.8}{\ignorespaces Relative improvement for all metrics in the \textit {Performance} scenario\relax }}{106}{figure.caption.80}%
\contentsline {figure}{\numberline {11.9}{\ignorespaces Relative improvement for all metrics in the \textit {Performance and Latency} scenario\relax }}{106}{figure.caption.80}%
\contentsline {figure}{\numberline {11.10}{\ignorespaces Relative improvement for \textbf {WNS} in all scenarios\relax }}{106}{figure.caption.81}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {12.1}{\ignorespaces Edge-detection effect of a \emph {Laplacian of Gaussian} filter\relax }}{109}{figure.caption.82}%
\contentsline {figure}{\numberline {12.2}{\ignorespaces A CPU Laplacian kernel written in C\relax }}{110}{figure.caption.83}%
\contentsline {figure}{\numberline {12.3}{\ignorespaces Excerpts of the BOAST code, in Ruby, that generates the Laplacian OpenCL kernel. The code is publicly hosted at GitHub~\cite {boast2021laplacian}\relax }}{111}{figure.caption.84}%
\contentsline {figure}{\numberline {12.4}{\ignorespaces A sample OpenCL Laplacian kernel generated by BOAST\relax }}{112}{figure.caption.85}%
\contentsline {figure}{\numberline {12.5}{\ignorespaces Overview of the ED approach to autotuning we implemented\relax }}{113}{figure.caption.86}%
\contentsline {figure}{\numberline {12.6}{\ignorespaces Overview of a single DLMT run. Each panel shows a view of the completely evaluated search space, from the perspective of one of the factors eliminated in a given step. A \(\color {red}\bm {\times }\) marks the predicted best level of each factor, and a \(\color {blue}\bm {\times }\) marks the factor level on the global optimum.\relax }}{118}{figure.caption.90}%
\contentsline {figure}{\numberline {12.7}{\ignorespaces Distribution of slowdowns in relation to the global optimum for 7 optimization methods on the Laplacian Kernel, using a budget of 125 points over 1000 repetitions\relax }}{120}{figure.caption.91}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {13.1}{\ignorespaces Cost of best points found on each run, and the iteration where they were found. RS and DLMT found no speedups with similar budgets for kernels marked with ``[0]'' and \emph {blue} headers, and similar speedups with similar budgets for kernels marked with ``[=]'' and \emph {orange} headers. DLMT found similar speedups using smaller budgets for kernels marked with ``[+]'' \emph {green} headers. Ellipses delimit an estimate of where 95\% of the underlying distribution lies\relax }}{126}{figure.caption.94}%
\contentsline {figure}{\numberline {13.2}{\ignorespaces Histograms of explored search spaces, showing the real count of measured configurations. Kernels are grouped in the same way as in Figure~\ref {fig:iteration_best_comparison}. DLMT spent fewer measurements than RS in configurations with smaller speedups or with slowdowns, even for kernels in the orange group. DLMT also spent more time exploring configurations with larger speedups\relax }}{127}{figure.caption.95}%
\contentsline {figure}{\numberline {13.3}{\ignorespaces Summary of our DLMT results, compared to uniform Random Sampling (RS), showing the configurations with smallest speedups, found in 10 independent runs. The top panel compares execution time, with color-encoded iterations, and the bottom panel compares iterations where configurations were found, with color-encoded execution times.\relax }}{129}{figure.caption.96}%
\contentsline {figure}{\numberline {13.4}{\ignorespaces Count of the model terms eliminated in each of the 4 steps of 10 independent runs, without cubic terms\relax }}{130}{figure.caption.98}%
\contentsline {figure}{\numberline {13.5}{\ignorespaces Measured execution time of all points in the designs constructed at each step, where panel headers mark the hostname of the machines used in each of the 10 separate experiments\relax }}{131}{figure.caption.99}%
\contentsline {figure}{\numberline {13.6}{\ignorespaces Fitting linear models and quadratic quantile regression, with \(\tau = 0.05\), \(0.25\), and \(0.5\), to separate factors for 10 uniform random sampling runs\relax }}{132}{figure.caption.100}%
\contentsline {figure}{\numberline {13.7}{\ignorespaces Same as above, for 10 uniform sampling runs with \textit {OMP} fixed to \textit {on}\relax }}{132}{figure.caption.101}%
\contentsline {figure}{\numberline {13.8}{\ignorespaces Performance of best points, step-by-step, by experiment\relax }}{134}{figure.caption.102}%
\contentsline {figure}{\numberline {13.9}{\ignorespaces Best \emph {bicg} configurations and iterations that found them, for Gaussian Process Regression with Expected Improvement acquisition function (GPR), and uniform Random Sampling (RS), with a budget of 400 measurements.\relax }}{135}{figure.caption.103}%
\contentsline {figure}{\numberline {13.10}{\ignorespaces Execution time of points measured by GPR along iterations, with pareto border in red\relax }}{137}{figure.caption.104}%
\contentsline {figure}{\numberline {13.11}{\ignorespaces Execution time of points measured by RS along iterations\relax }}{137}{figure.caption.105}%
\contentsline {figure}{\numberline {13.12}{\ignorespaces Best \emph {dgemv3} configurations and iterations that found them, for Gaussian Process Regression with Expected Improvement acquisition function (GPR), and uniform Random Sampling (RS), with a budget of 400 measurements.\relax }}{138}{figure.caption.106}%
\contentsline {figure}{\numberline {13.13}{\ignorespaces Empirical Roofline graph for the \emph {Xeon E5-2630v3} processor~\cite {intel2021xeonAPP,intel2021xeon}, showing the best point we found during all experiments with the memory-bound \emph {dgemv3} SPAPT kernel\relax }}{138}{figure.caption.107}%
\contentsline {figure}{\numberline {13.14}{\ignorespaces Empirical Roofline graph for the \emph {Xeon E5-2630v3} processor~\cite {intel2021xeonAPP,intel2021xeon}, showing the best point we found during all experiments with the memory-bound \emph {dgemv3} SPAPT kernel\relax }}{139}{figure.caption.108}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {14.1}{\ignorespaces Optimizing nonuniform weight quantization policies for the \textit {ResNet50} network, keeping total weight size below 10 \emph {MB}, and attempting to keep accuracy. The optimization methods we compared were a Sobol Sampler, Reinforcement Learning~\cite {wang2019haq}, and Gaussian Process Regression with Expected Improvement\relax }}{142}{figure.caption.109}%
\contentsline {figure}{\numberline {14.2}{\ignorespaces \textit {ResNet50} architecture, implemented in \textit {pytorch}\relax }}{144}{figure.caption.110}%
\contentsline {figure}{\numberline {14.3}{\ignorespaces Images sampled from some of the categories in \textit {ImageNet}, adapted from Deng \emph {et al.}~\cite {deng2009imagenet}\relax }}{145}{figure.caption.111}%
\contentsline {figure}{\numberline {14.4}{\ignorespaces Pseudocode implementations of the GPR method using Expected Improvement (EI), and of the constrained sampler using Sobol sequences}}{148}{figure.caption.113}%
\contentsline {figure}{\numberline {14.5}{\ignorespaces 10 repetitions of the 4 methods we compared, using a budget of 245 measurements, with 95\% confidence intervals for estimates of the mean\relax }}{149}{figure.caption.115}%
\contentsline {figure}{\numberline {14.6}{\ignorespaces Best quantization policy found by each method during optimization, across 10 repetitions\relax }}{150}{figure.caption.116}%
\contentsline {figure}{\numberline {14.7}{\ignorespaces Results with the baseline Reinforcement Learning method\relax }}{152}{figure.caption.117}%
\contentsline {figure}{\numberline {14.8}{\ignorespaces Results with Gaussian Process Regression\relax }}{153}{figure.caption.118}%
\contentsline {figure}{\numberline {14.9}{\ignorespaces Running optimized quantization policies with new \emph {ImageNet} samples, with 95\% confidence intervals for the mean estimate. The baseline is the uniform 8-bit \emph {ResNet50} quantization\relax }}{154}{figure.caption.119}%
\contentsline {figure}{\numberline {14.10}{\ignorespaces Execution time of the different steps of our GPR implementation. The initial sample measurement is not shown, and the \emph {y}-axis is logarithmic\relax }}{155}{figure.caption.120}%
\contentsline {figure}{\numberline {14.11}{\ignorespaces Sobol indices computed for the impact on \emph {Top5} accuracy of the quantization of each \emph {ResNet50} layer using only Sobol samples, in the left panel, and Sobol samples plus the search spaces explored by GPR, in the right panel\relax }}{156}{figure.caption.121}%
\contentsline {figure}{\numberline {14.12}{\ignorespaces Running GPR with the gathered ensemble of GPR experiments. The bottom row shows only the new measurements performed after fitting a Gaussian Process with the ensemble of experiments, and the top row shows all the data that were used\relax }}{157}{figure.caption.123}%
\contentsline {figure}{\numberline {14.13}{\ignorespaces Optimizing accuracy and weight with the modified baseline Reinforcement Learning method\relax }}{158}{figure.caption.124}%
\contentsline {figure}{\numberline {14.14}{\ignorespaces Optimizing accuracy and weight with Gaussian Process Regression with EI.\relax }}{158}{figure.caption.125}%
